# **BERT**
---

    Bert 是一种基于预训练的深层双向变压器，它是双向转换器，包含多伦多图书语料库和维基百科的大型语料库上结合使用了蒙版语言建模和下一句预测进项了预训练。
    
    论文摘要：
      表示transform的双向编码表示。BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.
      因此，只需使用一个额外的输出层就可以对经过预训练的BERT模型进行微调，以创建适用于各种任务（例如问题解答和语言推论）的最新模型，而无需进行大量任务处理，特定的体系结构修改。
      BERT在概念上很简单，在经验上也很强大。它在11种自然语言处理任务上获得了最新的最新结果，包括将GLUE得分提高到80.5％（绝对提高7.7％），MultiNLI准确性提高到86.7％（绝对提高4.6％），SQuAD v1.1问答测试F1达到93.2（绝对值提高1.5分）和SQuAD v2.0测试F1达到83.1（绝对值提高5.1点）
 
---
**Tips**

- BERT是具有绝对位置嵌入的模型，因此通常建议将输入填充在右侧而不是左侧。
- BERT受过掩盖语言模型（MLM）目标的训练。因此，它通常可以有效的预测屏蔽令牌和NLU，但对于文本生成而言并非最佳。在这方面使用因果语言建模（CLM），的目标训练模型更好。
- 与MLM一起，使用CLS作为序列近似值，适用下一个句子预测（NSP）目标训练BERT。用户可以使用此令牌来获取预测而不是令牌预测。但是，与使用CLS令牌相比，对序列进行平均可能会产生更好的效果。


---

   
    