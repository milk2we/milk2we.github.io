# **BERT**
---

    Bert 是一种基于预训练的深层双向变压器，它是双向转换器，包含多伦多图书语料库和维基百科的大型语料库上结合使用了蒙版语言建模和下一句预测进项了预训练。
    
    论文摘要：
      表示transform的双向编码表示。BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.
      因此，只需使用一个额外的输出层就可以对经过预训练的BERT模型进行微调，以创建适用于各种任务（例如问题解答和语言推论）的最新模型，而无需进行大量任务处理，特定的体系结构修改。
      BERT在概念上很简单，在经验上也很强大。它在11种自然语言处理任务上获得了最新的最新结果，包括将GLUE得分提高到80.5％（绝对提高7.7％），MultiNLI准确性提高到86.7％（绝对提高4.6％），SQuAD v1.1问答测试F1达到93.2（绝对值提高1.5分）和SQuAD v2.0测试F1达到83.1（绝对值提高5.1点）
 
---
**Tips**

- BERT是具有绝对位置嵌入的模型，因此通常建议将输入填充在右侧而不是左侧。
- BERT受过掩盖语言模型（MLM）目标的训练。因此，它通常可以有效的预测屏蔽令牌和NLU，但对于文本生成而言并非最佳。在这方面使用因果语言建模（CLM），的目标训练模型更好。
- 与MLM一起，使用CLS作为序列近似值，适用下一个句子预测（NSP）目标训练BERT。用户可以使用此令牌来获取预测而不是令牌预测。但是，与使用CLS令牌相比，对序列进行平均可能会产生更好的效果。


---

**知识点**
---
    BERT是多个transform的双向Transform的Encoder。集合了注意力机制，在训练过程中不需要标签。在pre-train方法上使用Masked LM和Next Sentence Prediction 两种方法分别捕捉词语和句子级别的representation。
   **Masked Language Model**  [link](https://zhuanlan.zhihu.com/p/46833276) 
       
   
   [!avatar](https://pic3.zhimg.com/80/v2-5686dfb7db40ae74003e42060b7b304a_720w.jpg)
   如图所示的ELMO是两个单向RNN够成的模型拼接。则，语言模型本身定义的是句子的概率：
   [!avatar](https://www.zhihu.com/equation?tex=p%28S%29+%3D+p%28w_1%2Cw_2%2Cw_3%2C...%2Cw_m%29%3Dp%28w_1%29p%28w_2%7Cw_1%29p%28w_3%7Cw_1%2Cw_2%29...p%28w_m%7Cw_1%2Cw_2%2C...%2Cw_%7Bm-1%7D%29%5C%5C+%3D%5Cprod_%7Bi%3D1%7D%5E%7Bm%7Dp%28w_i%7Cw_1%2Cw_2%2C...%2Cw_%7Bi-1%7D%29+%5Ctag%7B2%7D)
   前向RNN构成的语言模型计算是：[!avatar](https://www.zhihu.com/equation?tex=p%28w_1%2Cw_2%2Cw_3%2C...%2Cw_m%29+%3D%5Cprod_%7Bi%3D1%7D%5E%7Bm%7Dp%28w_i%7Cw_1%2Cw_2%2C...%2Cw_%7Bi-1%7D%29+%5Ctag%7B3%7D)
   也就是当前词的概率只依赖前面出现词的概率。而，后向RNN构成的语言模型计算是：[!avatar](https://www.zhihu.com/equation?tex=p%28w_1%2Cw_2%2Cw_3%2C...%2Cw_m%29+%3D%5Cprod_%7Bi%3D1%7D%5E%7Bm%7Dp%28w_i%7Cw_%7Bi%2B1%7D%2Cw_%7Bi%2B2%7D%2C...%2Cw_%7Bm%7D%29+%5Ctag%7B4%7D)
   也就是，当前词的概率只依赖后面出现词的概率。
   
   而，BERT的概率为[!avatar](https://www.zhihu.com/equation?tex=P%28w_i%7Cw_1%2C++...%2Cw_%7Bi-1%7D%2C+w_%7Bi%2B1%7D%2C...%2Cw_n%29)
   
   
   如何同时利用好前面词和后面词的概率呢？BERT提出MLM，也就是随机去掉句子中的部分token，然后模型来预测去掉的token是什么。这样实际上已经不是convention NN了，类似生成模型。而是单纯的作为分类问题，根据这个时刻的hidden—state来预测这个时刻的token是什么，而不是预测下一个时刻的词的分布。
   这里的随机操作，是随机mask语料中的15%的token，然后预测masked token，那么masked token位置输出的final hidden vector 传输给softmax网络即可得到masked token的预测结果了。
   
   这样操作存在问题是，fine-tuning的时候没有\[MASK] token, 因此存在pre-training 和 fine-tuning之间的mismatch，为了解决这个问题，采用如下策略。
  - 80%的时间里，将选中的词用\[MASK] token来代替。
  - 10%的时间里，用任意词代替。
  - 10%的时间里，不发生变化。
  
  **Next Sentence Prediction**
    因此将下一句话预测作为了第二个预训练任务。该任务的训练语料是两句话，来预测第二句话是否是第一句话的下一句话
   
  **Input Representation**
    多个Sentence拼成一个Sentence 
    token Representation = token embedding + segment（句） + position embedding

  **Pre-training过程**
    数据：BooksCorpus（800M words） + English Wikipedia（2500M words）
    
    Wikipedia的数据只取text段落，忽略列表、表格等
    
    从语料中，抽样2个spans作为sentence，第一个sentence作为A Embeding，第二个作为B Embeding；50%的概率B是实际的sentence，另外50% B是随机sentence.
    
    抽样时保证A+B的长度<512 tokens
    
    LM masking：在用wordpiece 分词（tokenization）后，均匀按15%的比例（rate）mask掉其中的tokens。没有任何特殊的操作。    
    
   **Config**
   Loss = mean masked LM Likelihood + mean next sentence prediction likelihood

  **Fine-tuning过程**
  这里fine-tuning之前对模型的修改非常简单，例如针对sequence-level classification problem(例如情感分析)，取第一个token的输出表示，喂给一个softmax层得到分类结果输出；对于token-level classification(例如NER)，取所有token的最后层transformer输出，喂给softmax层做分类。
  [!avatar](https://upload-images.jianshu.io/upload_images/242402-bd4f48f4b2554e43.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)