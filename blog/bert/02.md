## CODE base on huggingface.co

----

**BertConfig**

----

`
class transformers.BertConfig(vocab_size=30522, hidden_size=768, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072, hidden_act='gelu', hidden_dropout_prob=0.1, attention_probs_dropout_prob=0.1, max_position_embeddings=512, type_vocab_size=2, initializer_range=0.02, layer_norm_eps=1e-12, **kwargs)
`

   This objects inherit from **PretrainedConfig** and can be used to control the model output.
    
   This function will store the configuration of a **BertModel**

---
***Parameters***:
 - vocab_size(int, optional, defaults to 30522): Vocabulary size of the BERT model. Defines the different tokens that can be represented by
   the _input_ids_  passed to the forward method of _BertModel_.
 - hidden_size(int, opt, defaults to 768): Dimensionality of the encoder layers and the pooler layer
 - num_hidden_layers(int, opt, defaults to 12): Number of hidden layers in the Transformer encoder.
 - num_attention_layers(int, opt, defaults to 12): Number of attention heads for each attention layer in the Transformer encoder.
 - intermediate_size(int, opt, defaults to 3072): Dimensional of the "Intermediate" layer in the Transformer encoder
 - hidden_act(str or function, opt, defaults to "gelu"): The non-liner activation function.
 - hidden_dropout_prob(float, opt, defaults to 0.1): The dropout probability for all layers, besides of attention 
 - attention_probs_dropout_prob(float, opt, defaults to 512): The dropout ratio for the attention probabilities.
 - max_position_embeddings(int,opt, defaults to 512): The maximum  sequence length that this model might ever be used with.
 - type_vocab_size(int, opt, defaults to 2): The vocabulary size of the _token_type_ids_
 - initializer_range (float, optional, defaults to 0.02) – The standard deviation of the truncated_normal_initializer for initializing all weight matrices. 
 - layer_norm_eps (float, optional, defaults to 1e-12) – The epsilon used by the layer normalization layers.
 

 
    from transformers import BertModel, BertConfig
    # Initializing a BERT bert-base-uncased style configuration
    configuration = BertConfig()
    
    # Initializing a model from the bert-base-uncased style configuration
    model = BertModel(configuration)
    
    # Accessing the model configuration
    configuration = model.config`

  


  
    import torch
    from transformers import BertTokenizer, BertModel, BertForMaskedLM
    
    import logging
    logging.basicConfig(level=logging.INFO)
    
    # load pre-trained model tokenizer (vocabulary)
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    # 继承于PreTrainedTokenizer类，用于下载vocabulary。
    """
     Examples::

            # We can't instantiate directly the base class `PreTrainedTokenizer` so let's show our examples on a derived class: BertTokenizer

            # Download vocabulary from S3 and cache.
            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

            # Download vocabulary from S3 (user-uploaded) and cache.
            tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-german-cased')

            # If vocabulary files are in a directory (e.g. tokenizer was saved using `save_pretrained('./test/saved_model/')`)
            tokenizer = BertTokenizer.from_pretrained('./test/saved_model/')

            # If the tokenizer uses a single vocabulary file, you can point directly to this file
            tokenizer = BertTokenizer.from_pretrained('./test/saved_model/my_vocab.txt')

            # You can link tokens to special vocabulary when instantiating
            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', unk_token='<unk>')
            # You should be sure '<unk>' is in the vocabulary when doing that.
            # Otherwise use tokenizer.add_special_tokens({'unk_token': '<unk>'}) instead)
            assert tokenizer.unk_token == '<unk>'
    """ 

    # Tokenize input
    text = "[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]"
    tokenized_text = tokenizer.tokenize(text)
    # 继承
    # Converts a string in a sequence of tokens (string), using the tokenizer.
    #             Split in words for word-based vocabulary or sub-words for sub-word-based
    #             vocabularies
    print(tokenized_text)`
