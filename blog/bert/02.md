## CODE base on huggingface.co

----

**BertConfig**

----

`
class transformers.BertConfig(vocab_size=30522, hidden_size=768, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072, hidden_act='gelu', hidden_dropout_prob=0.1, attention_probs_dropout_prob=0.1, max_position_embeddings=512, type_vocab_size=2, initializer_range=0.02, layer_norm_eps=1e-12, **kwargs)
`

   This objects inherit from **PretrainedConfig** and can be used to control the model output.
    
   This function will store the configuration of a **BertModel**

---
***Parameters***:
 - vocab_size(int, optional, defaults to 30522): Vocabulary size of the BERT model. Defines the different tokens that can be represented by
   the _input_ids_  passed to the forward method of _BertModel_.
 - hidden_size(int, opt, defaults to 768): Dimensionality of the encoder layers and the pooler layer
 - num_hidden_layers(int, opt, defaults to 12): Number of hidden layers in the Transformer encoder.
 - num_attention_layers(int, opt, defaults to 12): Number of attention heads for each attention layer in the Transformer encoder.
 - intermediate_size(int, opt, defaults to 3072): Dimensional of the "Intermediate" layer in the Transformer encoder
 - hidden_act(str or function, opt, defaults to "gelu"): The non-liner activation function.
 - hidden_dropout_prob(float, opt, defaults to 0.1): The dropout probability for all layers, besides of attention 
 - attention_probs_dropout_prob(float, opt, defaults to 512): The dropout ratio for the attention probabilities.
 - max_position_embeddings(int,opt, defaults to 512): The maximum  sequence length that this model might ever be used with.
 - type_vocab_size(int, opt, defaults to 2): The vocabulary size of the _token_type_ids_
 - initializer_range (float, optional, defaults to 0.02) – The standard deviation of the truncated_normal_initializer for initializing all weight matrices. 
 - layer_norm_eps (float, optional, defaults to 1e-12) – The epsilon used by the layer normalization layers.
 
 `
 
    from transformers import BertModel, BertConfig
    # Initializing a BERT bert-base-uncased style configuration
    configuration = BertConfig()
    
    # Initializing a model from the bert-base-uncased style configuration
    model = BertModel(configuration)
    
    # Accessing the model configuration
    configuration = model.config`


