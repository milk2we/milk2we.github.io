## CODE base on huggingface.co

----

**BertConfig**

----

`
class transformers.BertConfig(vocab_size=30522, hidden_size=768, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072, hidden_act='gelu', hidden_dropout_prob=0.1, attention_probs_dropout_prob=0.1, max_position_embeddings=512, type_vocab_size=2, initializer_range=0.02, layer_norm_eps=1e-12, **kwargs)
`

   This objects inherit from **PretrainedConfig** and can be used to control the model output.
    
   This function will store the configuration of a **BertModel**

---
***Parameters***:
 - vocab_size(int, optional, defaults to 30522): Vocabulary size of the BERT model. Defines the different tokens that can be represented by
   the _input_ids_  passed to the forward method of _BertModel_.
 - hidden_size(int, opt, defaults to 768): Dimensionality of the encoder layers and the pooler layer
 - num_hidden_layers(int, opt, defaults to 12): Number of hidden layers in the Transformer encoder.
 - num_attention_layers(int, opt, defaults to 12): Number of attention heads for each attention layer in the Transformer encoder.
 - intermediate_size(int, opt, defaults to 3072): Dimensional of the "Intermediate" layer in the Transformer encoder
 - hidden_act(str or function, opt, defaults to "gelu"): The non-liner activation function.
 - hidden_dropout_prob(float, opt, defaults to 0.1): The dropout probability for all layers, besides of attention 
 - attention_probs_dropout_prob(float, opt, defaults to 512): The dropout ratio for the attention probabilities.
 - max_position_embeddings(int,opt, defaults to 512): The maximum  sequence length that this model might ever be used with.
 - type_vocab_size(int, opt, defaults to 2): The vocabulary size of the _token_type_ids_
 - initializer_range (float, optional, defaults to 0.02) – The standard deviation of the truncated_normal_initializer for initializing all weight matrices. 
 - layer_norm_eps (float, optional, defaults to 1e-12) – The epsilon used by the layer normalization layers.
 
 ---
 
  BERT 的输入是(x, seq_len, mask)，x是经过bert分词的id。seq_len是句子长度，mask是掩码。
  
  ---
#   输入编号
    输入id通常是传递给模型作为输入的唯一必需参数。它们是令牌索引，令牌的数字表示构建了将由模型用作输入的序列。
    
    每个标记器的工作方式不同，但基本机制保持不变。这是一个使用BERT标记器的示例，它是一个WordPiece标记器：
    
    from transformers import BertTokenizer
    tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
    
    sequence = "A Titan RTX has 24GB of VRAM"
    令牌生成器负责将序列拆分为令牌生成器词汇表中可用的令牌。
    
    # Continuation of the previous script
    tokenized_sequence = tokenizer.tokenize(sequence)
    assert tokenized_sequence == ['A', 'Titan', 'R', '##T', '##X', 'has', '24', '##GB', 'of', 'V', '##RA', '##M']
    然后可以将这些令牌转换为模型可以理解的ID。有几种方法可用于此目的，推荐的方法是encode或encode_plus，它们利用Rusting face / tokenizers的Rust实现来实现 最佳性能。
    
    # Continuation of the previous script
    encoded_sequence = tokenizer.encode(sequence)
    assert encoded_sequence == [101, 138, 18696, 155, 1942, 3190, 1144, 1572, 13745, 1104, 159, 9664, 2107, 102]
    该编码和encode_plus方法自动添加“特殊记号”，这是特殊的ID的模型使用。
    
    
# 注意面具
    注意掩码是将序列批处理在一起时使用的可选参数。此参数向模型指示应该注意哪些令牌，哪些不应该注意。
    
    例如，考虑以下两个序列：
    
    from transformers import BertTokenizer
    tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
    
    sequence_a = "This is a short sequence."
    sequence_b = "This is a rather long sequence. It is at least longer than the sequence A."
    
    encoded_sequence_a = tokenizer.encode(sequence_a)
    assert len(encoded_sequence_a) == 8
    
    encoded_sequence_b = tokenizer.encode(sequence_b)
    assert len(encoded_sequence_b) == 19
    这两个序列的长度不同，因此不能按原样放在同一张量中。需要将第一个序列填充到第二个序列的长度，或者将第二个序列截短到第一个序列的长度。
    
    在第一种情况下，ID列表将通过填充索引扩展：
    
    # Continuation of the previous script
    padded_sequence_a = tokenizer.encode(sequence_a, max_length=19, pad_to_max_length=True)
    
    assert padded_sequence_a == [101, 1188, 1110, 170, 1603, 4954,  119, 102,    0,    0,    0,    0,    0,    0,    0,    0,   0,   0,   0]
    assert encoded_sequence_b == [101, 1188, 1110, 170, 1897, 1263, 4954, 119, 1135, 1110, 1120, 1655, 2039, 1190, 1103, 4954, 138, 119, 102]
    然后可以将它们转换为PyTorch或TensorFlow中的张量。注意掩码是一个二进制张量，指示填充索引的位置，以便模型不会注意它们。对于 BertTokenizer，请1指示应注意的值，而0指示已填充的值。
    
    该方法encode_plus()可用于直接获取注意蒙版：
    
    # Continuation of the previous script
    sequence_a_dict = tokenizer.encode_plus(sequence_a, max_length=19, pad_to_max_length=True)
    
    assert sequence_a_dict['input_ids'] == [101, 1188, 1110, 170, 1603, 4954, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    assert sequence_a_dict['attention_mask'] == [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
  ---

 
    from transformers import BertModel, BertConfig
    # Initializing a BERT bert-base-uncased style configuration
    configuration = BertConfig()
    
    # Initializing a model from the bert-base-uncased style configuration
    model = BertModel(configuration)
    
    # Accessing the model configuration
    configuration = model.config`

  


  
    import torch
    from transformers import BertTokenizer, BertModel, BertForMaskedLM
    
    import logging
    logging.basicConfig(level=logging.INFO)
    
    # load pre-trained model tokenizer (vocabulary)
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    # 继承于PreTrainedTokenizer类，用于下载vocabulary。
    """
     Examples::

            # We can't instantiate directly the base class `PreTrainedTokenizer` so let's show our examples on a derived class: BertTokenizer

            # Download vocabulary from S3 and cache.
            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

            # Download vocabulary from S3 (user-uploaded) and cache.
            tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-german-cased')

            # If vocabulary files are in a directory (e.g. tokenizer was saved using `save_pretrained('./test/saved_model/')`)
            tokenizer = BertTokenizer.from_pretrained('./test/saved_model/')

            # If the tokenizer uses a single vocabulary file, you can point directly to this file
            tokenizer = BertTokenizer.from_pretrained('./test/saved_model/my_vocab.txt')

            # You can link tokens to special vocabulary when instantiating
            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', unk_token='<unk>')
            # You should be sure '<unk>' is in the vocabulary when doing that.
            # Otherwise use tokenizer.add_special_tokens({'unk_token': '<unk>'}) instead)
            assert tokenizer.unk_token == '<unk>'
    """ 

    # Tokenize input
    text = "[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]"
    tokenized_text = tokenizer.tokenize(text)
    # 继承
    # Converts a string in a sequence of tokens (string), using the tokenizer.
    #             Split in words for word-based vocabulary or sub-words for sub-word-based
    #             vocabularies
    print(tokenized_text)`
