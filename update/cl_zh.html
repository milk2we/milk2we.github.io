<!DOCTYPE html><html><head><meta charset="utf-8"><title>CS.CL</title></head><body>
<h2>2020-03-14</h2>
<h3>No. 1	引导注意力进入序列模型以预测对话行为</h3><h4>Pierre Colombo, Emile Chapuis, Matteo Manica, Emmanuel Vignon, Giovanna Varni, Chloe Clavel</h4>文摘：基于会话对话的对话行为预测（DA）任务是会话代理开发的关键组成部分。准确预测DAs需要对会话和全局标记依赖关系进行精确建模。我们利用SEQ2SEQ方法广泛采用的神经机器翻译（NMT），以改善标签序列的建模。已知Seq2seq模型学习复杂的全局依赖性，而目前提出的方法仅使用线性条件随机场（CRF）对局部标记依赖性建模。在这项工作中，我们介绍了一个为DA分类量身定制的seq2seq模型：一个分层编码器、一个新的引导注意机制和一个用于训练和推理的波束搜索。与最先进的技术相比，我们的模型不需要手工制作的功能，并且经过端到端的培训。此外，该方法在SwDA上的准确度得分为85%，在MRDA上的准确度得分为91.6%。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09419">PDF</a>
<h3>No. 2	对齐嵌入空间是一项具有挑战性的任务吗？现有方法分析</h3><h4>Russa Biswas, Mehwish Alam, Harald Sack</h4>文摘：近年来，在低维向量空间中进行词汇和知识图的表示学习及其在现实场景中的应用越来越受到重视。为了将多个KG嵌入应用于知识驱动的应用，如问答、命名实体消歧、知识图完成等，需要对不同的KG嵌入空间进行对齐。除了多语言和特定领域的信息外，不同的KG还带来了结构差异的问题，使得KG嵌入的对齐更具挑战性。本文对两种表示实体和实体词的嵌入空间的对齐方法进行了理论分析和比较。本文还以不同的应用为借口，对现有对准方法的性能和不足进行了评估。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09247">PDF</a>
<h3>No. 3	无监督跨语言单词嵌入的改进</h3><h4>Magdalena Biesialska, Marta R. Costa-jussà</h4>摘要：跨语言单词嵌入旨在通过允许学习多语言的单词表示来弥补高资源语言和低资源语言之间的差距，即使不使用任何直接的双语信号。这些方法中最大的一部分是基于投影的方法，将预先训练的嵌入映射到共享的潜在空间中。这些方法大多是基于正交变换，它假设语言向量空间是同构的。然而，这个标准并不一定成立，特别是对于形态学丰富的语言。本文提出了一种自监督的方法来改进无监督双语词嵌入的对齐。该模型将词的向量及其对应的翻译移动得更近，并增强了长度和中心不变性，从而可以更好地对齐跨语言嵌入。实验结果证明了该方法的有效性，因为在大多数情况下，该方法在双语词典归纳任务中都优于最新的方法。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09213">PDF</a>
<h3>No. 4	基于文本游戏的动态知识图学习</h3><h4>Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Côté, Mikuláš Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, William L. Hamilton</h4>摘要：玩基于文本的游戏需要处理自然语言和计划的技巧。虽然代理解决此任务的一个关键目标是在多个博弈中进行泛化，但大多数先前的工作要么集中在解决单个博弈，要么使用基于规则的启发式方法处理泛化。在这项工作中，我们研究以知识图（KG）形式呈现的结构化信息如何有助于有效的规划和概括。我们引入了一种新的基于变压器的序列到序列模型，该模型从环境的原始文本观察构建了一个“信念”KG，当它接收到新的观察时，在每个博弈步骤动态更新该信念图。为了训练该模型建立有用的图表示，我们引入并分析了一组与图相关的预训练任务。我们的经验证明，我们的模型中基于KG的表示有助于代理更快地收敛到多个基于文本的游戏的更好的策略，并且进一步，在未看到的游戏上实现更强的零射击性能。在看不见的游戏上的实验表明，我们的最佳代理比基于文本的基线性能好21.6%。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09127">PDF</a>
<h3>No. 5	随机加权编码器在总结任务中的优异性能</h3><h4>Jonathan Pilault, Jaehong Park, Christopher Pal</h4>文摘：本文研究了一类序列到序列模型中未经训练的随机初始化编码器的性能，并与完全训练的编码器在抽象摘要任务中的性能进行了比较。我们假设输入文本的随机投影有足够的表示能力来编码句子的层次结构和文档的语义。使用经过训练的解码器生成抽象的文本摘要，我们实证地证明，具有未经训练的随机初始化编码器的架构相对于具有完全训练的编码器的等效架构具有竞争力。我们进一步发现，编码器的容量不仅提高了整体模型的泛化能力，而且缩小了未经训练的随机初始化编码器与完全训练编码器之间的性能差距。据我们所知，这是第一次评估具有注意的一般序列到序列模型在抽象摘要上的训练和随机投影表示。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09084">PDF</a>
<h3>No. 6	用反馈存储器访问时序变压器的高级表示</h3><h4>Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, Sainbayar Sukhbaatar</h4>摘要：变压器是一种能并行处理输入令牌的前馈网络。虽然这种并行化使它们的计算效率更高，但它限制了模型充分利用输入的顺序性——给定层的表示只能访问较低层的表示，而不能访问先前时间步骤中已经构建的较高层的表示。在这项工作中，我们提出了反馈转换器架构，它将所有先前的表示公开到所有未来的表示，这意味着当前时间步的最低表示是由过去的最高级别抽象表示形成的。我们在语言建模、神经机器翻译、总结和强化学习的各种基准上证明，增加的表示能力可以改善变压器基线。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09402">PDF</a>
<h3>No. 7	基于关系匹配传播的众包集合实体分解</h3><h4>Jiacheng Huang, Wei Hu, Zhifeng Bao, Yuzhong Qu</h4>摘要：知识库（KBs）存储了丰富而异构的实体和事实。实体分解（ER）旨在识别KBs中引用同一现实对象的实体。最近的研究表明，让人类参与内质网的循环具有显著的好处。它们通常利用属性值上的成对相似性度量来解决实体问题，并利用群体来标记不确定的实体。然而，现有的方法仍存在一定程度上的人工成本高、标签不够等问题。在本文中，我们提出了一种新的方法，称为众包集合ER，它利用实体之间的关系来共同而不是独立地推断匹配。具体地说，它迭代地要求人类工作人员标记所选的实体对，并将标记信息传播到距离较远的邻居。在此过程中，我们讨论了候选实体剪枝、概率传播、最优问题选择和容错真理推理等问题。我们在真实数据集上的实验表明，与最新的方法相比，我们的方法以更少的标记获得了更高的精度。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09361">PDF</a>
<h3>No. 8	语言作为一种认知工具在好奇心驱动的探索中想象目标</h3><h4>Cédric Colas, Tristan Karch, Nicolas Lair, Jean-Michel Dussoux, Clément Moulin-Frier, Peter Ford Dominey, Pierre-Yves Oudeyer</h4>摘要：自主强化学习者必须有内在的动机去探索他们的环境，发现潜在的目标，表现它们并学习如何实现它们。当孩子们做同样的事情时，他们也会从语言中受益，利用语言来制定目标，并在学习其意义时想象新的目标。在我们提出的学习架构（想象）中，代理可以自由地探索其环境，并将社交伙伴对有趣交互的自然语言描述转化为潜在目标。想象通过共同学习一个语言模型和一个目标条件奖励函数来学习表达目标。就像人类一样，我们的代理使用语言组合性，通过组合已知的目标来生成新的目标。利用基于深度集和门控注意机制的模块化模型体系结构，IMAGINE可以自主地构建一系列行为，并为各种类型的泛化显示良好的零镜头泛化特性。当想象自己的目标时，代理利用奖励函数的零次泛化来进一步训练想象的目标并改进其行为。我们在一个模拟域中进行实验，在这个域中，代理与包含各种类型和颜色的对象的程序生成的场景交互，发现目标，想象其他场景，并学习如何实现它们。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09253">PDF</a><h2>2020-03-13</h2>
<h3>No. 1	引导注意力进入序列模型以预测对话行为</h3><h4>Pierre Colombo, Emile Chapuis, Matteo Manica, Emmanuel Vignon, Giovanna Varni, Chloe Clavel</h4>摘要：基于会话对话的对话行为预测任务是会话代理开发的关键组成部分。准确预测DAs需要对会话和全局标记依赖关系进行精确建模。我们利用SEQ2SEQ方法广泛采用的神经机器翻译（NMT），以改善标签序列的建模。已知Seq2seq模型学习复杂的全局依赖性，而目前提出的方法仅使用线性条件随机场（CRF）对局部标记依赖性建模。在这项工作中，我们介绍了一个为DA分类量身定制的seq2seq模型：一个分层编码器、一个新的引导注意机制和一个用于训练和推理的波束搜索。与最先进的技术相比，我们的模型不需要手工制作的功能，并且经过端到端的培训。此外，该方法在SwDA上的准确度得分为85%，在MRDA上的准确度得分为91.6%。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09419">PDF</a>
<h3>No. 2	对齐嵌入空间是一项具有挑战性的任务吗？现有方法分析</h3><h4>Russa Biswas, Mehwish Alam, Harald Sack</h4>文摘：近年来，在低维向量空间中进行词汇和知识图的表示学习及其在现实场景中的应用越来越受到重视。为了将多个KG嵌入应用于知识驱动的应用，如问答、命名实体消歧、知识图完成等，需要对不同的KG嵌入空间进行对齐。除了多语言和特定领域的信息外，不同的KG还带来了结构差异的问题，使得KG嵌入的对齐更具挑战性。本文对两种表示实体和实体词的嵌入空间的对齐方法进行了理论分析和比较。本文还以不同的应用为借口，对现有对准方法的性能和不足进行了评估。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09247">PDF</a>
<h3>No. 3	无监督跨语言单词嵌入的改进</h3><h4>Magdalena Biesialska, Marta R. Costa-jussà</h4>摘要：跨语言单词嵌入旨在通过允许学习多种语言的单词表示来弥补高资源和低资源语言之间的差距，即使不使用任何直接的双语信号。这些方法中最大的一部分是基于投影的方法，将预先训练好的嵌入映射到共享的潜在空间中。这些方法大多是基于正交变换，它假定语言向量空间是同构的。然而，这个标准并不一定成立，特别是对于形态学丰富的语言。本文提出了一种自监督的方法来改进无监督双语词嵌入的对齐。该模型将词的向量及其对应的翻译移动得更近，并增强了长度和中心不变性，从而可以更好地对齐跨语言嵌入。实验结果证明了该方法的有效性，因为在大多数情况下，该方法在双语词典归纳任务中都优于最新的方法。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09213">PDF</a>
<h3>No. 4	基于文本游戏的动态知识图学习</h3><h4>Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Côté, Mikuláš Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, William L. Hamilton</h4>摘要：玩基于文本的游戏需要处理自然语言和计划的技巧。虽然代理解决此任务的一个关键目标是在多个博弈中进行泛化，但大多数先前的工作要么集中在解决单个博弈，要么使用基于规则的启发式方法处理泛化。在这项工作中，我们研究以知识图（KG）形式呈现的结构化信息如何有助于有效的规划和概括。我们引入了一种新的基于变压器的序列到序列模型，该模型从环境的原始文本观察构建了一个“信念”KG，当它接收到新的观察时，在每个博弈步骤动态更新该信念图。为了训练该模型建立有用的图表示，我们引入并分析了一组与图相关的预训练任务。我们的经验证明，我们的模型中基于KG的表示有助于代理更快地收敛到多个基于文本的游戏的更好的策略，并且进一步，在未看到的游戏上实现更强的零射击性能。在看不见的游戏上的实验表明，我们的最佳代理比基于文本的基线性能好21.6%。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09127">PDF</a>
<h3>No. 5	随机加权编码器在总结任务中的优异性能</h3><h4>Jonathan Pilault, Jaehong Park, Christopher Pal</h4>文摘：本文研究了一类序列到序列模型中未经训练的随机初始化编码器的性能，并与完全训练的编码器在抽象摘要任务中的性能进行了比较。我们假设输入文本的随机投影有足够的表示能力来编码句子的层次结构和文档的语义。使用经过训练的解码器生成抽象的文本摘要，我们实证地证明，具有未经训练的随机初始化编码器的架构相对于具有完全训练的编码器的等效架构具有竞争力。我们进一步发现，编码器的容量不仅提高了整体模型的泛化能力，而且缩小了未经训练的随机初始化编码器与完全训练编码器之间的性能差距。据我们所知，这是第一次评估具有注意的一般序列到序列模型在抽象摘要上的训练和随机投影表示。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09084">PDF</a>
<h3>No. 6	用反馈存储器访问时序变压器的高级表示</h3><h4>Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, Sainbayar Sukhbaatar</h4>摘要：变压器是一种能并行处理输入令牌的前馈网络。虽然这种并行化使它们的计算效率更高，但它限制了模型充分利用输入的顺序性——给定层的表示只能访问较低层的表示，而不能访问先前时间步骤中已经构建的较高层的表示。在这项工作中，我们提出了反馈转换器架构，它将所有先前的表示公开到所有未来的表示，这意味着当前时间步的最低表示是由过去的最高级别抽象表示形成的。我们在语言建模、神经机器翻译、总结和强化学习的各种基准上证明，增加的表示能力可以改善变压器基线。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09402">PDF</a>
<h3>No. 7	基于关系匹配传播的众包集合实体分解</h3><h4>Jiacheng Huang, Wei Hu, Zhifeng Bao, Yuzhong Qu</h4>摘要：知识库（KBs）存储了丰富而异构的实体和事实。实体分解（ER）旨在识别KBs中引用同一现实对象的实体。最近的研究表明，让人类参与内质网的循环有显著的好处。它们通常利用属性值上的成对相似度量来解决实体问题，并利用群体来标记不确定的实体。然而，现有的方法仍存在一定程度上的人工成本高、标签不够等问题。在本文中，我们提出了一种新的方法，称为众包集合ER，它利用实体之间的关系来共同而不是独立地推断匹配。具体地说，它迭代地要求人类工作人员标记所选的实体对，并将标记信息传播到距离较远的邻居。在此过程中，我们讨论了候选实体剪枝、概率传播、最优问题选择和容错真理推理等问题。我们在真实数据集上的实验表明，与最新的方法相比，我们的方法以更少的标记获得了更高的精度。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09361">PDF</a>
<h3>No. 8	语言作为一种认知工具在好奇心驱动的探索中想象目标</h3><h4>Cédric Colas, Tristan Karch, Nicolas Lair, Jean-Michel Dussoux, Clément Moulin-Frier, Peter Ford Dominey, Pierre-Yves Oudeyer</h4>摘要：自主强化学习者必须有内在的动机去探索他们的环境，发现潜在的目标，表现它们并学习如何实现它们。当孩子们做同样的事情时，他们也会从语言中受益，利用语言来制定目标，并在学习其意义时想象新的目标。在我们提出的学习架构（想象）中，代理可以自由地探索其环境，并将社交伙伴对有趣交互的自然语言描述转化为潜在目标。想象通过共同学习一个语言模型和一个目标条件奖励函数来学习表达目标。就像人类一样，我们的代理使用语言组合性，通过组合已知的目标来生成新的目标。利用基于深度集和门控注意机制的模块化模型体系结构，IMAGINE可以自主地构建一系列行为，并为各种类型的泛化显示良好的零炮泛化特性。当想象自己的目标时，代理利用奖励函数的零次泛化来进一步训练想象的目标并改进其行为。我们在一个模拟域中进行实验，在这个域中，代理与包含各种类型和颜色的对象的程序生成的场景交互，发现目标，想象其他场景，并学习如何实现它们。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09253">PDF</a><h2>2020-03-12</h2>
<h3>No. 1	引导注意力进入序列模型以预测对话行为</h3><h4>Pierre Colombo, Emile Chapuis, Matteo Manica, Emmanuel Vignon, Giovanna Varni, Chloe Clavel</h4>文摘：基于会话对话的对话行为预测（DA）任务是会话代理开发的关键组成部分。准确预测DAs需要对会话和全局标记依赖关系进行精确建模。我们利用SEQ2SEQ方法广泛采用的神经机器翻译（NMT），以改善标签序列的建模。已知Seq2seq模型学习复杂的全局依赖性，而目前提出的方法仅使用线性条件随机场（CRF）对局部标记依赖性建模。在这项工作中，我们介绍了一个为DA分类量身定制的seq2seq模型：一个分层编码器、一个新的引导注意机制和一个用于训练和推理的波束搜索。与最先进的技术相比，我们的模型不需要手工制作的功能，并且经过端到端的培训。此外，该方法在SwDA上的准确度得分为85%，在MRDA上的准确度得分为91.6%。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09419">PDF</a>
<h3>No. 2	对齐嵌入空间是一项具有挑战性的任务吗？现有方法分析</h3><h4>Russa Biswas, Mehwish Alam, Harald Sack</h4>文摘：近年来，在低维向量空间中进行词汇和知识图的表示学习及其在现实场景中的应用越来越受到重视。为了将多个KG嵌入应用于知识驱动的应用，如问答、命名实体消歧、知识图完成等，需要对不同的KG嵌入空间进行对齐。除了多语言和特定领域的信息外，不同的KG还带来了结构差异的问题，使得KG嵌入的对齐更具挑战性。本文对两种表示实体和实体词的嵌入空间的对齐方法进行了理论分析和比较。本文还以不同的应用为借口，对现有对准方法的性能和不足进行了评估。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09247">PDF</a>
<h3>No. 3	无监督跨语言单词嵌入的改进</h3><h4>Magdalena Biesialska, Marta R. Costa-jussà</h4>摘要：跨语言单词嵌入旨在通过允许学习多种语言的单词表示来弥补高资源和低资源语言之间的差距，即使不使用任何直接的双语信号。这些方法中最大的一部分是基于投影的方法，将预先训练好的嵌入映射到共享的潜在空间中。这些方法大多是基于正交变换，它假设语言向量空间是同构的。然而，这个标准并不一定成立，特别是对于形态学丰富的语言。本文提出了一种自监督的方法来改进无监督双语词嵌入的对齐。该模型将词的向量及其对应的翻译移动得更近，并增强了长度和中心不变性，从而可以更好地对齐跨语言嵌入。实验结果证明了该方法的有效性，因为在大多数情况下，该方法在双语词典归纳任务中都优于最新的方法。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09213">PDF</a>
<h3>No. 4	基于文本游戏的动态知识图学习</h3><h4>Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Côté, Mikuláš Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, William L. Hamilton</h4>摘要：玩基于文本的游戏需要处理自然语言和计划的技巧。虽然代理解决此任务的一个关键目标是在多个博弈中进行泛化，但大多数先前的工作要么集中在解决单个博弈，要么使用基于规则的启发式方法处理泛化。在这项工作中，我们研究以知识图（KG）形式呈现的结构化信息如何有助于有效的规划和概括。我们引入了一种新的基于变压器的序列到序列模型，该模型从环境的原始文本观察构建了一个“信念”KG，当它接收到新的观察时，在每个博弈步骤动态更新该信念图。为了训练该模型建立有用的图表示，我们引入并分析了一组与图相关的预训练任务。我们的经验证明，我们的模型中基于KG的表示有助于代理更快地收敛到多个基于文本的游戏的更好的策略，并且进一步，在未看到的游戏上实现更强的零射击性能。在看不见的游戏上的实验表明，我们的最佳代理比基于文本的基线性能好21.6%。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09127">PDF</a>
<h3>No. 5	随机加权编码器在总结任务中的优异性能</h3><h4>Jonathan Pilault, Jaehong Park, Christopher Pal</h4>文摘：本文研究了一类序列到序列模型中未经训练的随机初始化编码器的性能，并与完全训练的编码器在抽象摘要任务中的性能进行了比较。我们假设输入文本的随机投影有足够的表示能力来编码句子的层次结构和文档的语义。使用经过训练的解码器生成抽象的文本摘要，我们实证地证明，具有未经训练的随机初始化编码器的架构相对于具有完全训练的编码器的等效架构具有竞争力。我们进一步发现，编码器的容量不仅提高了整体模型的泛化能力，而且缩小了未经训练的随机初始化编码器与完全训练编码器之间的性能差距。据我们所知，这是第一次评估具有注意的一般序列到序列模型在抽象摘要上的训练和随机投影表示。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09084">PDF</a>
<h3>No. 6	用反馈存储器访问时序变压器的高级表示</h3><h4>Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, Sainbayar Sukhbaatar</h4>摘要：变压器是一种能并行处理输入令牌的前馈网络。虽然这种并行化使它们的计算效率更高，但它限制了模型充分利用输入的顺序性——给定层的表示只能访问较低层的表示，而不能访问先前时间步骤中已经构建的较高层的表示。在这项工作中，我们提出了反馈转换器架构，它将所有先前的表示公开到所有未来的表示，这意味着当前时间步的最低表示是由过去的最高级别抽象表示形成的。我们在语言建模、神经机器翻译、总结和强化学习的各种基准上证明，增加的表示能力可以改善变压器基线。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09402">PDF</a>
<h3>No. 7	基于关系匹配传播的众包集合实体分解</h3><h4>Jiacheng Huang, Wei Hu, Zhifeng Bao, Yuzhong Qu</h4>摘要：知识库（KBs）存储了丰富而异构的实体和事实。实体分解（ER）旨在识别KBs中引用同一现实对象的实体。最近的研究表明，让人类参与内质网的循环有显著的好处。它们通常利用属性值上的成对相似性度量来解决实体问题，并利用群体来标记不确定的实体。然而，现有的方法仍存在一定程度上的人工成本高、标签不够等问题。在本文中，我们提出了一种新的方法，称为众包集合ER，它利用实体之间的关系来共同而不是独立地推断匹配。具体地说，它迭代地要求人类工作人员标记所选的实体对，并将标记信息传播到距离较远的邻居。在此过程中，我们讨论了候选实体剪枝、概率传播、最优问题选择和容错真理推理等问题。我们在真实数据集上的实验表明，与最新的方法相比，我们的方法以更少的标记获得了更高的精度。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09361">PDF</a>
<h3>No. 8	语言作为一种认知工具在好奇心驱动的探索中想象目标</h3><h4>Cédric Colas, Tristan Karch, Nicolas Lair, Jean-Michel Dussoux, Clément Moulin-Frier, Peter Ford Dominey, Pierre-Yves Oudeyer</h4>摘要：自主强化学习者必须有内在的动机去探索他们的环境，发现潜在的目标，表现它们并学习如何实现它们。当孩子们做同样的事情时，他们也会从语言中受益，利用语言来制定目标，并在学习其意义时想象新的目标。在我们提出的学习架构（想象）中，代理可以自由地探索其环境，并将社交伙伴对有趣交互的自然语言描述转化为潜在目标。想象通过共同学习一个语言模型和一个目标条件奖励函数来学习表达目标。就像人类一样，我们的代理使用语言组合性，通过组合已知的目标来生成新的目标。利用基于深度集和门控注意机制的模块化模型体系结构，IMAGINE可以自主地构建一系列行为，并为各种类型的泛化显示良好的零镜头泛化特性。当想象自己的目标时，代理利用奖励函数的零次泛化来进一步训练想象的目标并改进其行为。我们在一个模拟域中进行实验，在这个域中，代理与包含各种类型和颜色的对象的程序生成的场景交互，发现目标，想象其他场景，并学习如何实现它们。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09253">PDF</a><h2>2020-03-12</h2>
<h3>No. 1	引导注意力进入序列模型以预测对话行为</h3><h4>Pierre Colombo, Emile Chapuis, Matteo Manica, Emmanuel Vignon, Giovanna Varni, Chloe Clavel</h4>文摘：基于会话对话的对话行为预测（DA）任务是会话代理开发的关键组成部分。准确预测DAs需要对会话和全局标记依赖关系进行精确建模。我们利用SEQ2SEQ方法广泛采用的神经机器翻译（NMT），以改善标签序列的建模。已知Seq2seq模型学习复杂的全局依赖性，而目前提出的方法仅使用线性条件随机场（CRF）对局部标记依赖性建模。在这项工作中，我们介绍了一个为DA分类量身定制的seq2seq模型：一个分层编码器、一个新的引导注意机制和一个用于训练和推理的波束搜索。与最先进的技术相比，我们的模型不需要手工制作的功能，并且经过端到端的培训。此外，该方法在SwDA上的准确度得分为85%，在MRDA上的准确度得分为91.6%。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09419">PDF</a>
<h3>No. 2	对齐嵌入空间是一项具有挑战性的任务吗？现有方法分析</h3><h4>Russa Biswas, Mehwish Alam, Harald Sack</h4>文摘：近年来，在低维向量空间中进行词汇和知识图的表示学习及其在现实场景中的应用越来越受到重视。为了将多个KG嵌入应用于知识驱动的应用，如问答、命名实体消歧、知识图完成等，需要对不同的KG嵌入空间进行对齐。除了多语言和特定领域的信息外，不同的KG还带来了结构差异的问题，使得KG嵌入的对齐更具挑战性。本文对两种表示实体和实体词的嵌入空间的对齐方法进行了理论分析和比较。本文还以不同的应用为借口，对现有对准方法的性能和不足进行了评估。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09247">PDF</a>
<h3>No. 3	无监督跨语言单词嵌入的改进</h3><h4>Magdalena Biesialska, Marta R. Costa-jussà</h4>摘要：跨语言单词嵌入旨在通过允许学习多语言的单词表示来弥补高资源语言和低资源语言之间的差距，即使不使用任何直接的双语信号。这些方法中最大的一部分是基于投影的方法，将预先训练好的嵌入映射到共享的潜在空间中。这些方法大多是基于正交变换，它假设语言向量空间是同构的。然而，这个标准并不一定成立，特别是对于形态学丰富的语言。本文提出了一种自监督的方法来改进无监督双语词嵌入的对齐。该模型将词的向量及其对应的翻译移动得更近，并增强了长度和中心不变性，从而可以更好地对齐跨语言嵌入。实验结果证明了该方法的有效性，因为在大多数情况下，该方法在双语词典归纳任务中都优于最新的方法。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09213">PDF</a>
<h3>No. 4	基于文本游戏的动态知识图学习</h3><h4>Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Côté, Mikuláš Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, William L. Hamilton</h4>摘要：玩基于文本的游戏需要处理自然语言和计划的技巧。虽然代理解决此任务的一个关键目标是在多个博弈中进行泛化，但大多数先前的工作要么集中在解决单个博弈，要么使用基于规则的启发式方法处理泛化。在这项工作中，我们研究以知识图（KG）形式呈现的结构化信息如何有助于有效的规划和概括。我们引入了一种新的基于变压器的序列到序列模型，该模型从环境的原始文本观察构建了一个“信念”KG，当它接收到新的观察时，在每个博弈步骤动态更新该信念图。为了训练该模型建立有用的图表示，我们引入并分析了一组与图相关的预训练任务。我们的经验证明，我们的模型中基于KG的表示有助于代理更快地收敛到多个基于文本的游戏的更好的策略，并且进一步，在未看到的游戏上实现更强的零射击性能。在看不见的游戏上的实验表明，我们的最佳代理比基于文本的基线性能好21.6%。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09127">PDF</a>
<h3>No. 5	随机加权编码器在总结任务中的优异性能</h3><h4>Jonathan Pilault, Jaehong Park, Christopher Pal</h4>文摘：本文研究了一类序列到序列模型中未经训练的随机初始化编码器的性能，并与完全训练的编码器在抽象摘要任务中的性能进行了比较。我们假设输入文本的随机投影有足够的表示能力来编码句子的层次结构和文档的语义。使用经过训练的解码器生成抽象的文本摘要，我们实证地证明，具有未经训练的随机初始化编码器的架构相对于具有完全训练的编码器的等效架构具有竞争力。我们进一步发现，编码器的容量不仅提高了整体模型的泛化能力，而且缩小了未经训练的随机初始化编码器与完全训练编码器之间的性能差距。据我们所知，这是第一次评估具有注意的一般序列到序列模型在抽象摘要上的训练和随机投影表示。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09084">PDF</a>
<h3>No. 6	用反馈存储器访问时序变压器的高级表示</h3><h4>Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, Sainbayar Sukhbaatar</h4>摘要：变压器是一种能并行处理输入令牌的前馈网络。虽然这种并行化使它们的计算效率更高，但它限制了模型充分利用输入的顺序性——给定层的表示只能访问较低层的表示，而不能访问先前时间步骤中已经构建的较高层的表示。在这项工作中，我们提出了反馈转换器架构，它将所有先前的表示公开到所有未来的表示，这意味着当前时间步的最低表示是由过去的最高级别抽象表示形成的。我们在语言建模、神经机器翻译、总结和强化学习的各种基准上证明，增加的表示能力可以改善变压器的基准线。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09402">PDF</a>
<h3>No. 7	基于关系匹配传播的众包集合实体分解</h3><h4>Jiacheng Huang, Wei Hu, Zhifeng Bao, Yuzhong Qu</h4>摘要：知识库（KBs）存储了丰富而异构的实体和事实。实体分解（ER）旨在识别KBs中引用同一现实对象的实体。最近的研究表明，让人类参与内质网的循环有显著的好处。它们通常利用属性值上的成对相似度量来解决实体问题，并利用群体来标记不确定的实体。然而，现有的方法仍存在一定程度上的人工成本高、标签不够等问题。在本文中，我们提出了一种新的方法，称为众包集合ER，它利用实体之间的关系来共同而不是独立地推断匹配。具体地说，它迭代地要求人类工作人员标记所选的实体对，并将标记信息传播到距离较远的邻居。在此过程中，我们讨论了候选实体剪枝、概率传播、最优问题选择和容错真理推理等问题。我们在真实数据集上的实验表明，与最新的方法相比，我们的方法以更少的标记获得了更高的精度。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09361">PDF</a>
<h3>No. 8	语言作为一种认知工具在好奇心驱动的探索中想象目标</h3><h4>Cédric Colas, Tristan Karch, Nicolas Lair, Jean-Michel Dussoux, Clément Moulin-Frier, Peter Ford Dominey, Pierre-Yves Oudeyer</h4>摘要：自主强化学习者必须有内在的动机去探索他们的环境，发现潜在的目标，表现它们并学习如何实现它们。当孩子们做同样的事情时，他们也会从语言中受益，利用语言来制定目标，并在学习其意义时想象新的目标。在我们提出的学习架构（想象）中，代理可以自由地探索其环境，并将社交伙伴对有趣交互的自然语言描述转化为潜在目标。想象通过共同学习一个语言模型和一个目标条件奖励函数来学习表达目标。就像人类一样，我们的代理使用语言组合性，通过组合已知的目标来生成新的目标。利用基于深度集和门控注意机制的模块化模型体系结构，IMAGINE可以自主地构建一系列行为，并为各种类型的泛化显示良好的零镜头泛化特性。当想象自己的目标时，代理利用奖励函数的零次泛化来进一步训练想象的目标并改进其行为。我们在一个模拟域中进行实验，在这个域中，代理与包含各种类型和颜色的对象的程序生成的场景交互，发现目标，想象其他场景，并学习如何实现它们。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09253">PDF</a><h2>2020-03-11</h2>
<h3>No. 1	引导注意力进入序列模型以预测对话行为</h3><h4>Pierre Colombo, Emile Chapuis, Matteo Manica, Emmanuel Vignon, Giovanna Varni, Chloe Clavel</h4>文摘：基于会话对话的对话行为预测（DA）任务是会话代理开发的关键组成部分。准确预测DAs需要对会话和全局标记依赖关系进行精确建模。我们利用SEQ2SEQ方法广泛采用的神经机器翻译（NMT），以改善标签序列的建模。已知Seq2seq模型学习复杂的全局依赖性，而目前提出的方法仅使用线性条件随机场（CRF）对局部标记依赖性建模。在这项工作中，我们介绍了一个为DA分类量身定制的seq2seq模型：一个分层编码器、一个新的引导注意机制和一个用于训练和推理的波束搜索。与最先进的技术相比，我们的模型不需要手工制作的功能，并且经过端到端的培训。此外，该方法在SwDA上的准确度得分为85%，在MRDA上的准确度得分为91.6%。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09419">PDF</a>
<h3>No. 2	对齐嵌入空间是一项具有挑战性的任务吗？现有方法分析</h3><h4>Russa Biswas, Mehwish Alam, Harald Sack</h4>文摘：近年来，在低维向量空间中进行词汇和知识图的表示学习及其在现实场景中的应用越来越受到重视。为了将多个KG嵌入应用于知识驱动的应用，如问答、命名实体消歧、知识图完成等，需要对不同的KG嵌入空间进行对齐。除了多语言和特定领域的信息外，不同的KG还带来了结构差异的问题，使得KG嵌入的对齐更具挑战性。本文对两种表示实体和实体词的嵌入空间的对齐方法进行了理论分析和比较。本文还以不同的应用为借口，对现有对准方法的性能和不足进行了评估。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09247">PDF</a>
<h3>No. 3	无监督跨语言单词嵌入的改进</h3><h4>Magdalena Biesialska, Marta R. Costa-jussà</h4>摘要：跨语言单词嵌入旨在通过允许学习多种语言的单词表示来弥补高资源和低资源语言之间的差距，即使不使用任何直接的双语信号。这些方法中最大的一部分是基于投影的方法，将预先训练好的嵌入映射到共享的潜在空间中。这些方法大多是基于正交变换，它假定语言向量空间是同构的。然而，这个标准并不一定成立，特别是对于形态学丰富的语言。本文提出了一种自监督的方法来改进无监督双语词嵌入的对齐。该模型将词的向量及其对应的翻译移动得更近，并增强了长度和中心不变性，从而可以更好地对齐跨语言嵌入。实验结果证明了该方法的有效性，因为在大多数情况下，该方法在双语词典归纳任务中都优于最新的方法。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09213">PDF</a>
<h3>No. 4	基于文本游戏的动态知识图学习</h3><h4>Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Côté, Mikuláš Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, William L. Hamilton</h4>摘要：玩基于文本的游戏需要处理自然语言和计划的技巧。虽然代理解决此任务的一个关键目标是在多个博弈中进行泛化，但大多数先前的工作要么集中在解决单个博弈，要么使用基于规则的启发式方法处理泛化。在这项工作中，我们研究以知识图（KG）形式呈现的结构化信息如何有助于有效的规划和概括。我们引入了一种新的基于变换器的序列到序列模型，该模型从环境的原始文本观测值中构造一个“信念”KG，当它接收到新的观测值时，在每个游戏步骤中动态地更新这个信念图。为了训练该模型建立有用的图表示，我们引入并分析了一组与图相关的预训练任务。我们的经验证明，我们的模型中基于KG的表示有助于代理更快地收敛到多个基于文本的游戏的更好的策略，并且进一步，在未看到的游戏上实现更强的零射击性能。在看不见的游戏上的实验表明，我们的最佳代理比基于文本的基线性能好21.6%。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09127">PDF</a>
<h3>No. 5	随机加权编码器在总结任务中的优异性能</h3><h4>Jonathan Pilault, Jaehong Park, Christopher Pal</h4>文摘：本文研究了一类序列到序列模型中未经训练的随机初始化编码器的性能，并与完全训练的编码器在抽象摘要任务中的性能进行了比较。我们假设输入文本的随机投影有足够的表示能力来编码句子的层次结构和文档的语义。使用经过训练的解码器生成抽象的文本摘要，我们实证地证明，具有未经训练的随机初始化编码器的架构相对于具有完全训练的编码器的等效架构具有竞争力。我们进一步发现，编码器的容量不仅提高了整体模型的泛化能力，而且缩小了未经训练的随机初始化编码器与完全训练编码器之间的性能差距。据我们所知，这是第一次评估具有注意的一般序列到序列模型在抽象摘要上的训练和随机投影表示。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09084">PDF</a>
<h3>No. 6	用反馈存储器访问时序变压器的高级表示</h3><h4>Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, Sainbayar Sukhbaatar</h4>摘要：变压器是一种能并行处理输入令牌的前馈网络。虽然这种并行化使它们的计算效率更高，但它限制了模型充分利用输入的顺序性——给定层的表示只能访问较低层的表示，而不能访问先前时间步骤中已经构建的较高层的表示。在这项工作中，我们提出了反馈转换器架构，它将所有先前的表示公开到所有未来的表示，这意味着当前时间步的最低表示是由过去的最高级别抽象表示形成的。我们在语言建模、神经机器翻译、总结和强化学习的各种基准上证明，增加的表示能力可以改善变压器基线。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09402">PDF</a>
<h3>No. 7	基于关系匹配传播的众包集合实体分解</h3><h4>Jiacheng Huang, Wei Hu, Zhifeng Bao, Yuzhong Qu</h4>摘要：知识库（KBs）存储了丰富而异构的实体和事实。实体分解（ER）旨在识别KBs中引用同一现实对象的实体。最近的研究表明，让人类参与内质网的循环有显著的好处。它们通常利用属性值上的成对相似度量来解决实体问题，并利用群体来标记不确定的实体。然而，现有的方法仍存在一定程度上的人工成本高、标签不够等问题。在本文中，我们提出了一种新的方法，称为众包集合ER，它利用实体之间的关系来共同而不是独立地推断匹配。具体地说，它迭代地要求人类工作人员标记所选的实体对，并将标记信息传播到距离较远的邻居。在此过程中，我们讨论了候选实体剪枝、概率传播、最优问题选择和容错真理推理等问题。我们在真实数据集上的实验表明，与最新的方法相比，我们的方法以更少的标记获得了更高的精度。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09361">PDF</a>
<h3>No. 8	语言作为一种认知工具在好奇心驱动的探索中想象目标</h3><h4>Cédric Colas, Tristan Karch, Nicolas Lair, Jean-Michel Dussoux, Clément Moulin-Frier, Peter Ford Dominey, Pierre-Yves Oudeyer</h4>摘要：自主强化学习者必须有内在的动机去探索他们的环境，发现潜在的目标，表现它们并学习如何实现它们。当孩子们做同样的事情时，他们也会从语言中受益，利用语言来制定目标，并在学习其意义时想象新的目标。在我们提出的学习架构（想象）中，代理可以自由地探索其环境，并将社交伙伴对有趣交互的自然语言描述转化为潜在目标。想象通过共同学习一个语言模型和一个目标条件奖励函数来学习表达目标。就像人类一样，我们的代理使用语言组合性，通过组合已知的目标来生成新的目标。利用基于深度集和门控注意机制的模块化模型体系结构，IMAGINE可以自主地构建一系列行为，并为各种类型的泛化显示良好的零镜头泛化特性。当想象自己的目标时，代理利用奖励函数的零次泛化来进一步训练想象的目标并改进其行为。我们在一个模拟域中进行实验，在这个域中，代理与包含各种类型和颜色的对象的程序生成的场景交互，发现目标，想象其他场景，并学习如何实现它们。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09253">PDF</a><h2>2020-03-10</h2>
<h3>No. 1	引导注意力进入序列模型以预测对话行为</h3><h4>Pierre Colombo, Emile Chapuis, Matteo Manica, Emmanuel Vignon, Giovanna Varni, Chloe Clavel</h4>摘要：基于会话对话的对话行为预测任务是会话代理开发的关键组成部分。准确预测DAs需要对会话和全局标记依赖关系进行精确建模。我们利用SEQ2SEQ方法广泛采用的神经机器翻译（NMT），以改善标签序列的建模。已知Seq2seq模型学习复杂的全局依赖性，而目前提出的方法仅使用线性条件随机场（CRF）对局部标记依赖性建模。在这项工作中，我们介绍了一个为DA分类量身定制的seq2seq模型：一个分层编码器、一个新的引导注意机制和一个用于训练和推理的波束搜索。与最先进的技术相比，我们的模型不需要手工制作的功能，并且经过端到端的培训。此外，该方法在SwDA上的准确度得分为85%，在MRDA上的准确度得分为91.6%。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09419">PDF</a>
<h3>No. 2	对齐嵌入空间是一项具有挑战性的任务吗？现有方法分析</h3><h4>Russa Biswas, Mehwish Alam, Harald Sack</h4>文摘：近年来，在低维向量空间中进行词汇和知识图的表示学习及其在现实场景中的应用越来越受到重视。为了将多个KG嵌入应用于知识驱动的应用，如问答、命名实体消歧、知识图完成等，需要对不同的KG嵌入空间进行对齐。除了多语言和特定领域的信息外，不同的KG还带来了结构差异的问题，使得KG嵌入的对齐更具挑战性。本文对两种表示实体和实体词的嵌入空间的对齐方法进行了理论分析和比较。本文还以不同的应用为借口，对现有对准方法的性能和不足进行了评估。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09247">PDF</a>
<h3>No. 3	无监督跨语言单词嵌入的改进</h3><h4>Magdalena Biesialska, Marta R. Costa-jussà</h4>摘要：跨语言单词嵌入旨在通过允许学习多语言的单词表示来弥补高资源语言和低资源语言之间的差距，即使不使用任何直接的双语信号。这些方法中最大的一部分是基于投影的方法，将预先训练好的嵌入映射到共享的潜在空间中。这些方法大多是基于正交变换，它假设语言向量空间是同构的。然而，这个标准并不一定成立，特别是对于形态学丰富的语言。本文提出了一种自监督的方法来改进无监督双语词嵌入的对齐。该模型将词的向量及其对应的翻译移动得更近，并增强了长度和中心不变性，从而可以更好地对齐跨语言嵌入。实验结果证明了该方法的有效性，因为在大多数情况下，该方法在双语词典归纳任务中都优于最新的方法。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09213">PDF</a>
<h3>No. 4	基于文本游戏的动态知识图学习</h3><h4>Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Côté, Mikuláš Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, William L. Hamilton</h4>摘要：玩基于文本的游戏需要处理自然语言和计划的技巧。虽然代理解决此任务的一个关键目标是在多个博弈中进行泛化，但大多数先前的工作要么集中在解决单个博弈，要么使用基于规则的启发式方法处理泛化。在这项工作中，我们研究以知识图（KG）形式呈现的结构化信息如何有助于有效的规划和概括。我们引入了一种新的基于变压器的序列到序列模型，该模型从环境的原始文本观察构建了一个“信念”KG，当它接收到新的观察时，在每个博弈步骤动态更新该信念图。为了训练该模型建立有用的图表示，我们引入并分析了一组与图相关的预训练任务。我们的经验证明，我们的模型中基于KG的表示有助于代理更快地收敛到多个基于文本的游戏的更好的策略，并且进一步，在未看到的游戏上实现更强的零射击性能。在看不见的游戏上的实验表明，我们的最佳代理比基于文本的基线性能好21.6%。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09127">PDF</a>
<h3>No. 5	随机加权编码器在总结任务中的优异性能</h3><h4>Jonathan Pilault, Jaehong Park, Christopher Pal</h4>文摘：本文研究了一类序列到序列模型中未经训练的随机初始化编码器的性能，并与完全训练的编码器在抽象摘要任务中的性能进行了比较。我们假设输入文本的随机投影有足够的表示能力来编码句子的层次结构和文档的语义。使用经过训练的解码器生成抽象的文本摘要，我们实证地证明，具有未经训练的随机初始化编码器的架构相对于具有完全训练的编码器的等效架构具有竞争力。我们进一步发现，编码器的容量不仅提高了整体模型的泛化能力，而且缩小了未经训练的随机初始化编码器与完全训练编码器之间的性能差距。据我们所知，这是第一次评估具有注意的一般序列到序列模型在抽象摘要上的训练和随机投影表示。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09084">PDF</a>
<h3>No. 6	用反馈存储器访问时序变压器的高级表示</h3><h4>Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, Sainbayar Sukhbaatar</h4>摘要：变压器是一种能并行处理输入令牌的前馈网络。虽然这种并行化使它们的计算效率更高，但它限制了模型充分利用输入的顺序性——给定层的表示只能访问较低层的表示，而不能访问先前时间步骤中已经构建的较高层的表示。在这项工作中，我们提出了反馈转换器架构，它将所有先前的表示公开到所有未来的表示，这意味着当前时间步的最低表示是由过去的最高级别抽象表示形成的。我们在语言建模、神经机器翻译、总结和强化学习的各种基准上证明，增加的表示能力可以改善变压器基线。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09402">PDF</a>
<h3>No. 7	基于关系匹配传播的众包集合实体分解</h3><h4>Jiacheng Huang, Wei Hu, Zhifeng Bao, Yuzhong Qu</h4>摘要：知识库（KBs）存储了丰富而异构的实体和事实。实体分解（ER）旨在识别KBs中引用同一现实对象的实体。最近的研究表明，让人类参与内质网的循环具有显著的好处。它们通常利用属性值上的成对相似度量来解决实体问题，并利用群体来标记不确定的实体。然而，现有的方法仍存在一定程度上的人工成本高、标签不够等问题。在本文中，我们提出了一种新的方法，称为众包集合ER，它利用实体之间的关系来共同而不是独立地推断匹配。具体地说，它迭代地要求人类工作人员标记所选的实体对，并将标记信息传播到距离较远的邻居。在此过程中，我们讨论了候选实体剪枝、概率传播、最优问题选择和容错真理推理等问题。我们在真实数据集上的实验表明，与最新的方法相比，我们的方法以更少的标记获得了更高的精度。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09361">PDF</a>
<h3>No. 8	语言作为一种认知工具在好奇心驱动的探索中想象目标</h3><h4>Cédric Colas, Tristan Karch, Nicolas Lair, Jean-Michel Dussoux, Clément Moulin-Frier, Peter Ford Dominey, Pierre-Yves Oudeyer</h4>摘要：自主强化学习者必须有内在的动机去探索他们的环境，发现潜在的目标，表现它们并学习如何实现它们。当孩子们做同样的事情时，他们也会从语言中受益，利用语言来制定目标，并在学习其意义时想象新的目标。在我们提出的学习架构（想象）中，代理可以自由地探索其环境，并将社交伙伴对有趣交互的自然语言描述转化为潜在目标。想象通过共同学习一个语言模型和一个目标条件奖励函数来学习表达目标。就像人类一样，我们的代理使用语言组合性，通过组合已知的目标来生成新的目标。利用基于深度集和门控注意机制的模块化模型体系结构，IMAGINE可以自主地构建一系列行为，并为各种类型的泛化显示良好的零炮泛化特性。当想象自己的目标时，代理利用奖励函数的零次泛化来进一步训练想象的目标并改进其行为。我们在一个模拟域中进行实验，在这个域中，代理与包含各种类型和颜色的对象的程序生成的场景交互，发现目标，想象其他场景，并学习如何实现它们。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09253">PDF</a><h2>2020-03-09</h2>
<h3>No. 1	引导注意力进入序列模型以预测对话行为</h3><h4>Pierre Colombo, Emile Chapuis, Matteo Manica, Emmanuel Vignon, Giovanna Varni, Chloe Clavel</h4>文摘：基于会话对话的对话行为预测（DA）任务是会话代理开发的关键组成部分。准确预测DAs需要对会话和全局标记依赖关系进行精确建模。我们利用SEQ2SEQ方法广泛采用的神经机器翻译（NMT），以改善标签序列的建模。已知Seq2seq模型学习复杂的全局依赖性，而目前提出的方法仅使用线性条件随机场（CRF）对局部标记依赖性建模。在这项工作中，我们介绍了一个为DA分类量身定制的seq2seq模型：一个分层编码器、一个新的引导注意机制和一个用于训练和推理的波束搜索。与最先进的技术相比，我们的模型不需要手工制作的功能，并且经过端到端的培训。此外，该方法在SwDA上的准确度得分为85%，在MRDA上的准确度得分为91.6%。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09419">PDF</a>
<h3>No. 2	对齐嵌入空间是一项具有挑战性的任务吗？现有方法分析</h3><h4>Russa Biswas, Mehwish Alam, Harald Sack</h4>文摘：近年来，在低维向量空间中进行词汇和知识图的表示学习及其在现实场景中的应用越来越受到重视。为了将多个KG嵌入应用于知识驱动的应用，如问答、命名实体消歧、知识图完成等，需要对不同的KG嵌入空间进行对齐。除了多语言和特定领域的信息外，不同的KG还带来了结构差异的问题，使得KG嵌入的对齐更具挑战性。本文对两种表示实体和实体词的嵌入空间的对齐方法进行了理论分析和比较。本文还以不同的应用为借口，对现有对准方法的性能和不足进行了评估。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09247">PDF</a>
<h3>No. 3	无监督跨语言单词嵌入的改进</h3><h4>Magdalena Biesialska, Marta R. Costa-jussà</h4>摘要：跨语言单词嵌入旨在通过允许学习多语言的单词表示来弥补高资源语言和低资源语言之间的差距，即使不使用任何直接的双语信号。这些方法中最大的一部分是基于投影的方法，将预先训练好的嵌入映射到共享的潜在空间中。这些方法大多是基于正交变换，它假设语言向量空间是同构的。然而，这个标准并不一定成立，特别是对于形态学丰富的语言。本文提出了一种自监督的方法来改进无监督双语词嵌入的对齐。该模型将词的向量及其对应的翻译移动得更近，并增强了长度和中心不变性，从而可以更好地对齐跨语言嵌入。实验结果证明了该方法的有效性，因为在大多数情况下，该方法在双语词典归纳任务中都优于最新的方法。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09213">PDF</a>
<h3>No. 4	基于文本游戏的动态知识图学习</h3><h4>Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Côté, Mikuláš Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, William L. Hamilton</h4>摘要：玩基于文本的游戏需要处理自然语言和计划的技巧。虽然代理解决此任务的一个关键目标是在多个博弈中进行泛化，但大多数先前的工作要么集中在解决单个博弈，要么使用基于规则的启发式方法处理泛化。在这项工作中，我们研究以知识图（KG）形式呈现的结构化信息如何有助于有效的规划和概括。我们引入了一种新的基于变压器的序列到序列模型，该模型从环境的原始文本观察构建了一个“信念”KG，当它接收到新的观察时，在每个博弈步骤动态更新该信念图。为了训练该模型建立有用的图表示，我们引入并分析了一组与图相关的预训练任务。我们的经验证明，我们的模型中基于KG的表示有助于代理更快地收敛到多个基于文本的游戏的更好的策略，并且进一步，在未看到的游戏上实现更强的零射击性能。在看不见的游戏上的实验表明，我们的最佳代理比基于文本的基线性能好21.6%。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09127">PDF</a>
<h3>No. 5	随机加权编码器在总结任务中的优异性能</h3><h4>Jonathan Pilault, Jaehong Park, Christopher Pal</h4>文摘：本文研究了一类序列到序列模型中未经训练的随机初始化编码器的性能，并与完全训练的编码器在抽象摘要任务中的性能进行了比较。我们假设输入文本的随机投影有足够的表示能力来编码句子的层次结构和文档的语义。使用经过训练的解码器生成抽象的文本摘要，我们实证地证明，具有未经训练的随机初始化编码器的架构相对于具有完全训练的编码器的等效架构具有竞争力。我们进一步发现，编码器的容量不仅提高了整体模型的泛化能力，而且缩小了未经训练的随机初始化编码器与完全训练编码器之间的性能差距。据我们所知，这是第一次评估具有注意的一般序列到序列模型在抽象摘要上的训练和随机投影表示。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09084">PDF</a>
<h3>No. 6	用反馈存储器访问时序变压器的高级表示</h3><h4>Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, Sainbayar Sukhbaatar</h4>摘要：变压器是一种能并行处理输入令牌的前馈网络。虽然这种并行化使它们的计算效率更高，但它限制了模型充分利用输入的顺序性——给定层的表示只能访问较低层的表示，而不能访问先前时间步骤中已经构建的较高层的表示。在这项工作中，我们提出了反馈转换器架构，它将所有先前的表示公开到所有未来的表示，这意味着当前时间步的最低表示是由过去的最高级别抽象表示形成的。我们在语言建模、神经机器翻译、总结和强化学习的各种基准上证明，增加的表示能力可以改善变压器基线。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09402">PDF</a>
<h3>No. 7	基于关系匹配传播的众包集合实体分解</h3><h4>Jiacheng Huang, Wei Hu, Zhifeng Bao, Yuzhong Qu</h4>摘要：知识库（KBs）存储了丰富而异构的实体和事实。实体分解（ER）旨在识别KBs中引用同一现实对象的实体。最近的研究表明，让人类参与内质网的循环有显著的好处。它们通常利用属性值上的成对相似度量来解决实体问题，并利用群体来标记不确定的实体。然而，现有的方法仍存在一定程度上的人工成本高、标签不够等问题。在本文中，我们提出了一种新的方法，称为众包集合ER，它利用实体之间的关系来共同而不是独立地推断匹配。具体地说，它迭代地要求人类工作人员标记所选的实体对，并将标记信息传播到距离较远的邻居。在此过程中，我们讨论了候选实体剪枝、概率传播、最优问题选择和容错真理推理等问题。我们在真实数据集上的实验表明，与最新的方法相比，我们的方法以更少的标记获得了更高的精度。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09361">PDF</a>
<h3>No. 8	语言作为一种认知工具在好奇心驱动的探索中想象目标</h3><h4>Cédric Colas, Tristan Karch, Nicolas Lair, Jean-Michel Dussoux, Clément Moulin-Frier, Peter Ford Dominey, Pierre-Yves Oudeyer</h4>摘要：自主强化学习者必须有内在的动机去探索他们的环境，发现潜在的目标，表现它们并学习如何实现它们。当孩子们做同样的事情时，他们也会从语言中受益，利用语言来制定目标，并在学习其意义时想象新的目标。在我们提出的学习架构（想象）中，代理可以自由地探索其环境，并将社交伙伴对有趣交互的自然语言描述转化为潜在目标。想象通过共同学习一个语言模型和一个目标条件奖励函数来学习表达目标。就像人类一样，我们的代理使用语言组合性，通过组合已知的目标来生成新的目标。利用基于深度集和门控注意机制的模块化模型体系结构，IMAGINE可以自主地构建一系列行为，并为各种类型的泛化显示良好的零镜头泛化特性。当想象自己的目标时，代理利用奖励函数的零次泛化来进一步训练想象的目标并改进其行为。我们在一个模拟域中进行实验，在这个域中，代理与包含各种类型和颜色的对象的程序生成的场景交互，发现目标，想象其他场景，并学习如何实现它们。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09253">PDF</a><h2>2020-03-08</h2>
<h3>No. 1	引导注意力进入序列模型以预测对话行为</h3><h4>Pierre Colombo, Emile Chapuis, Matteo Manica, Emmanuel Vignon, Giovanna Varni, Chloe Clavel</h4>文摘：基于会话对话的对话行为预测（DA）任务是会话代理开发的关键组成部分。准确预测DAs需要对会话和全局标记依赖关系进行精确建模。我们利用SEQ2SEQ方法广泛采用的神经机器翻译（NMT），以改善标签序列的建模。已知Seq2seq模型学习复杂的全局依赖性，而目前提出的方法仅使用线性条件随机场（CRF）对局部标记依赖性建模。在这项工作中，我们介绍了一个为DA分类量身定制的seq2seq模型：一个分层编码器、一个新的引导注意机制和一个用于训练和推理的波束搜索。与最先进的技术相比，我们的模型不需要手工制作的功能，并且经过端到端的培训。此外，该方法在SwDA上的准确度得分为85%，在MRDA上的准确度得分为91.6%。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09419">PDF</a>
<h3>No. 2	对齐嵌入空间是一项具有挑战性的任务吗？现有方法分析</h3><h4>Russa Biswas, Mehwish Alam, Harald Sack</h4>文摘：近年来，在低维向量空间中进行词汇和知识图的表示学习及其在现实场景中的应用越来越受到重视。为了将多个KG嵌入应用于知识驱动的应用，如问答、命名实体消歧、知识图完成等，需要对不同的KG嵌入空间进行对齐。除了多语言和特定领域的信息外，不同的KG还带来了结构差异的问题，使得KG嵌入的对齐更具挑战性。本文对两种表示实体和实体词的嵌入空间的对齐方法进行了理论分析和比较。本文还以不同的应用为借口，对现有对准方法的性能和不足进行了评估。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09247">PDF</a>
<h3>No. 3	无监督跨语言单词嵌入的改进</h3><h4>Magdalena Biesialska, Marta R. Costa-jussà</h4>摘要：跨语言单词嵌入旨在通过允许学习多种语言的单词表示来弥补高资源和低资源语言之间的差距，即使不使用任何直接的双语信号。这些方法中最大的一部分是基于投影的方法，将预先训练好的嵌入映射到共享的潜在空间中。这些方法大多是基于正交变换，它假设语言向量空间是同构的。然而，这个标准并不一定成立，特别是对于形态学丰富的语言。本文提出了一种自监督的方法来改进无监督双语词嵌入的对齐。该模型将词的向量及其对应的翻译移动得更近，并增强了长度和中心不变性，从而可以更好地对齐跨语言嵌入。实验结果证明了该方法的有效性，因为在大多数情况下，该方法在双语词典归纳任务中都优于最新的方法。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09213">PDF</a>
<h3>No. 4	基于文本游戏的动态知识图学习</h3><h4>Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Côté, Mikuláš Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, William L. Hamilton</h4>摘要：玩基于文本的游戏需要处理自然语言和计划的技巧。虽然代理解决此任务的一个关键目标是在多个博弈中进行泛化，但大多数先前的工作要么集中在解决单个博弈，要么使用基于规则的启发式方法处理泛化。在这项工作中，我们研究以知识图（KG）形式呈现的结构化信息如何有助于有效的规划和概括。我们引入了一种新的基于变压器的序列到序列模型，该模型从环境的原始文本观察构建了一个“信念”KG，当它接收到新的观察时，在每个博弈步骤动态更新该信念图。为了训练该模型建立有用的图表示，我们引入并分析了一组与图相关的预训练任务。我们的经验证明，我们的模型中基于KG的表示有助于代理更快地收敛到多个基于文本的游戏的更好的策略，并且进一步，在未看到的游戏上实现更强的零射击性能。在看不见的游戏上的实验表明，我们的最佳代理比基于文本的基线性能好21.6%。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09127">PDF</a>
<h3>No. 5	随机加权编码器在总结任务中的优异性能</h3><h4>Jonathan Pilault, Jaehong Park, Christopher Pal</h4>文摘：本文研究了一类序列到序列模型中未经训练的随机初始化编码器的性能，并与完全训练的编码器在抽象摘要任务中的性能进行了比较。我们假设输入文本的随机投影有足够的表示能力来编码句子的层次结构和文档的语义。使用经过训练的解码器生成抽象的文本摘要，我们实证地证明，具有未经训练的随机初始化编码器的架构相对于具有完全训练的编码器的等效架构具有竞争力。我们进一步发现，编码器的容量不仅提高了整体模型的泛化能力，而且缩小了未经训练的随机初始化编码器与完全训练编码器之间的性能差距。据我们所知，这是第一次评估具有注意的一般序列到序列模型在抽象摘要上的训练和随机投影表示。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09084">PDF</a>
<h3>No. 6	用反馈存储器访问时序变压器的高级表示</h3><h4>Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, Sainbayar Sukhbaatar</h4>摘要：变压器是一种能并行处理输入令牌的前馈网络。虽然这种并行化使它们的计算效率更高，但它限制了模型充分利用输入的顺序性——给定层的表示只能访问较低层的表示，而不能访问先前时间步骤中已经构建的较高层的表示。在这项工作中，我们提出了反馈转换器架构，它将所有先前的表示公开到所有未来的表示，这意味着当前时间步的最低表示是由过去的最高级别抽象表示形成的。我们在语言建模、神经机器翻译、总结和强化学习的各种基准上证明，增加的表示能力可以改善变压器基线。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09402">PDF</a>
<h3>No. 7	基于关系匹配传播的众包集合实体分解</h3><h4>Jiacheng Huang, Wei Hu, Zhifeng Bao, Yuzhong Qu</h4>摘要：知识库（KBs）存储了丰富而异构的实体和事实。实体分解（ER）旨在识别KBs中引用同一现实对象的实体。最近的研究表明，让人类参与内质网的循环有显著的好处。它们通常利用属性值上的成对相似度量来解决实体问题，并利用群体来标记不确定的实体。然而，现有的方法仍存在一定程度上的人工成本高、标签不够等问题。在本文中，我们提出了一种新的方法，称为众包集合ER，它利用实体之间的关系来共同而不是独立地推断匹配。具体地说，它迭代地要求人类工作人员标记所选的实体对，并将标记信息传播到距离较远的邻居。在此过程中，我们讨论了候选实体剪枝、概率传播、最优问题选择和容错真理推理等问题。我们在真实数据集上的实验表明，与最新的方法相比，我们的方法以更少的标记获得了更高的精度。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09361">PDF</a>
<h3>No. 8	语言作为一种认知工具在好奇心驱动的探索中想象目标</h3><h4>Cédric Colas, Tristan Karch, Nicolas Lair, Jean-Michel Dussoux, Clément Moulin-Frier, Peter Ford Dominey, Pierre-Yves Oudeyer</h4>摘要：自主强化学习者必须有内在的动机去探索他们的环境，发现潜在的目标，表现它们并学习如何实现它们。当孩子们做同样的事情时，他们也会从语言中受益，利用语言来制定目标，并在学习其意义时想象新的目标。在我们提出的学习架构（想象）中，代理可以自由地探索其环境，并将社交伙伴对有趣交互的自然语言描述转化为潜在目标。想象通过共同学习一个语言模型和一个目标条件奖励函数来学习表达目标。就像人类一样，我们的代理使用语言组合性，通过组合已知的目标来生成新的目标。利用基于深度集和门控注意机制的模块化模型体系结构，IMAGINE可以自主地构建一系列行为，并为各种类型的泛化显示良好的零镜头泛化特性。当想象自己的目标时，代理利用奖励函数的零次泛化来进一步训练想象的目标并改进其行为。我们在一个模拟域中进行实验，在这个域中，代理与包含各种类型和颜色的对象的程序生成的场景交互，发现目标，想象其他场景，并学习如何实现它们。<br><a href = "http://xxx.itp.ac.cn/pdf/2002.09253">PDF</a>
</body></html>