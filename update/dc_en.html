<!DOCTYPE html><html><head><meta charset="utf-8"><title>Distributed, Parallel, and Cluster Computing  authors/titles recent submissions</title></head><body>
<h2>2020-03-13</h2>
<h3>No. 1	Fast Implementation of Morphological Filtering Using ARM NEON Extension</h3><h4>Elena Limonova, Arseny Terekhin, Dmitry Nikolaev, Vladimir Arlazarov</h4> Abstract: In this paper we consider speedup potential of morphological image filtering on ARM processors. Morphological operations are widely used in image analysis and recognition and their speedup in some cases can significantly reduce overall execution time of recognition. More specifically, we propose fast implementation of erosion and dilation using ARM SIMD extension NEON. These operations with the rectangular structuring element are separable. They were implemented using the advantages of separability as sequential horizontal and vertical passes. Each pass was implemented using van Herk/Gil-Werman algorithm for large windows and low-constant linear complexity algorithm for small windows. Final implementation was improved with SIMD and used a combination of these methods. We also considered fast transpose implementation of 8x8 and 16x16 matrices using ARM NEON to get additional computational gain for morphological operations. Experiments showed 3 times efficiency increase for final implementation of erosion and dilation compared to van Herk/Gil-Werman algorithm without SIMD, 5.7 times speedup for 8x8 matrix transpose and 12 times speedup for 16x16 matrix transpose compared to transpose without SIMD. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09474">PDF</a>
<h3>No. 2	Faasm: Lightweight Isolation for Efficient Stateful Serverless Computing</h3><h4>Simon Shillaker, Peter Pietzuch</h4> Abstract: Serverless computing is an excellent fit for big data processing because it can scale quickly and cheaply to thousands of parallel functions. Existing serverless platforms, however, isolate functions in ephemeral, stateless containers. This means that functions cannot efficiently share memory, forcing users to serialise data repeatedly when composing functions. We observe that container-based isolation is ill-suited for serverless big data processing. Instead it requires a lightweight isolation approach that allows for efficient state sharing. We introduce Faaslets, a new isolation abstraction for serverless big data computing. Faaslets isolate the memory of executed functions using software-fault isolation (SFI), as provided by WebAssembly, while allowing memory regions to be shared between functions in the same address space. Faaslets can thus avoid expensive data movement when functions are co-located on the same machine. Our runtime for Faaslets, Faasm, isolates other resources, e.g. CPU and network, using standard Linux cgroups, and provides a low-level POSIX host interface for networking, file system access and dynamic loading. To reduce initialisation times, Faasm restores Faaslets from already-initialised snapshots. We compare Faasm to a standard container-based platform and show that, when training a machine learning model, it achieves a 2x speed-up with 10x less memory; for serving machine learning inference, Faasm doubles the throughput and reduces tail latency by 90%. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09344">PDF</a>
<h3>No. 3	Methods and Experiences for Developing Abstractions for Data-intensive,  Scientific Applications</h3><h4>Andre Luckow, Shantenu Jha</h4> Abstract: Developing software for scientific applications that require the integration of diverse types of computing, instruments and data present challenges that are distinct from commercial software, due to scale, heterogeneity, and the need to integrate various programming and computational models with evolving and heterogeneous infrastructure. Pervasive and effective abstractions are thus critical. The process of developing abstractions for scientific applications and infrastructures is not well understood. While theory-based approaches are suited for well-defined, closed environments, they have severe limitations for designing abstractions for complex, real-world systems. The design science research (DSR) method provides the basis for designing effective systems that can handle real-world complexities. DSR consists of two complementary phases: design and evaluation. This paper applies the DSR method to the development of abstractions for scientific applications. Specifically, we address the critical problem of distributed resource management on heterogeneous infrastructure, a challenge that currently limits many scientific applications. We use the pilot-abstraction, a widely used resource management abstraction for high-performance, high throughput, big data, and streaming applications, as a case study. We evaluate the activities of the process and extensively evaluate the artifacts using different methods, including conceptual modeling, performance characterizations, and modeling. We demonstrate the applicability of the DSR method for holistically handling the complexity of parallel and distributed computing environments addressing important application, system, and infrastructure challenges of scientific applications. Finally, we capture our experiences and formulate different lessons learned. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09009">PDF</a>
<h3>No. 4	Distributed Mean Estimation with Optimal Error Bounds</h3><h4>Dan Alistarh, Saleh Ashkboos, Peter Davies</h4> Abstract: Motivated by applications to distributed optimization and machine learning, we consider the distributed mean estimation problem, in which $n$ nodes are each assigned a multi-dimensional input vector, and must cooperate to estimate the mean of the input vectors, while minimizing communication. In this paper, we provide the first tight bounds for this problem, in terms of the trade-off between the amount of communication between nodes and the variance of the node estimates relative to the true value of the mean. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09268">PDF</a>
<h3>No. 5	A polynomial lower bound on adaptive complexity of submodular  maximization</h3><h4>Wenzheng Li, Paul Liu, Jan Vondrak</h4> Abstract: In large-data applications, it is desirable to design algorithms with a high degree of parallelization. In the context of submodular optimization, adaptive complexity has become a widely-used measure of an algorithm's "sequentiality". Algorithms in the adaptive model proceed in rounds, and can issue polynomially many queries to a function $f$ in each round. The queries in each round must be independent, produced by a computation that depends only on query results obtained in previous rounds. In this work, we examine two fundamental variants of submodular maximization in the adaptive complexity model: cardinality-constrained monotone maximization, and unconstrained non-monotone maximization. Our main result is that an $r$-round algorithm for cardinality-constrained monotone maximization cannot achieve a factor better than $1 - 1/e - \Omega(\min \{ \frac{1}{r}, \frac{\log^2 n}{r^3} \})$, for any $r < n^c$ (where $c>0$ is some constant). This is the first result showing that the number of rounds must blow up polynomially large as we approach the optimal factor of $1-1/e$. For the unconstrained non-monotone maximization problem, we show a positive result: For every instance, and every $\delta>0$, either we obtain a $(1/2-\delta)$-approximation in $1$ round, or a $(1/2+\Omega(\delta^2))$-approximation in $O(1/\delta^2)$ rounds. In particular (in contrast to the cardinality-constrained case), there cannot be an instance where (i) it is impossible to achieve a factor better than $1/2$ regardless of the number of rounds, and (ii) it takes $r$ rounds to achieve a factor of $1/2-O(1/r)$. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09130">PDF</a>
<h3>No. 6	Asynchronous parallel adaptive stochastic gradient methods</h3><h4>Yangyang Xu, Colin Sutcher-Shepard, Yibo Xu, Jie Chen</h4> Abstract: Stochastic gradient methods (SGMs) are the predominant approaches to train deep learning models. The adaptive versions (e.g., Adam and AMSGrad) have been extensively used in practice, partly because they achieve faster convergence than the non-adaptive versions while incurring little overhead. On the other hand, asynchronous (async) parallel computing has exhibited much better speed-up over its synchronous (sync) counterpart. However, async-parallel implementation has only been demonstrated to the non-adaptive SGMs. The difficulty for adaptive SGMs originates from the second moment term that makes the convergence analysis challenging with async updates. In this paper, we propose an async-parallel adaptive SGM based on AMSGrad. We show that the proposed method inherits the convergence guarantee of AMSGrad for both convex and non-convex problems, if the staleness (also called delay) caused by asynchrony is bounded. Our convergence rate results indicate a nearly linear parallelization speed-up if $\tau=o(K^{\frac{1}{4}})$, where $\tau$ is the staleness and $K$ is the number of iterations. The proposed method is tested on both convex and non-convex machine learning problems, and the numerical results demonstrate its clear advantages over the sync counterpart. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09095">PDF</a>
<h3>No. 7	Uncertainty Principle for Communication Compression in Distributed and  Federated Learning and the Search for an Optimal Compressor</h3><h4>Mher Safaryan, Egor Shulgin, Peter Richt√°rik</h4> Abstract: In order to mitigate the high communication cost in distributed and federated learning, various vector compression schemes, such as quantization, sparsification and dithering, have become very popular. In designing a compression method, one aims to communicate as few bits as possible, which minimizes the cost per communication round, while at the same time attempting to impart as little distortion (variance) to the communicated messages as possible, which minimizes the adverse effect of the compression on the overall number of communication rounds. However, intuitively, these two goals are fundamentally in conflict: the more compression we allow, the more distorted the messages become. We formalize this intuition and prove an {\em uncertainty principle} for randomized compression operators, thus quantifying this limitation mathematically, and {\em effectively providing lower bounds on what might be achievable with communication compression}. Motivated by these developments, we call for the search for the optimal compression operator. In an attempt to take a first step in this direction, we construct a new unbiased compression method inspired by the Kashin representation of vectors, which we call {\em Kashin compression (KC)}. In contrast to all previously proposed compression mechanisms, we prove that KC enjoys a {\em dimension independent} variance bound with an explicit formula even in the regime when only a few bits need to be communicate per each vector entry. We show how KC can be provably and efficiently combined with several existing optimization algorithms, in all cases leading to communication complexity improvements on previous state of the art. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08958">PDF</a><h2>2020-03-12</h2>
<h3>No. 1	Fast Implementation of Morphological Filtering Using ARM NEON Extension</h3><h4>Elena Limonova, Arseny Terekhin, Dmitry Nikolaev, Vladimir Arlazarov</h4> Abstract: In this paper we consider speedup potential of morphological image filtering on ARM processors. Morphological operations are widely used in image analysis and recognition and their speedup in some cases can significantly reduce overall execution time of recognition. More specifically, we propose fast implementation of erosion and dilation using ARM SIMD extension NEON. These operations with the rectangular structuring element are separable. They were implemented using the advantages of separability as sequential horizontal and vertical passes. Each pass was implemented using van Herk/Gil-Werman algorithm for large windows and low-constant linear complexity algorithm for small windows. Final implementation was improved with SIMD and used a combination of these methods. We also considered fast transpose implementation of 8x8 and 16x16 matrices using ARM NEON to get additional computational gain for morphological operations. Experiments showed 3 times efficiency increase for final implementation of erosion and dilation compared to van Herk/Gil-Werman algorithm without SIMD, 5.7 times speedup for 8x8 matrix transpose and 12 times speedup for 16x16 matrix transpose compared to transpose without SIMD. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09474">PDF</a>
<h3>No. 2	Faasm: Lightweight Isolation for Efficient Stateful Serverless Computing</h3><h4>Simon Shillaker, Peter Pietzuch</h4> Abstract: Serverless computing is an excellent fit for big data processing because it can scale quickly and cheaply to thousands of parallel functions. Existing serverless platforms, however, isolate functions in ephemeral, stateless containers. This means that functions cannot efficiently share memory, forcing users to serialise data repeatedly when composing functions. We observe that container-based isolation is ill-suited for serverless big data processing. Instead it requires a lightweight isolation approach that allows for efficient state sharing. We introduce Faaslets, a new isolation abstraction for serverless big data computing. Faaslets isolate the memory of executed functions using software-fault isolation (SFI), as provided by WebAssembly, while allowing memory regions to be shared between functions in the same address space. Faaslets can thus avoid expensive data movement when functions are co-located on the same machine. Our runtime for Faaslets, Faasm, isolates other resources, e.g. CPU and network, using standard Linux cgroups, and provides a low-level POSIX host interface for networking, file system access and dynamic loading. To reduce initialisation times, Faasm restores Faaslets from already-initialised snapshots. We compare Faasm to a standard container-based platform and show that, when training a machine learning model, it achieves a 2x speed-up with 10x less memory; for serving machine learning inference, Faasm doubles the throughput and reduces tail latency by 90%. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09344">PDF</a>
<h3>No. 3	Methods and Experiences for Developing Abstractions for Data-intensive,  Scientific Applications</h3><h4>Andre Luckow, Shantenu Jha</h4> Abstract: Developing software for scientific applications that require the integration of diverse types of computing, instruments and data present challenges that are distinct from commercial software, due to scale, heterogeneity, and the need to integrate various programming and computational models with evolving and heterogeneous infrastructure. Pervasive and effective abstractions are thus critical. The process of developing abstractions for scientific applications and infrastructures is not well understood. While theory-based approaches are suited for well-defined, closed environments, they have severe limitations for designing abstractions for complex, real-world systems. The design science research (DSR) method provides the basis for designing effective systems that can handle real-world complexities. DSR consists of two complementary phases: design and evaluation. This paper applies the DSR method to the development of abstractions for scientific applications. Specifically, we address the critical problem of distributed resource management on heterogeneous infrastructure, a challenge that currently limits many scientific applications. We use the pilot-abstraction, a widely used resource management abstraction for high-performance, high throughput, big data, and streaming applications, as a case study. We evaluate the activities of the process and extensively evaluate the artifacts using different methods, including conceptual modeling, performance characterizations, and modeling. We demonstrate the applicability of the DSR method for holistically handling the complexity of parallel and distributed computing environments addressing important application, system, and infrastructure challenges of scientific applications. Finally, we capture our experiences and formulate different lessons learned. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09009">PDF</a>
<h3>No. 4	Distributed Mean Estimation with Optimal Error Bounds</h3><h4>Dan Alistarh, Saleh Ashkboos, Peter Davies</h4> Abstract: Motivated by applications to distributed optimization and machine learning, we consider the distributed mean estimation problem, in which $n$ nodes are each assigned a multi-dimensional input vector, and must cooperate to estimate the mean of the input vectors, while minimizing communication. In this paper, we provide the first tight bounds for this problem, in terms of the trade-off between the amount of communication between nodes and the variance of the node estimates relative to the true value of the mean. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09268">PDF</a>
<h3>No. 5	A polynomial lower bound on adaptive complexity of submodular  maximization</h3><h4>Wenzheng Li, Paul Liu, Jan Vondrak</h4> Abstract: In large-data applications, it is desirable to design algorithms with a high degree of parallelization. In the context of submodular optimization, adaptive complexity has become a widely-used measure of an algorithm's "sequentiality". Algorithms in the adaptive model proceed in rounds, and can issue polynomially many queries to a function $f$ in each round. The queries in each round must be independent, produced by a computation that depends only on query results obtained in previous rounds. In this work, we examine two fundamental variants of submodular maximization in the adaptive complexity model: cardinality-constrained monotone maximization, and unconstrained non-monotone maximization. Our main result is that an $r$-round algorithm for cardinality-constrained monotone maximization cannot achieve a factor better than $1 - 1/e - \Omega(\min \{ \frac{1}{r}, \frac{\log^2 n}{r^3} \})$, for any $r < n^c$ (where $c>0$ is some constant). This is the first result showing that the number of rounds must blow up polynomially large as we approach the optimal factor of $1-1/e$. For the unconstrained non-monotone maximization problem, we show a positive result: For every instance, and every $\delta>0$, either we obtain a $(1/2-\delta)$-approximation in $1$ round, or a $(1/2+\Omega(\delta^2))$-approximation in $O(1/\delta^2)$ rounds. In particular (in contrast to the cardinality-constrained case), there cannot be an instance where (i) it is impossible to achieve a factor better than $1/2$ regardless of the number of rounds, and (ii) it takes $r$ rounds to achieve a factor of $1/2-O(1/r)$. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09130">PDF</a>
<h3>No. 6	Asynchronous parallel adaptive stochastic gradient methods</h3><h4>Yangyang Xu, Colin Sutcher-Shepard, Yibo Xu, Jie Chen</h4> Abstract: Stochastic gradient methods (SGMs) are the predominant approaches to train deep learning models. The adaptive versions (e.g., Adam and AMSGrad) have been extensively used in practice, partly because they achieve faster convergence than the non-adaptive versions while incurring little overhead. On the other hand, asynchronous (async) parallel computing has exhibited much better speed-up over its synchronous (sync) counterpart. However, async-parallel implementation has only been demonstrated to the non-adaptive SGMs. The difficulty for adaptive SGMs originates from the second moment term that makes the convergence analysis challenging with async updates. In this paper, we propose an async-parallel adaptive SGM based on AMSGrad. We show that the proposed method inherits the convergence guarantee of AMSGrad for both convex and non-convex problems, if the staleness (also called delay) caused by asynchrony is bounded. Our convergence rate results indicate a nearly linear parallelization speed-up if $\tau=o(K^{\frac{1}{4}})$, where $\tau$ is the staleness and $K$ is the number of iterations. The proposed method is tested on both convex and non-convex machine learning problems, and the numerical results demonstrate its clear advantages over the sync counterpart. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09095">PDF</a>
<h3>No. 7	Uncertainty Principle for Communication Compression in Distributed and  Federated Learning and the Search for an Optimal Compressor</h3><h4>Mher Safaryan, Egor Shulgin, Peter Richt√°rik</h4> Abstract: In order to mitigate the high communication cost in distributed and federated learning, various vector compression schemes, such as quantization, sparsification and dithering, have become very popular. In designing a compression method, one aims to communicate as few bits as possible, which minimizes the cost per communication round, while at the same time attempting to impart as little distortion (variance) to the communicated messages as possible, which minimizes the adverse effect of the compression on the overall number of communication rounds. However, intuitively, these two goals are fundamentally in conflict: the more compression we allow, the more distorted the messages become. We formalize this intuition and prove an {\em uncertainty principle} for randomized compression operators, thus quantifying this limitation mathematically, and {\em effectively providing lower bounds on what might be achievable with communication compression}. Motivated by these developments, we call for the search for the optimal compression operator. In an attempt to take a first step in this direction, we construct a new unbiased compression method inspired by the Kashin representation of vectors, which we call {\em Kashin compression (KC)}. In contrast to all previously proposed compression mechanisms, we prove that KC enjoys a {\em dimension independent} variance bound with an explicit formula even in the regime when only a few bits need to be communicate per each vector entry. We show how KC can be provably and efficiently combined with several existing optimization algorithms, in all cases leading to communication complexity improvements on previous state of the art. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08958">PDF</a><h2>2020-03-12</h2>
<h3>No. 1	Fast Implementation of Morphological Filtering Using ARM NEON Extension</h3><h4>Elena Limonova, Arseny Terekhin, Dmitry Nikolaev, Vladimir Arlazarov</h4> Abstract: In this paper we consider speedup potential of morphological image filtering on ARM processors. Morphological operations are widely used in image analysis and recognition and their speedup in some cases can significantly reduce overall execution time of recognition. More specifically, we propose fast implementation of erosion and dilation using ARM SIMD extension NEON. These operations with the rectangular structuring element are separable. They were implemented using the advantages of separability as sequential horizontal and vertical passes. Each pass was implemented using van Herk/Gil-Werman algorithm for large windows and low-constant linear complexity algorithm for small windows. Final implementation was improved with SIMD and used a combination of these methods. We also considered fast transpose implementation of 8x8 and 16x16 matrices using ARM NEON to get additional computational gain for morphological operations. Experiments showed 3 times efficiency increase for final implementation of erosion and dilation compared to van Herk/Gil-Werman algorithm without SIMD, 5.7 times speedup for 8x8 matrix transpose and 12 times speedup for 16x16 matrix transpose compared to transpose without SIMD. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09474">PDF</a>
<h3>No. 2	Faasm: Lightweight Isolation for Efficient Stateful Serverless Computing</h3><h4>Simon Shillaker, Peter Pietzuch</h4> Abstract: Serverless computing is an excellent fit for big data processing because it can scale quickly and cheaply to thousands of parallel functions. Existing serverless platforms, however, isolate functions in ephemeral, stateless containers. This means that functions cannot efficiently share memory, forcing users to serialise data repeatedly when composing functions. We observe that container-based isolation is ill-suited for serverless big data processing. Instead it requires a lightweight isolation approach that allows for efficient state sharing. We introduce Faaslets, a new isolation abstraction for serverless big data computing. Faaslets isolate the memory of executed functions using software-fault isolation (SFI), as provided by WebAssembly, while allowing memory regions to be shared between functions in the same address space. Faaslets can thus avoid expensive data movement when functions are co-located on the same machine. Our runtime for Faaslets, Faasm, isolates other resources, e.g. CPU and network, using standard Linux cgroups, and provides a low-level POSIX host interface for networking, file system access and dynamic loading. To reduce initialisation times, Faasm restores Faaslets from already-initialised snapshots. We compare Faasm to a standard container-based platform and show that, when training a machine learning model, it achieves a 2x speed-up with 10x less memory; for serving machine learning inference, Faasm doubles the throughput and reduces tail latency by 90%. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09344">PDF</a>
<h3>No. 3	Methods and Experiences for Developing Abstractions for Data-intensive,  Scientific Applications</h3><h4>Andre Luckow, Shantenu Jha</h4> Abstract: Developing software for scientific applications that require the integration of diverse types of computing, instruments and data present challenges that are distinct from commercial software, due to scale, heterogeneity, and the need to integrate various programming and computational models with evolving and heterogeneous infrastructure. Pervasive and effective abstractions are thus critical. The process of developing abstractions for scientific applications and infrastructures is not well understood. While theory-based approaches are suited for well-defined, closed environments, they have severe limitations for designing abstractions for complex, real-world systems. The design science research (DSR) method provides the basis for designing effective systems that can handle real-world complexities. DSR consists of two complementary phases: design and evaluation. This paper applies the DSR method to the development of abstractions for scientific applications. Specifically, we address the critical problem of distributed resource management on heterogeneous infrastructure, a challenge that currently limits many scientific applications. We use the pilot-abstraction, a widely used resource management abstraction for high-performance, high throughput, big data, and streaming applications, as a case study. We evaluate the activities of the process and extensively evaluate the artifacts using different methods, including conceptual modeling, performance characterizations, and modeling. We demonstrate the applicability of the DSR method for holistically handling the complexity of parallel and distributed computing environments addressing important application, system, and infrastructure challenges of scientific applications. Finally, we capture our experiences and formulate different lessons learned. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09009">PDF</a>
<h3>No. 4	Distributed Mean Estimation with Optimal Error Bounds</h3><h4>Dan Alistarh, Saleh Ashkboos, Peter Davies</h4> Abstract: Motivated by applications to distributed optimization and machine learning, we consider the distributed mean estimation problem, in which $n$ nodes are each assigned a multi-dimensional input vector, and must cooperate to estimate the mean of the input vectors, while minimizing communication. In this paper, we provide the first tight bounds for this problem, in terms of the trade-off between the amount of communication between nodes and the variance of the node estimates relative to the true value of the mean. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09268">PDF</a>
<h3>No. 5	A polynomial lower bound on adaptive complexity of submodular  maximization</h3><h4>Wenzheng Li, Paul Liu, Jan Vondrak</h4> Abstract: In large-data applications, it is desirable to design algorithms with a high degree of parallelization. In the context of submodular optimization, adaptive complexity has become a widely-used measure of an algorithm's "sequentiality". Algorithms in the adaptive model proceed in rounds, and can issue polynomially many queries to a function $f$ in each round. The queries in each round must be independent, produced by a computation that depends only on query results obtained in previous rounds. In this work, we examine two fundamental variants of submodular maximization in the adaptive complexity model: cardinality-constrained monotone maximization, and unconstrained non-monotone maximization. Our main result is that an $r$-round algorithm for cardinality-constrained monotone maximization cannot achieve a factor better than $1 - 1/e - \Omega(\min \{ \frac{1}{r}, \frac{\log^2 n}{r^3} \})$, for any $r < n^c$ (where $c>0$ is some constant). This is the first result showing that the number of rounds must blow up polynomially large as we approach the optimal factor of $1-1/e$. For the unconstrained non-monotone maximization problem, we show a positive result: For every instance, and every $\delta>0$, either we obtain a $(1/2-\delta)$-approximation in $1$ round, or a $(1/2+\Omega(\delta^2))$-approximation in $O(1/\delta^2)$ rounds. In particular (in contrast to the cardinality-constrained case), there cannot be an instance where (i) it is impossible to achieve a factor better than $1/2$ regardless of the number of rounds, and (ii) it takes $r$ rounds to achieve a factor of $1/2-O(1/r)$. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09130">PDF</a>
<h3>No. 6	Asynchronous parallel adaptive stochastic gradient methods</h3><h4>Yangyang Xu, Colin Sutcher-Shepard, Yibo Xu, Jie Chen</h4> Abstract: Stochastic gradient methods (SGMs) are the predominant approaches to train deep learning models. The adaptive versions (e.g., Adam and AMSGrad) have been extensively used in practice, partly because they achieve faster convergence than the non-adaptive versions while incurring little overhead. On the other hand, asynchronous (async) parallel computing has exhibited much better speed-up over its synchronous (sync) counterpart. However, async-parallel implementation has only been demonstrated to the non-adaptive SGMs. The difficulty for adaptive SGMs originates from the second moment term that makes the convergence analysis challenging with async updates. In this paper, we propose an async-parallel adaptive SGM based on AMSGrad. We show that the proposed method inherits the convergence guarantee of AMSGrad for both convex and non-convex problems, if the staleness (also called delay) caused by asynchrony is bounded. Our convergence rate results indicate a nearly linear parallelization speed-up if $\tau=o(K^{\frac{1}{4}})$, where $\tau$ is the staleness and $K$ is the number of iterations. The proposed method is tested on both convex and non-convex machine learning problems, and the numerical results demonstrate its clear advantages over the sync counterpart. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09095">PDF</a>
<h3>No. 7	Uncertainty Principle for Communication Compression in Distributed and  Federated Learning and the Search for an Optimal Compressor</h3><h4>Mher Safaryan, Egor Shulgin, Peter Richt√°rik</h4> Abstract: In order to mitigate the high communication cost in distributed and federated learning, various vector compression schemes, such as quantization, sparsification and dithering, have become very popular. In designing a compression method, one aims to communicate as few bits as possible, which minimizes the cost per communication round, while at the same time attempting to impart as little distortion (variance) to the communicated messages as possible, which minimizes the adverse effect of the compression on the overall number of communication rounds. However, intuitively, these two goals are fundamentally in conflict: the more compression we allow, the more distorted the messages become. We formalize this intuition and prove an {\em uncertainty principle} for randomized compression operators, thus quantifying this limitation mathematically, and {\em effectively providing lower bounds on what might be achievable with communication compression}. Motivated by these developments, we call for the search for the optimal compression operator. In an attempt to take a first step in this direction, we construct a new unbiased compression method inspired by the Kashin representation of vectors, which we call {\em Kashin compression (KC)}. In contrast to all previously proposed compression mechanisms, we prove that KC enjoys a {\em dimension independent} variance bound with an explicit formula even in the regime when only a few bits need to be communicate per each vector entry. We show how KC can be provably and efficiently combined with several existing optimization algorithms, in all cases leading to communication complexity improvements on previous state of the art. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08958">PDF</a><h2>2020-03-11</h2>
<h3>No. 1	Fast Implementation of Morphological Filtering Using ARM NEON Extension</h3><h4>Elena Limonova, Arseny Terekhin, Dmitry Nikolaev, Vladimir Arlazarov</h4> Abstract: In this paper we consider speedup potential of morphological image filtering on ARM processors. Morphological operations are widely used in image analysis and recognition and their speedup in some cases can significantly reduce overall execution time of recognition. More specifically, we propose fast implementation of erosion and dilation using ARM SIMD extension NEON. These operations with the rectangular structuring element are separable. They were implemented using the advantages of separability as sequential horizontal and vertical passes. Each pass was implemented using van Herk/Gil-Werman algorithm for large windows and low-constant linear complexity algorithm for small windows. Final implementation was improved with SIMD and used a combination of these methods. We also considered fast transpose implementation of 8x8 and 16x16 matrices using ARM NEON to get additional computational gain for morphological operations. Experiments showed 3 times efficiency increase for final implementation of erosion and dilation compared to van Herk/Gil-Werman algorithm without SIMD, 5.7 times speedup for 8x8 matrix transpose and 12 times speedup for 16x16 matrix transpose compared to transpose without SIMD. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09474">PDF</a>
<h3>No. 2	Faasm: Lightweight Isolation for Efficient Stateful Serverless Computing</h3><h4>Simon Shillaker, Peter Pietzuch</h4> Abstract: Serverless computing is an excellent fit for big data processing because it can scale quickly and cheaply to thousands of parallel functions. Existing serverless platforms, however, isolate functions in ephemeral, stateless containers. This means that functions cannot efficiently share memory, forcing users to serialise data repeatedly when composing functions. We observe that container-based isolation is ill-suited for serverless big data processing. Instead it requires a lightweight isolation approach that allows for efficient state sharing. We introduce Faaslets, a new isolation abstraction for serverless big data computing. Faaslets isolate the memory of executed functions using software-fault isolation (SFI), as provided by WebAssembly, while allowing memory regions to be shared between functions in the same address space. Faaslets can thus avoid expensive data movement when functions are co-located on the same machine. Our runtime for Faaslets, Faasm, isolates other resources, e.g. CPU and network, using standard Linux cgroups, and provides a low-level POSIX host interface for networking, file system access and dynamic loading. To reduce initialisation times, Faasm restores Faaslets from already-initialised snapshots. We compare Faasm to a standard container-based platform and show that, when training a machine learning model, it achieves a 2x speed-up with 10x less memory; for serving machine learning inference, Faasm doubles the throughput and reduces tail latency by 90%. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09344">PDF</a>
<h3>No. 3	Methods and Experiences for Developing Abstractions for Data-intensive,  Scientific Applications</h3><h4>Andre Luckow, Shantenu Jha</h4> Abstract: Developing software for scientific applications that require the integration of diverse types of computing, instruments and data present challenges that are distinct from commercial software, due to scale, heterogeneity, and the need to integrate various programming and computational models with evolving and heterogeneous infrastructure. Pervasive and effective abstractions are thus critical. The process of developing abstractions for scientific applications and infrastructures is not well understood. While theory-based approaches are suited for well-defined, closed environments, they have severe limitations for designing abstractions for complex, real-world systems. The design science research (DSR) method provides the basis for designing effective systems that can handle real-world complexities. DSR consists of two complementary phases: design and evaluation. This paper applies the DSR method to the development of abstractions for scientific applications. Specifically, we address the critical problem of distributed resource management on heterogeneous infrastructure, a challenge that currently limits many scientific applications. We use the pilot-abstraction, a widely used resource management abstraction for high-performance, high throughput, big data, and streaming applications, as a case study. We evaluate the activities of the process and extensively evaluate the artifacts using different methods, including conceptual modeling, performance characterizations, and modeling. We demonstrate the applicability of the DSR method for holistically handling the complexity of parallel and distributed computing environments addressing important application, system, and infrastructure challenges of scientific applications. Finally, we capture our experiences and formulate different lessons learned. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09009">PDF</a>
<h3>No. 4	Distributed Mean Estimation with Optimal Error Bounds</h3><h4>Dan Alistarh, Saleh Ashkboos, Peter Davies</h4> Abstract: Motivated by applications to distributed optimization and machine learning, we consider the distributed mean estimation problem, in which $n$ nodes are each assigned a multi-dimensional input vector, and must cooperate to estimate the mean of the input vectors, while minimizing communication. In this paper, we provide the first tight bounds for this problem, in terms of the trade-off between the amount of communication between nodes and the variance of the node estimates relative to the true value of the mean. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09268">PDF</a>
<h3>No. 5	A polynomial lower bound on adaptive complexity of submodular  maximization</h3><h4>Wenzheng Li, Paul Liu, Jan Vondrak</h4> Abstract: In large-data applications, it is desirable to design algorithms with a high degree of parallelization. In the context of submodular optimization, adaptive complexity has become a widely-used measure of an algorithm's "sequentiality". Algorithms in the adaptive model proceed in rounds, and can issue polynomially many queries to a function $f$ in each round. The queries in each round must be independent, produced by a computation that depends only on query results obtained in previous rounds. In this work, we examine two fundamental variants of submodular maximization in the adaptive complexity model: cardinality-constrained monotone maximization, and unconstrained non-monotone maximization. Our main result is that an $r$-round algorithm for cardinality-constrained monotone maximization cannot achieve a factor better than $1 - 1/e - \Omega(\min \{ \frac{1}{r}, \frac{\log^2 n}{r^3} \})$, for any $r < n^c$ (where $c>0$ is some constant). This is the first result showing that the number of rounds must blow up polynomially large as we approach the optimal factor of $1-1/e$. For the unconstrained non-monotone maximization problem, we show a positive result: For every instance, and every $\delta>0$, either we obtain a $(1/2-\delta)$-approximation in $1$ round, or a $(1/2+\Omega(\delta^2))$-approximation in $O(1/\delta^2)$ rounds. In particular (in contrast to the cardinality-constrained case), there cannot be an instance where (i) it is impossible to achieve a factor better than $1/2$ regardless of the number of rounds, and (ii) it takes $r$ rounds to achieve a factor of $1/2-O(1/r)$. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09130">PDF</a>
<h3>No. 6	Asynchronous parallel adaptive stochastic gradient methods</h3><h4>Yangyang Xu, Colin Sutcher-Shepard, Yibo Xu, Jie Chen</h4> Abstract: Stochastic gradient methods (SGMs) are the predominant approaches to train deep learning models. The adaptive versions (e.g., Adam and AMSGrad) have been extensively used in practice, partly because they achieve faster convergence than the non-adaptive versions while incurring little overhead. On the other hand, asynchronous (async) parallel computing has exhibited much better speed-up over its synchronous (sync) counterpart. However, async-parallel implementation has only been demonstrated to the non-adaptive SGMs. The difficulty for adaptive SGMs originates from the second moment term that makes the convergence analysis challenging with async updates. In this paper, we propose an async-parallel adaptive SGM based on AMSGrad. We show that the proposed method inherits the convergence guarantee of AMSGrad for both convex and non-convex problems, if the staleness (also called delay) caused by asynchrony is bounded. Our convergence rate results indicate a nearly linear parallelization speed-up if $\tau=o(K^{\frac{1}{4}})$, where $\tau$ is the staleness and $K$ is the number of iterations. The proposed method is tested on both convex and non-convex machine learning problems, and the numerical results demonstrate its clear advantages over the sync counterpart. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09095">PDF</a>
<h3>No. 7	Uncertainty Principle for Communication Compression in Distributed and  Federated Learning and the Search for an Optimal Compressor</h3><h4>Mher Safaryan, Egor Shulgin, Peter Richt√°rik</h4> Abstract: In order to mitigate the high communication cost in distributed and federated learning, various vector compression schemes, such as quantization, sparsification and dithering, have become very popular. In designing a compression method, one aims to communicate as few bits as possible, which minimizes the cost per communication round, while at the same time attempting to impart as little distortion (variance) to the communicated messages as possible, which minimizes the adverse effect of the compression on the overall number of communication rounds. However, intuitively, these two goals are fundamentally in conflict: the more compression we allow, the more distorted the messages become. We formalize this intuition and prove an {\em uncertainty principle} for randomized compression operators, thus quantifying this limitation mathematically, and {\em effectively providing lower bounds on what might be achievable with communication compression}. Motivated by these developments, we call for the search for the optimal compression operator. In an attempt to take a first step in this direction, we construct a new unbiased compression method inspired by the Kashin representation of vectors, which we call {\em Kashin compression (KC)}. In contrast to all previously proposed compression mechanisms, we prove that KC enjoys a {\em dimension independent} variance bound with an explicit formula even in the regime when only a few bits need to be communicate per each vector entry. We show how KC can be provably and efficiently combined with several existing optimization algorithms, in all cases leading to communication complexity improvements on previous state of the art. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08958">PDF</a><h2>2020-03-10</h2>
<h3>No. 1	Fast Implementation of Morphological Filtering Using ARM NEON Extension</h3><h4>Elena Limonova, Arseny Terekhin, Dmitry Nikolaev, Vladimir Arlazarov</h4> Abstract: In this paper we consider speedup potential of morphological image filtering on ARM processors. Morphological operations are widely used in image analysis and recognition and their speedup in some cases can significantly reduce overall execution time of recognition. More specifically, we propose fast implementation of erosion and dilation using ARM SIMD extension NEON. These operations with the rectangular structuring element are separable. They were implemented using the advantages of separability as sequential horizontal and vertical passes. Each pass was implemented using van Herk/Gil-Werman algorithm for large windows and low-constant linear complexity algorithm for small windows. Final implementation was improved with SIMD and used a combination of these methods. We also considered fast transpose implementation of 8x8 and 16x16 matrices using ARM NEON to get additional computational gain for morphological operations. Experiments showed 3 times efficiency increase for final implementation of erosion and dilation compared to van Herk/Gil-Werman algorithm without SIMD, 5.7 times speedup for 8x8 matrix transpose and 12 times speedup for 16x16 matrix transpose compared to transpose without SIMD. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09474">PDF</a>
<h3>No. 2	Faasm: Lightweight Isolation for Efficient Stateful Serverless Computing</h3><h4>Simon Shillaker, Peter Pietzuch</h4> Abstract: Serverless computing is an excellent fit for big data processing because it can scale quickly and cheaply to thousands of parallel functions. Existing serverless platforms, however, isolate functions in ephemeral, stateless containers. This means that functions cannot efficiently share memory, forcing users to serialise data repeatedly when composing functions. We observe that container-based isolation is ill-suited for serverless big data processing. Instead it requires a lightweight isolation approach that allows for efficient state sharing. We introduce Faaslets, a new isolation abstraction for serverless big data computing. Faaslets isolate the memory of executed functions using software-fault isolation (SFI), as provided by WebAssembly, while allowing memory regions to be shared between functions in the same address space. Faaslets can thus avoid expensive data movement when functions are co-located on the same machine. Our runtime for Faaslets, Faasm, isolates other resources, e.g. CPU and network, using standard Linux cgroups, and provides a low-level POSIX host interface for networking, file system access and dynamic loading. To reduce initialisation times, Faasm restores Faaslets from already-initialised snapshots. We compare Faasm to a standard container-based platform and show that, when training a machine learning model, it achieves a 2x speed-up with 10x less memory; for serving machine learning inference, Faasm doubles the throughput and reduces tail latency by 90%. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09344">PDF</a>
<h3>No. 3	Methods and Experiences for Developing Abstractions for Data-intensive,  Scientific Applications</h3><h4>Andre Luckow, Shantenu Jha</h4> Abstract: Developing software for scientific applications that require the integration of diverse types of computing, instruments and data present challenges that are distinct from commercial software, due to scale, heterogeneity, and the need to integrate various programming and computational models with evolving and heterogeneous infrastructure. Pervasive and effective abstractions are thus critical. The process of developing abstractions for scientific applications and infrastructures is not well understood. While theory-based approaches are suited for well-defined, closed environments, they have severe limitations for designing abstractions for complex, real-world systems. The design science research (DSR) method provides the basis for designing effective systems that can handle real-world complexities. DSR consists of two complementary phases: design and evaluation. This paper applies the DSR method to the development of abstractions for scientific applications. Specifically, we address the critical problem of distributed resource management on heterogeneous infrastructure, a challenge that currently limits many scientific applications. We use the pilot-abstraction, a widely used resource management abstraction for high-performance, high throughput, big data, and streaming applications, as a case study. We evaluate the activities of the process and extensively evaluate the artifacts using different methods, including conceptual modeling, performance characterizations, and modeling. We demonstrate the applicability of the DSR method for holistically handling the complexity of parallel and distributed computing environments addressing important application, system, and infrastructure challenges of scientific applications. Finally, we capture our experiences and formulate different lessons learned. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09009">PDF</a>
<h3>No. 4	Distributed Mean Estimation with Optimal Error Bounds</h3><h4>Dan Alistarh, Saleh Ashkboos, Peter Davies</h4> Abstract: Motivated by applications to distributed optimization and machine learning, we consider the distributed mean estimation problem, in which $n$ nodes are each assigned a multi-dimensional input vector, and must cooperate to estimate the mean of the input vectors, while minimizing communication. In this paper, we provide the first tight bounds for this problem, in terms of the trade-off between the amount of communication between nodes and the variance of the node estimates relative to the true value of the mean. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09268">PDF</a>
<h3>No. 5	A polynomial lower bound on adaptive complexity of submodular  maximization</h3><h4>Wenzheng Li, Paul Liu, Jan Vondrak</h4> Abstract: In large-data applications, it is desirable to design algorithms with a high degree of parallelization. In the context of submodular optimization, adaptive complexity has become a widely-used measure of an algorithm's "sequentiality". Algorithms in the adaptive model proceed in rounds, and can issue polynomially many queries to a function $f$ in each round. The queries in each round must be independent, produced by a computation that depends only on query results obtained in previous rounds. In this work, we examine two fundamental variants of submodular maximization in the adaptive complexity model: cardinality-constrained monotone maximization, and unconstrained non-monotone maximization. Our main result is that an $r$-round algorithm for cardinality-constrained monotone maximization cannot achieve a factor better than $1 - 1/e - \Omega(\min \{ \frac{1}{r}, \frac{\log^2 n}{r^3} \})$, for any $r < n^c$ (where $c>0$ is some constant). This is the first result showing that the number of rounds must blow up polynomially large as we approach the optimal factor of $1-1/e$. For the unconstrained non-monotone maximization problem, we show a positive result: For every instance, and every $\delta>0$, either we obtain a $(1/2-\delta)$-approximation in $1$ round, or a $(1/2+\Omega(\delta^2))$-approximation in $O(1/\delta^2)$ rounds. In particular (in contrast to the cardinality-constrained case), there cannot be an instance where (i) it is impossible to achieve a factor better than $1/2$ regardless of the number of rounds, and (ii) it takes $r$ rounds to achieve a factor of $1/2-O(1/r)$. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09130">PDF</a>
<h3>No. 6	Asynchronous parallel adaptive stochastic gradient methods</h3><h4>Yangyang Xu, Colin Sutcher-Shepard, Yibo Xu, Jie Chen</h4> Abstract: Stochastic gradient methods (SGMs) are the predominant approaches to train deep learning models. The adaptive versions (e.g., Adam and AMSGrad) have been extensively used in practice, partly because they achieve faster convergence than the non-adaptive versions while incurring little overhead. On the other hand, asynchronous (async) parallel computing has exhibited much better speed-up over its synchronous (sync) counterpart. However, async-parallel implementation has only been demonstrated to the non-adaptive SGMs. The difficulty for adaptive SGMs originates from the second moment term that makes the convergence analysis challenging with async updates. In this paper, we propose an async-parallel adaptive SGM based on AMSGrad. We show that the proposed method inherits the convergence guarantee of AMSGrad for both convex and non-convex problems, if the staleness (also called delay) caused by asynchrony is bounded. Our convergence rate results indicate a nearly linear parallelization speed-up if $\tau=o(K^{\frac{1}{4}})$, where $\tau$ is the staleness and $K$ is the number of iterations. The proposed method is tested on both convex and non-convex machine learning problems, and the numerical results demonstrate its clear advantages over the sync counterpart. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09095">PDF</a>
<h3>No. 7	Uncertainty Principle for Communication Compression in Distributed and  Federated Learning and the Search for an Optimal Compressor</h3><h4>Mher Safaryan, Egor Shulgin, Peter Richt√°rik</h4> Abstract: In order to mitigate the high communication cost in distributed and federated learning, various vector compression schemes, such as quantization, sparsification and dithering, have become very popular. In designing a compression method, one aims to communicate as few bits as possible, which minimizes the cost per communication round, while at the same time attempting to impart as little distortion (variance) to the communicated messages as possible, which minimizes the adverse effect of the compression on the overall number of communication rounds. However, intuitively, these two goals are fundamentally in conflict: the more compression we allow, the more distorted the messages become. We formalize this intuition and prove an {\em uncertainty principle} for randomized compression operators, thus quantifying this limitation mathematically, and {\em effectively providing lower bounds on what might be achievable with communication compression}. Motivated by these developments, we call for the search for the optimal compression operator. In an attempt to take a first step in this direction, we construct a new unbiased compression method inspired by the Kashin representation of vectors, which we call {\em Kashin compression (KC)}. In contrast to all previously proposed compression mechanisms, we prove that KC enjoys a {\em dimension independent} variance bound with an explicit formula even in the regime when only a few bits need to be communicate per each vector entry. We show how KC can be provably and efficiently combined with several existing optimization algorithms, in all cases leading to communication complexity improvements on previous state of the art. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08958">PDF</a><h2>2020-03-09</h2>
<h3>No. 1	Fast Implementation of Morphological Filtering Using ARM NEON Extension</h3><h4>Elena Limonova, Arseny Terekhin, Dmitry Nikolaev, Vladimir Arlazarov</h4> Abstract: In this paper we consider speedup potential of morphological image filtering on ARM processors. Morphological operations are widely used in image analysis and recognition and their speedup in some cases can significantly reduce overall execution time of recognition. More specifically, we propose fast implementation of erosion and dilation using ARM SIMD extension NEON. These operations with the rectangular structuring element are separable. They were implemented using the advantages of separability as sequential horizontal and vertical passes. Each pass was implemented using van Herk/Gil-Werman algorithm for large windows and low-constant linear complexity algorithm for small windows. Final implementation was improved with SIMD and used a combination of these methods. We also considered fast transpose implementation of 8x8 and 16x16 matrices using ARM NEON to get additional computational gain for morphological operations. Experiments showed 3 times efficiency increase for final implementation of erosion and dilation compared to van Herk/Gil-Werman algorithm without SIMD, 5.7 times speedup for 8x8 matrix transpose and 12 times speedup for 16x16 matrix transpose compared to transpose without SIMD. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09474">PDF</a>
<h3>No. 2	Faasm: Lightweight Isolation for Efficient Stateful Serverless Computing</h3><h4>Simon Shillaker, Peter Pietzuch</h4> Abstract: Serverless computing is an excellent fit for big data processing because it can scale quickly and cheaply to thousands of parallel functions. Existing serverless platforms, however, isolate functions in ephemeral, stateless containers. This means that functions cannot efficiently share memory, forcing users to serialise data repeatedly when composing functions. We observe that container-based isolation is ill-suited for serverless big data processing. Instead it requires a lightweight isolation approach that allows for efficient state sharing. We introduce Faaslets, a new isolation abstraction for serverless big data computing. Faaslets isolate the memory of executed functions using software-fault isolation (SFI), as provided by WebAssembly, while allowing memory regions to be shared between functions in the same address space. Faaslets can thus avoid expensive data movement when functions are co-located on the same machine. Our runtime for Faaslets, Faasm, isolates other resources, e.g. CPU and network, using standard Linux cgroups, and provides a low-level POSIX host interface for networking, file system access and dynamic loading. To reduce initialisation times, Faasm restores Faaslets from already-initialised snapshots. We compare Faasm to a standard container-based platform and show that, when training a machine learning model, it achieves a 2x speed-up with 10x less memory; for serving machine learning inference, Faasm doubles the throughput and reduces tail latency by 90%. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09344">PDF</a>
<h3>No. 3	Methods and Experiences for Developing Abstractions for Data-intensive,  Scientific Applications</h3><h4>Andre Luckow, Shantenu Jha</h4> Abstract: Developing software for scientific applications that require the integration of diverse types of computing, instruments and data present challenges that are distinct from commercial software, due to scale, heterogeneity, and the need to integrate various programming and computational models with evolving and heterogeneous infrastructure. Pervasive and effective abstractions are thus critical. The process of developing abstractions for scientific applications and infrastructures is not well understood. While theory-based approaches are suited for well-defined, closed environments, they have severe limitations for designing abstractions for complex, real-world systems. The design science research (DSR) method provides the basis for designing effective systems that can handle real-world complexities. DSR consists of two complementary phases: design and evaluation. This paper applies the DSR method to the development of abstractions for scientific applications. Specifically, we address the critical problem of distributed resource management on heterogeneous infrastructure, a challenge that currently limits many scientific applications. We use the pilot-abstraction, a widely used resource management abstraction for high-performance, high throughput, big data, and streaming applications, as a case study. We evaluate the activities of the process and extensively evaluate the artifacts using different methods, including conceptual modeling, performance characterizations, and modeling. We demonstrate the applicability of the DSR method for holistically handling the complexity of parallel and distributed computing environments addressing important application, system, and infrastructure challenges of scientific applications. Finally, we capture our experiences and formulate different lessons learned. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09009">PDF</a>
<h3>No. 4	Distributed Mean Estimation with Optimal Error Bounds</h3><h4>Dan Alistarh, Saleh Ashkboos, Peter Davies</h4> Abstract: Motivated by applications to distributed optimization and machine learning, we consider the distributed mean estimation problem, in which $n$ nodes are each assigned a multi-dimensional input vector, and must cooperate to estimate the mean of the input vectors, while minimizing communication. In this paper, we provide the first tight bounds for this problem, in terms of the trade-off between the amount of communication between nodes and the variance of the node estimates relative to the true value of the mean. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09268">PDF</a>
<h3>No. 5	A polynomial lower bound on adaptive complexity of submodular  maximization</h3><h4>Wenzheng Li, Paul Liu, Jan Vondrak</h4> Abstract: In large-data applications, it is desirable to design algorithms with a high degree of parallelization. In the context of submodular optimization, adaptive complexity has become a widely-used measure of an algorithm's "sequentiality". Algorithms in the adaptive model proceed in rounds, and can issue polynomially many queries to a function $f$ in each round. The queries in each round must be independent, produced by a computation that depends only on query results obtained in previous rounds. In this work, we examine two fundamental variants of submodular maximization in the adaptive complexity model: cardinality-constrained monotone maximization, and unconstrained non-monotone maximization. Our main result is that an $r$-round algorithm for cardinality-constrained monotone maximization cannot achieve a factor better than $1 - 1/e - \Omega(\min \{ \frac{1}{r}, \frac{\log^2 n}{r^3} \})$, for any $r < n^c$ (where $c>0$ is some constant). This is the first result showing that the number of rounds must blow up polynomially large as we approach the optimal factor of $1-1/e$. For the unconstrained non-monotone maximization problem, we show a positive result: For every instance, and every $\delta>0$, either we obtain a $(1/2-\delta)$-approximation in $1$ round, or a $(1/2+\Omega(\delta^2))$-approximation in $O(1/\delta^2)$ rounds. In particular (in contrast to the cardinality-constrained case), there cannot be an instance where (i) it is impossible to achieve a factor better than $1/2$ regardless of the number of rounds, and (ii) it takes $r$ rounds to achieve a factor of $1/2-O(1/r)$. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09130">PDF</a>
<h3>No. 6	Asynchronous parallel adaptive stochastic gradient methods</h3><h4>Yangyang Xu, Colin Sutcher-Shepard, Yibo Xu, Jie Chen</h4> Abstract: Stochastic gradient methods (SGMs) are the predominant approaches to train deep learning models. The adaptive versions (e.g., Adam and AMSGrad) have been extensively used in practice, partly because they achieve faster convergence than the non-adaptive versions while incurring little overhead. On the other hand, asynchronous (async) parallel computing has exhibited much better speed-up over its synchronous (sync) counterpart. However, async-parallel implementation has only been demonstrated to the non-adaptive SGMs. The difficulty for adaptive SGMs originates from the second moment term that makes the convergence analysis challenging with async updates. In this paper, we propose an async-parallel adaptive SGM based on AMSGrad. We show that the proposed method inherits the convergence guarantee of AMSGrad for both convex and non-convex problems, if the staleness (also called delay) caused by asynchrony is bounded. Our convergence rate results indicate a nearly linear parallelization speed-up if $\tau=o(K^{\frac{1}{4}})$, where $\tau$ is the staleness and $K$ is the number of iterations. The proposed method is tested on both convex and non-convex machine learning problems, and the numerical results demonstrate its clear advantages over the sync counterpart. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09095">PDF</a>
<h3>No. 7	Uncertainty Principle for Communication Compression in Distributed and  Federated Learning and the Search for an Optimal Compressor</h3><h4>Mher Safaryan, Egor Shulgin, Peter Richt√°rik</h4> Abstract: In order to mitigate the high communication cost in distributed and federated learning, various vector compression schemes, such as quantization, sparsification and dithering, have become very popular. In designing a compression method, one aims to communicate as few bits as possible, which minimizes the cost per communication round, while at the same time attempting to impart as little distortion (variance) to the communicated messages as possible, which minimizes the adverse effect of the compression on the overall number of communication rounds. However, intuitively, these two goals are fundamentally in conflict: the more compression we allow, the more distorted the messages become. We formalize this intuition and prove an {\em uncertainty principle} for randomized compression operators, thus quantifying this limitation mathematically, and {\em effectively providing lower bounds on what might be achievable with communication compression}. Motivated by these developments, we call for the search for the optimal compression operator. In an attempt to take a first step in this direction, we construct a new unbiased compression method inspired by the Kashin representation of vectors, which we call {\em Kashin compression (KC)}. In contrast to all previously proposed compression mechanisms, we prove that KC enjoys a {\em dimension independent} variance bound with an explicit formula even in the regime when only a few bits need to be communicate per each vector entry. We show how KC can be provably and efficiently combined with several existing optimization algorithms, in all cases leading to communication complexity improvements on previous state of the art. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08958">PDF</a><h2>2020-03-08</h2>
<h3>No. 1	Fast Implementation of Morphological Filtering Using ARM NEON Extension</h3><h4>Elena Limonova, Arseny Terekhin, Dmitry Nikolaev, Vladimir Arlazarov</h4> Abstract: In this paper we consider speedup potential of morphological image filtering on ARM processors. Morphological operations are widely used in image analysis and recognition and their speedup in some cases can significantly reduce overall execution time of recognition. More specifically, we propose fast implementation of erosion and dilation using ARM SIMD extension NEON. These operations with the rectangular structuring element are separable. They were implemented using the advantages of separability as sequential horizontal and vertical passes. Each pass was implemented using van Herk/Gil-Werman algorithm for large windows and low-constant linear complexity algorithm for small windows. Final implementation was improved with SIMD and used a combination of these methods. We also considered fast transpose implementation of 8x8 and 16x16 matrices using ARM NEON to get additional computational gain for morphological operations. Experiments showed 3 times efficiency increase for final implementation of erosion and dilation compared to van Herk/Gil-Werman algorithm without SIMD, 5.7 times speedup for 8x8 matrix transpose and 12 times speedup for 16x16 matrix transpose compared to transpose without SIMD. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09474">PDF</a>
<h3>No. 2	Faasm: Lightweight Isolation for Efficient Stateful Serverless Computing</h3><h4>Simon Shillaker, Peter Pietzuch</h4> Abstract: Serverless computing is an excellent fit for big data processing because it can scale quickly and cheaply to thousands of parallel functions. Existing serverless platforms, however, isolate functions in ephemeral, stateless containers. This means that functions cannot efficiently share memory, forcing users to serialise data repeatedly when composing functions. We observe that container-based isolation is ill-suited for serverless big data processing. Instead it requires a lightweight isolation approach that allows for efficient state sharing. We introduce Faaslets, a new isolation abstraction for serverless big data computing. Faaslets isolate the memory of executed functions using software-fault isolation (SFI), as provided by WebAssembly, while allowing memory regions to be shared between functions in the same address space. Faaslets can thus avoid expensive data movement when functions are co-located on the same machine. Our runtime for Faaslets, Faasm, isolates other resources, e.g. CPU and network, using standard Linux cgroups, and provides a low-level POSIX host interface for networking, file system access and dynamic loading. To reduce initialisation times, Faasm restores Faaslets from already-initialised snapshots. We compare Faasm to a standard container-based platform and show that, when training a machine learning model, it achieves a 2x speed-up with 10x less memory; for serving machine learning inference, Faasm doubles the throughput and reduces tail latency by 90%. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09344">PDF</a>
<h3>No. 3	Methods and Experiences for Developing Abstractions for Data-intensive,  Scientific Applications</h3><h4>Andre Luckow, Shantenu Jha</h4> Abstract: Developing software for scientific applications that require the integration of diverse types of computing, instruments and data present challenges that are distinct from commercial software, due to scale, heterogeneity, and the need to integrate various programming and computational models with evolving and heterogeneous infrastructure. Pervasive and effective abstractions are thus critical. The process of developing abstractions for scientific applications and infrastructures is not well understood. While theory-based approaches are suited for well-defined, closed environments, they have severe limitations for designing abstractions for complex, real-world systems. The design science research (DSR) method provides the basis for designing effective systems that can handle real-world complexities. DSR consists of two complementary phases: design and evaluation. This paper applies the DSR method to the development of abstractions for scientific applications. Specifically, we address the critical problem of distributed resource management on heterogeneous infrastructure, a challenge that currently limits many scientific applications. We use the pilot-abstraction, a widely used resource management abstraction for high-performance, high throughput, big data, and streaming applications, as a case study. We evaluate the activities of the process and extensively evaluate the artifacts using different methods, including conceptual modeling, performance characterizations, and modeling. We demonstrate the applicability of the DSR method for holistically handling the complexity of parallel and distributed computing environments addressing important application, system, and infrastructure challenges of scientific applications. Finally, we capture our experiences and formulate different lessons learned. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09009">PDF</a>
<h3>No. 4	Distributed Mean Estimation with Optimal Error Bounds</h3><h4>Dan Alistarh, Saleh Ashkboos, Peter Davies</h4> Abstract: Motivated by applications to distributed optimization and machine learning, we consider the distributed mean estimation problem, in which $n$ nodes are each assigned a multi-dimensional input vector, and must cooperate to estimate the mean of the input vectors, while minimizing communication. In this paper, we provide the first tight bounds for this problem, in terms of the trade-off between the amount of communication between nodes and the variance of the node estimates relative to the true value of the mean. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09268">PDF</a>
<h3>No. 5	A polynomial lower bound on adaptive complexity of submodular  maximization</h3><h4>Wenzheng Li, Paul Liu, Jan Vondrak</h4> Abstract: In large-data applications, it is desirable to design algorithms with a high degree of parallelization. In the context of submodular optimization, adaptive complexity has become a widely-used measure of an algorithm's "sequentiality". Algorithms in the adaptive model proceed in rounds, and can issue polynomially many queries to a function $f$ in each round. The queries in each round must be independent, produced by a computation that depends only on query results obtained in previous rounds. In this work, we examine two fundamental variants of submodular maximization in the adaptive complexity model: cardinality-constrained monotone maximization, and unconstrained non-monotone maximization. Our main result is that an $r$-round algorithm for cardinality-constrained monotone maximization cannot achieve a factor better than $1 - 1/e - \Omega(\min \{ \frac{1}{r}, \frac{\log^2 n}{r^3} \})$, for any $r < n^c$ (where $c>0$ is some constant). This is the first result showing that the number of rounds must blow up polynomially large as we approach the optimal factor of $1-1/e$. For the unconstrained non-monotone maximization problem, we show a positive result: For every instance, and every $\delta>0$, either we obtain a $(1/2-\delta)$-approximation in $1$ round, or a $(1/2+\Omega(\delta^2))$-approximation in $O(1/\delta^2)$ rounds. In particular (in contrast to the cardinality-constrained case), there cannot be an instance where (i) it is impossible to achieve a factor better than $1/2$ regardless of the number of rounds, and (ii) it takes $r$ rounds to achieve a factor of $1/2-O(1/r)$. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09130">PDF</a>
<h3>No. 6	Asynchronous parallel adaptive stochastic gradient methods</h3><h4>Yangyang Xu, Colin Sutcher-Shepard, Yibo Xu, Jie Chen</h4> Abstract: Stochastic gradient methods (SGMs) are the predominant approaches to train deep learning models. The adaptive versions (e.g., Adam and AMSGrad) have been extensively used in practice, partly because they achieve faster convergence than the non-adaptive versions while incurring little overhead. On the other hand, asynchronous (async) parallel computing has exhibited much better speed-up over its synchronous (sync) counterpart. However, async-parallel implementation has only been demonstrated to the non-adaptive SGMs. The difficulty for adaptive SGMs originates from the second moment term that makes the convergence analysis challenging with async updates. In this paper, we propose an async-parallel adaptive SGM based on AMSGrad. We show that the proposed method inherits the convergence guarantee of AMSGrad for both convex and non-convex problems, if the staleness (also called delay) caused by asynchrony is bounded. Our convergence rate results indicate a nearly linear parallelization speed-up if $\tau=o(K^{\frac{1}{4}})$, where $\tau$ is the staleness and $K$ is the number of iterations. The proposed method is tested on both convex and non-convex machine learning problems, and the numerical results demonstrate its clear advantages over the sync counterpart. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09095">PDF</a>
<h3>No. 7	Uncertainty Principle for Communication Compression in Distributed and  Federated Learning and the Search for an Optimal Compressor</h3><h4>Mher Safaryan, Egor Shulgin, Peter Richt√°rik</h4> Abstract: In order to mitigate the high communication cost in distributed and federated learning, various vector compression schemes, such as quantization, sparsification and dithering, have become very popular. In designing a compression method, one aims to communicate as few bits as possible, which minimizes the cost per communication round, while at the same time attempting to impart as little distortion (variance) to the communicated messages as possible, which minimizes the adverse effect of the compression on the overall number of communication rounds. However, intuitively, these two goals are fundamentally in conflict: the more compression we allow, the more distorted the messages become. We formalize this intuition and prove an {\em uncertainty principle} for randomized compression operators, thus quantifying this limitation mathematically, and {\em effectively providing lower bounds on what might be achievable with communication compression}. Motivated by these developments, we call for the search for the optimal compression operator. In an attempt to take a first step in this direction, we construct a new unbiased compression method inspired by the Kashin representation of vectors, which we call {\em Kashin compression (KC)}. In contrast to all previously proposed compression mechanisms, we prove that KC enjoys a {\em dimension independent} variance bound with an explicit formula even in the regime when only a few bits need to be communicate per each vector entry. We show how KC can be provably and efficiently combined with several existing optimization algorithms, in all cases leading to communication complexity improvements on previous state of the art. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08958">PDF</a><h2>2020-03-08</h2>
<h3>No. 1	Fast Implementation of Morphological Filtering Using ARM NEON Extension</h3><h4>Elena Limonova, Arseny Terekhin, Dmitry Nikolaev, Vladimir Arlazarov</h4> Abstract: In this paper we consider speedup potential of morphological image filtering on ARM processors. Morphological operations are widely used in image analysis and recognition and their speedup in some cases can significantly reduce overall execution time of recognition. More specifically, we propose fast implementation of erosion and dilation using ARM SIMD extension NEON. These operations with the rectangular structuring element are separable. They were implemented using the advantages of separability as sequential horizontal and vertical passes. Each pass was implemented using van Herk/Gil-Werman algorithm for large windows and low-constant linear complexity algorithm for small windows. Final implementation was improved with SIMD and used a combination of these methods. We also considered fast transpose implementation of 8x8 and 16x16 matrices using ARM NEON to get additional computational gain for morphological operations. Experiments showed 3 times efficiency increase for final implementation of erosion and dilation compared to van Herk/Gil-Werman algorithm without SIMD, 5.7 times speedup for 8x8 matrix transpose and 12 times speedup for 16x16 matrix transpose compared to transpose without SIMD. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09474">PDF</a>
<h3>No. 2	Faasm: Lightweight Isolation for Efficient Stateful Serverless Computing</h3><h4>Simon Shillaker, Peter Pietzuch</h4> Abstract: Serverless computing is an excellent fit for big data processing because it can scale quickly and cheaply to thousands of parallel functions. Existing serverless platforms, however, isolate functions in ephemeral, stateless containers. This means that functions cannot efficiently share memory, forcing users to serialise data repeatedly when composing functions. We observe that container-based isolation is ill-suited for serverless big data processing. Instead it requires a lightweight isolation approach that allows for efficient state sharing. We introduce Faaslets, a new isolation abstraction for serverless big data computing. Faaslets isolate the memory of executed functions using software-fault isolation (SFI), as provided by WebAssembly, while allowing memory regions to be shared between functions in the same address space. Faaslets can thus avoid expensive data movement when functions are co-located on the same machine. Our runtime for Faaslets, Faasm, isolates other resources, e.g. CPU and network, using standard Linux cgroups, and provides a low-level POSIX host interface for networking, file system access and dynamic loading. To reduce initialisation times, Faasm restores Faaslets from already-initialised snapshots. We compare Faasm to a standard container-based platform and show that, when training a machine learning model, it achieves a 2x speed-up with 10x less memory; for serving machine learning inference, Faasm doubles the throughput and reduces tail latency by 90%. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09344">PDF</a>
<h3>No. 3	Methods and Experiences for Developing Abstractions for Data-intensive,  Scientific Applications</h3><h4>Andre Luckow, Shantenu Jha</h4> Abstract: Developing software for scientific applications that require the integration of diverse types of computing, instruments and data present challenges that are distinct from commercial software, due to scale, heterogeneity, and the need to integrate various programming and computational models with evolving and heterogeneous infrastructure. Pervasive and effective abstractions are thus critical. The process of developing abstractions for scientific applications and infrastructures is not well understood. While theory-based approaches are suited for well-defined, closed environments, they have severe limitations for designing abstractions for complex, real-world systems. The design science research (DSR) method provides the basis for designing effective systems that can handle real-world complexities. DSR consists of two complementary phases: design and evaluation. This paper applies the DSR method to the development of abstractions for scientific applications. Specifically, we address the critical problem of distributed resource management on heterogeneous infrastructure, a challenge that currently limits many scientific applications. We use the pilot-abstraction, a widely used resource management abstraction for high-performance, high throughput, big data, and streaming applications, as a case study. We evaluate the activities of the process and extensively evaluate the artifacts using different methods, including conceptual modeling, performance characterizations, and modeling. We demonstrate the applicability of the DSR method for holistically handling the complexity of parallel and distributed computing environments addressing important application, system, and infrastructure challenges of scientific applications. Finally, we capture our experiences and formulate different lessons learned. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09009">PDF</a>
<h3>No. 4	Distributed Mean Estimation with Optimal Error Bounds</h3><h4>Dan Alistarh, Saleh Ashkboos, Peter Davies</h4> Abstract: Motivated by applications to distributed optimization and machine learning, we consider the distributed mean estimation problem, in which $n$ nodes are each assigned a multi-dimensional input vector, and must cooperate to estimate the mean of the input vectors, while minimizing communication. In this paper, we provide the first tight bounds for this problem, in terms of the trade-off between the amount of communication between nodes and the variance of the node estimates relative to the true value of the mean. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09268">PDF</a>
<h3>No. 5	A polynomial lower bound on adaptive complexity of submodular  maximization</h3><h4>Wenzheng Li, Paul Liu, Jan Vondrak</h4> Abstract: In large-data applications, it is desirable to design algorithms with a high degree of parallelization. In the context of submodular optimization, adaptive complexity has become a widely-used measure of an algorithm's "sequentiality". Algorithms in the adaptive model proceed in rounds, and can issue polynomially many queries to a function $f$ in each round. The queries in each round must be independent, produced by a computation that depends only on query results obtained in previous rounds. In this work, we examine two fundamental variants of submodular maximization in the adaptive complexity model: cardinality-constrained monotone maximization, and unconstrained non-monotone maximization. Our main result is that an $r$-round algorithm for cardinality-constrained monotone maximization cannot achieve a factor better than $1 - 1/e - \Omega(\min \{ \frac{1}{r}, \frac{\log^2 n}{r^3} \})$, for any $r < n^c$ (where $c>0$ is some constant). This is the first result showing that the number of rounds must blow up polynomially large as we approach the optimal factor of $1-1/e$. For the unconstrained non-monotone maximization problem, we show a positive result: For every instance, and every $\delta>0$, either we obtain a $(1/2-\delta)$-approximation in $1$ round, or a $(1/2+\Omega(\delta^2))$-approximation in $O(1/\delta^2)$ rounds. In particular (in contrast to the cardinality-constrained case), there cannot be an instance where (i) it is impossible to achieve a factor better than $1/2$ regardless of the number of rounds, and (ii) it takes $r$ rounds to achieve a factor of $1/2-O(1/r)$. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09130">PDF</a>
<h3>No. 6	Asynchronous parallel adaptive stochastic gradient methods</h3><h4>Yangyang Xu, Colin Sutcher-Shepard, Yibo Xu, Jie Chen</h4> Abstract: Stochastic gradient methods (SGMs) are the predominant approaches to train deep learning models. The adaptive versions (e.g., Adam and AMSGrad) have been extensively used in practice, partly because they achieve faster convergence than the non-adaptive versions while incurring little overhead. On the other hand, asynchronous (async) parallel computing has exhibited much better speed-up over its synchronous (sync) counterpart. However, async-parallel implementation has only been demonstrated to the non-adaptive SGMs. The difficulty for adaptive SGMs originates from the second moment term that makes the convergence analysis challenging with async updates. In this paper, we propose an async-parallel adaptive SGM based on AMSGrad. We show that the proposed method inherits the convergence guarantee of AMSGrad for both convex and non-convex problems, if the staleness (also called delay) caused by asynchrony is bounded. Our convergence rate results indicate a nearly linear parallelization speed-up if $\tau=o(K^{\frac{1}{4}})$, where $\tau$ is the staleness and $K$ is the number of iterations. The proposed method is tested on both convex and non-convex machine learning problems, and the numerical results demonstrate its clear advantages over the sync counterpart. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09095">PDF</a>
<h3>No. 7	Uncertainty Principle for Communication Compression in Distributed and  Federated Learning and the Search for an Optimal Compressor</h3><h4>Mher Safaryan, Egor Shulgin, Peter Richt√°rik</h4> Abstract: In order to mitigate the high communication cost in distributed and federated learning, various vector compression schemes, such as quantization, sparsification and dithering, have become very popular. In designing a compression method, one aims to communicate as few bits as possible, which minimizes the cost per communication round, while at the same time attempting to impart as little distortion (variance) to the communicated messages as possible, which minimizes the adverse effect of the compression on the overall number of communication rounds. However, intuitively, these two goals are fundamentally in conflict: the more compression we allow, the more distorted the messages become. We formalize this intuition and prove an {\em uncertainty principle} for randomized compression operators, thus quantifying this limitation mathematically, and {\em effectively providing lower bounds on what might be achievable with communication compression}. Motivated by these developments, we call for the search for the optimal compression operator. In an attempt to take a first step in this direction, we construct a new unbiased compression method inspired by the Kashin representation of vectors, which we call {\em Kashin compression (KC)}. In contrast to all previously proposed compression mechanisms, we prove that KC enjoys a {\em dimension independent} variance bound with an explicit formula even in the regime when only a few bits need to be communicate per each vector entry. We show how KC can be provably and efficiently combined with several existing optimization algorithms, in all cases leading to communication complexity improvements on previous state of the art. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08958">PDF</a><h2>2020-03-07</h2>
<h3>No. 1	Fast Implementation of Morphological Filtering Using ARM NEON Extension</h3><h4>Elena Limonova, Arseny Terekhin, Dmitry Nikolaev, Vladimir Arlazarov</h4> Abstract: In this paper we consider speedup potential of morphological image filtering on ARM processors. Morphological operations are widely used in image analysis and recognition and their speedup in some cases can significantly reduce overall execution time of recognition. More specifically, we propose fast implementation of erosion and dilation using ARM SIMD extension NEON. These operations with the rectangular structuring element are separable. They were implemented using the advantages of separability as sequential horizontal and vertical passes. Each pass was implemented using van Herk/Gil-Werman algorithm for large windows and low-constant linear complexity algorithm for small windows. Final implementation was improved with SIMD and used a combination of these methods. We also considered fast transpose implementation of 8x8 and 16x16 matrices using ARM NEON to get additional computational gain for morphological operations. Experiments showed 3 times efficiency increase for final implementation of erosion and dilation compared to van Herk/Gil-Werman algorithm without SIMD, 5.7 times speedup for 8x8 matrix transpose and 12 times speedup for 16x16 matrix transpose compared to transpose without SIMD. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09474">PDF</a>
<h3>No. 2	Faasm: Lightweight Isolation for Efficient Stateful Serverless Computing</h3><h4>Simon Shillaker, Peter Pietzuch</h4> Abstract: Serverless computing is an excellent fit for big data processing because it can scale quickly and cheaply to thousands of parallel functions. Existing serverless platforms, however, isolate functions in ephemeral, stateless containers. This means that functions cannot efficiently share memory, forcing users to serialise data repeatedly when composing functions. We observe that container-based isolation is ill-suited for serverless big data processing. Instead it requires a lightweight isolation approach that allows for efficient state sharing. We introduce Faaslets, a new isolation abstraction for serverless big data computing. Faaslets isolate the memory of executed functions using software-fault isolation (SFI), as provided by WebAssembly, while allowing memory regions to be shared between functions in the same address space. Faaslets can thus avoid expensive data movement when functions are co-located on the same machine. Our runtime for Faaslets, Faasm, isolates other resources, e.g. CPU and network, using standard Linux cgroups, and provides a low-level POSIX host interface for networking, file system access and dynamic loading. To reduce initialisation times, Faasm restores Faaslets from already-initialised snapshots. We compare Faasm to a standard container-based platform and show that, when training a machine learning model, it achieves a 2x speed-up with 10x less memory; for serving machine learning inference, Faasm doubles the throughput and reduces tail latency by 90%. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09344">PDF</a>
<h3>No. 3	Methods and Experiences for Developing Abstractions for Data-intensive,  Scientific Applications</h3><h4>Andre Luckow, Shantenu Jha</h4> Abstract: Developing software for scientific applications that require the integration of diverse types of computing, instruments and data present challenges that are distinct from commercial software, due to scale, heterogeneity, and the need to integrate various programming and computational models with evolving and heterogeneous infrastructure. Pervasive and effective abstractions are thus critical. The process of developing abstractions for scientific applications and infrastructures is not well understood. While theory-based approaches are suited for well-defined, closed environments, they have severe limitations for designing abstractions for complex, real-world systems. The design science research (DSR) method provides the basis for designing effective systems that can handle real-world complexities. DSR consists of two complementary phases: design and evaluation. This paper applies the DSR method to the development of abstractions for scientific applications. Specifically, we address the critical problem of distributed resource management on heterogeneous infrastructure, a challenge that currently limits many scientific applications. We use the pilot-abstraction, a widely used resource management abstraction for high-performance, high throughput, big data, and streaming applications, as a case study. We evaluate the activities of the process and extensively evaluate the artifacts using different methods, including conceptual modeling, performance characterizations, and modeling. We demonstrate the applicability of the DSR method for holistically handling the complexity of parallel and distributed computing environments addressing important application, system, and infrastructure challenges of scientific applications. Finally, we capture our experiences and formulate different lessons learned. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09009">PDF</a>
<h3>No. 4	Distributed Mean Estimation with Optimal Error Bounds</h3><h4>Dan Alistarh, Saleh Ashkboos, Peter Davies</h4> Abstract: Motivated by applications to distributed optimization and machine learning, we consider the distributed mean estimation problem, in which $n$ nodes are each assigned a multi-dimensional input vector, and must cooperate to estimate the mean of the input vectors, while minimizing communication. In this paper, we provide the first tight bounds for this problem, in terms of the trade-off between the amount of communication between nodes and the variance of the node estimates relative to the true value of the mean. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09268">PDF</a>
<h3>No. 5	A polynomial lower bound on adaptive complexity of submodular  maximization</h3><h4>Wenzheng Li, Paul Liu, Jan Vondrak</h4> Abstract: In large-data applications, it is desirable to design algorithms with a high degree of parallelization. In the context of submodular optimization, adaptive complexity has become a widely-used measure of an algorithm's "sequentiality". Algorithms in the adaptive model proceed in rounds, and can issue polynomially many queries to a function $f$ in each round. The queries in each round must be independent, produced by a computation that depends only on query results obtained in previous rounds. In this work, we examine two fundamental variants of submodular maximization in the adaptive complexity model: cardinality-constrained monotone maximization, and unconstrained non-monotone maximization. Our main result is that an $r$-round algorithm for cardinality-constrained monotone maximization cannot achieve a factor better than $1 - 1/e - \Omega(\min \{ \frac{1}{r}, \frac{\log^2 n}{r^3} \})$, for any $r < n^c$ (where $c>0$ is some constant). This is the first result showing that the number of rounds must blow up polynomially large as we approach the optimal factor of $1-1/e$. For the unconstrained non-monotone maximization problem, we show a positive result: For every instance, and every $\delta>0$, either we obtain a $(1/2-\delta)$-approximation in $1$ round, or a $(1/2+\Omega(\delta^2))$-approximation in $O(1/\delta^2)$ rounds. In particular (in contrast to the cardinality-constrained case), there cannot be an instance where (i) it is impossible to achieve a factor better than $1/2$ regardless of the number of rounds, and (ii) it takes $r$ rounds to achieve a factor of $1/2-O(1/r)$. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09130">PDF</a>
<h3>No. 6	Asynchronous parallel adaptive stochastic gradient methods</h3><h4>Yangyang Xu, Colin Sutcher-Shepard, Yibo Xu, Jie Chen</h4> Abstract: Stochastic gradient methods (SGMs) are the predominant approaches to train deep learning models. The adaptive versions (e.g., Adam and AMSGrad) have been extensively used in practice, partly because they achieve faster convergence than the non-adaptive versions while incurring little overhead. On the other hand, asynchronous (async) parallel computing has exhibited much better speed-up over its synchronous (sync) counterpart. However, async-parallel implementation has only been demonstrated to the non-adaptive SGMs. The difficulty for adaptive SGMs originates from the second moment term that makes the convergence analysis challenging with async updates. In this paper, we propose an async-parallel adaptive SGM based on AMSGrad. We show that the proposed method inherits the convergence guarantee of AMSGrad for both convex and non-convex problems, if the staleness (also called delay) caused by asynchrony is bounded. Our convergence rate results indicate a nearly linear parallelization speed-up if $\tau=o(K^{\frac{1}{4}})$, where $\tau$ is the staleness and $K$ is the number of iterations. The proposed method is tested on both convex and non-convex machine learning problems, and the numerical results demonstrate its clear advantages over the sync counterpart. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09095">PDF</a>
<h3>No. 7	Uncertainty Principle for Communication Compression in Distributed and  Federated Learning and the Search for an Optimal Compressor</h3><h4>Mher Safaryan, Egor Shulgin, Peter Richt√°rik</h4> Abstract: In order to mitigate the high communication cost in distributed and federated learning, various vector compression schemes, such as quantization, sparsification and dithering, have become very popular. In designing a compression method, one aims to communicate as few bits as possible, which minimizes the cost per communication round, while at the same time attempting to impart as little distortion (variance) to the communicated messages as possible, which minimizes the adverse effect of the compression on the overall number of communication rounds. However, intuitively, these two goals are fundamentally in conflict: the more compression we allow, the more distorted the messages become. We formalize this intuition and prove an {\em uncertainty principle} for randomized compression operators, thus quantifying this limitation mathematically, and {\em effectively providing lower bounds on what might be achievable with communication compression}. Motivated by these developments, we call for the search for the optimal compression operator. In an attempt to take a first step in this direction, we construct a new unbiased compression method inspired by the Kashin representation of vectors, which we call {\em Kashin compression (KC)}. In contrast to all previously proposed compression mechanisms, we prove that KC enjoys a {\em dimension independent} variance bound with an explicit formula even in the regime when only a few bits need to be communicate per each vector entry. We show how KC can be provably and efficiently combined with several existing optimization algorithms, in all cases leading to communication complexity improvements on previous state of the art. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08958">PDF</a><h2>2020-03-06</h2>
<h3>No. 1	Fast Implementation of Morphological Filtering Using ARM NEON Extension</h3><h4>Elena Limonova, Arseny Terekhin, Dmitry Nikolaev, Vladimir Arlazarov</h4> Abstract: In this paper we consider speedup potential of morphological image filtering on ARM processors. Morphological operations are widely used in image analysis and recognition and their speedup in some cases can significantly reduce overall execution time of recognition. More specifically, we propose fast implementation of erosion and dilation using ARM SIMD extension NEON. These operations with the rectangular structuring element are separable. They were implemented using the advantages of separability as sequential horizontal and vertical passes. Each pass was implemented using van Herk/Gil-Werman algorithm for large windows and low-constant linear complexity algorithm for small windows. Final implementation was improved with SIMD and used a combination of these methods. We also considered fast transpose implementation of 8x8 and 16x16 matrices using ARM NEON to get additional computational gain for morphological operations. Experiments showed 3 times efficiency increase for final implementation of erosion and dilation compared to van Herk/Gil-Werman algorithm without SIMD, 5.7 times speedup for 8x8 matrix transpose and 12 times speedup for 16x16 matrix transpose compared to transpose without SIMD. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09474">PDF</a>
<h3>No. 2	Faasm: Lightweight Isolation for Efficient Stateful Serverless Computing</h3><h4>Simon Shillaker, Peter Pietzuch</h4> Abstract: Serverless computing is an excellent fit for big data processing because it can scale quickly and cheaply to thousands of parallel functions. Existing serverless platforms, however, isolate functions in ephemeral, stateless containers. This means that functions cannot efficiently share memory, forcing users to serialise data repeatedly when composing functions. We observe that container-based isolation is ill-suited for serverless big data processing. Instead it requires a lightweight isolation approach that allows for efficient state sharing. We introduce Faaslets, a new isolation abstraction for serverless big data computing. Faaslets isolate the memory of executed functions using software-fault isolation (SFI), as provided by WebAssembly, while allowing memory regions to be shared between functions in the same address space. Faaslets can thus avoid expensive data movement when functions are co-located on the same machine. Our runtime for Faaslets, Faasm, isolates other resources, e.g. CPU and network, using standard Linux cgroups, and provides a low-level POSIX host interface for networking, file system access and dynamic loading. To reduce initialisation times, Faasm restores Faaslets from already-initialised snapshots. We compare Faasm to a standard container-based platform and show that, when training a machine learning model, it achieves a 2x speed-up with 10x less memory; for serving machine learning inference, Faasm doubles the throughput and reduces tail latency by 90%. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09344">PDF</a>
<h3>No. 3	Methods and Experiences for Developing Abstractions for Data-intensive,  Scientific Applications</h3><h4>Andre Luckow, Shantenu Jha</h4> Abstract: Developing software for scientific applications that require the integration of diverse types of computing, instruments and data present challenges that are distinct from commercial software, due to scale, heterogeneity, and the need to integrate various programming and computational models with evolving and heterogeneous infrastructure. Pervasive and effective abstractions are thus critical. The process of developing abstractions for scientific applications and infrastructures is not well understood. While theory-based approaches are suited for well-defined, closed environments, they have severe limitations for designing abstractions for complex, real-world systems. The design science research (DSR) method provides the basis for designing effective systems that can handle real-world complexities. DSR consists of two complementary phases: design and evaluation. This paper applies the DSR method to the development of abstractions for scientific applications. Specifically, we address the critical problem of distributed resource management on heterogeneous infrastructure, a challenge that currently limits many scientific applications. We use the pilot-abstraction, a widely used resource management abstraction for high-performance, high throughput, big data, and streaming applications, as a case study. We evaluate the activities of the process and extensively evaluate the artifacts using different methods, including conceptual modeling, performance characterizations, and modeling. We demonstrate the applicability of the DSR method for holistically handling the complexity of parallel and distributed computing environments addressing important application, system, and infrastructure challenges of scientific applications. Finally, we capture our experiences and formulate different lessons learned. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09009">PDF</a>
<h3>No. 4	Distributed Mean Estimation with Optimal Error Bounds</h3><h4>Dan Alistarh, Saleh Ashkboos, Peter Davies</h4> Abstract: Motivated by applications to distributed optimization and machine learning, we consider the distributed mean estimation problem, in which $n$ nodes are each assigned a multi-dimensional input vector, and must cooperate to estimate the mean of the input vectors, while minimizing communication. In this paper, we provide the first tight bounds for this problem, in terms of the trade-off between the amount of communication between nodes and the variance of the node estimates relative to the true value of the mean. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09268">PDF</a>
<h3>No. 5	A polynomial lower bound on adaptive complexity of submodular  maximization</h3><h4>Wenzheng Li, Paul Liu, Jan Vondrak</h4> Abstract: In large-data applications, it is desirable to design algorithms with a high degree of parallelization. In the context of submodular optimization, adaptive complexity has become a widely-used measure of an algorithm's "sequentiality". Algorithms in the adaptive model proceed in rounds, and can issue polynomially many queries to a function $f$ in each round. The queries in each round must be independent, produced by a computation that depends only on query results obtained in previous rounds. In this work, we examine two fundamental variants of submodular maximization in the adaptive complexity model: cardinality-constrained monotone maximization, and unconstrained non-monotone maximization. Our main result is that an $r$-round algorithm for cardinality-constrained monotone maximization cannot achieve a factor better than $1 - 1/e - \Omega(\min \{ \frac{1}{r}, \frac{\log^2 n}{r^3} \})$, for any $r < n^c$ (where $c>0$ is some constant). This is the first result showing that the number of rounds must blow up polynomially large as we approach the optimal factor of $1-1/e$. For the unconstrained non-monotone maximization problem, we show a positive result: For every instance, and every $\delta>0$, either we obtain a $(1/2-\delta)$-approximation in $1$ round, or a $(1/2+\Omega(\delta^2))$-approximation in $O(1/\delta^2)$ rounds. In particular (in contrast to the cardinality-constrained case), there cannot be an instance where (i) it is impossible to achieve a factor better than $1/2$ regardless of the number of rounds, and (ii) it takes $r$ rounds to achieve a factor of $1/2-O(1/r)$. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09130">PDF</a>
<h3>No. 6	Asynchronous parallel adaptive stochastic gradient methods</h3><h4>Yangyang Xu, Colin Sutcher-Shepard, Yibo Xu, Jie Chen</h4> Abstract: Stochastic gradient methods (SGMs) are the predominant approaches to train deep learning models. The adaptive versions (e.g., Adam and AMSGrad) have been extensively used in practice, partly because they achieve faster convergence than the non-adaptive versions while incurring little overhead. On the other hand, asynchronous (async) parallel computing has exhibited much better speed-up over its synchronous (sync) counterpart. However, async-parallel implementation has only been demonstrated to the non-adaptive SGMs. The difficulty for adaptive SGMs originates from the second moment term that makes the convergence analysis challenging with async updates. In this paper, we propose an async-parallel adaptive SGM based on AMSGrad. We show that the proposed method inherits the convergence guarantee of AMSGrad for both convex and non-convex problems, if the staleness (also called delay) caused by asynchrony is bounded. Our convergence rate results indicate a nearly linear parallelization speed-up if $\tau=o(K^{\frac{1}{4}})$, where $\tau$ is the staleness and $K$ is the number of iterations. The proposed method is tested on both convex and non-convex machine learning problems, and the numerical results demonstrate its clear advantages over the sync counterpart. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09095">PDF</a>
<h3>No. 7	Uncertainty Principle for Communication Compression in Distributed and  Federated Learning and the Search for an Optimal Compressor</h3><h4>Mher Safaryan, Egor Shulgin, Peter Richt√°rik</h4> Abstract: In order to mitigate the high communication cost in distributed and federated learning, various vector compression schemes, such as quantization, sparsification and dithering, have become very popular. In designing a compression method, one aims to communicate as few bits as possible, which minimizes the cost per communication round, while at the same time attempting to impart as little distortion (variance) to the communicated messages as possible, which minimizes the adverse effect of the compression on the overall number of communication rounds. However, intuitively, these two goals are fundamentally in conflict: the more compression we allow, the more distorted the messages become. We formalize this intuition and prove an {\em uncertainty principle} for randomized compression operators, thus quantifying this limitation mathematically, and {\em effectively providing lower bounds on what might be achievable with communication compression}. Motivated by these developments, we call for the search for the optimal compression operator. In an attempt to take a first step in this direction, we construct a new unbiased compression method inspired by the Kashin representation of vectors, which we call {\em Kashin compression (KC)}. In contrast to all previously proposed compression mechanisms, we prove that KC enjoys a {\em dimension independent} variance bound with an explicit formula even in the regime when only a few bits need to be communicate per each vector entry. We show how KC can be provably and efficiently combined with several existing optimization algorithms, in all cases leading to communication complexity improvements on previous state of the art. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08958">PDF</a><h2>2020-03-05</h2>
<h3>No. 1	Fast Implementation of Morphological Filtering Using ARM NEON Extension</h3><h4>Elena Limonova, Arseny Terekhin, Dmitry Nikolaev, Vladimir Arlazarov</h4> Abstract: In this paper we consider speedup potential of morphological image filtering on ARM processors. Morphological operations are widely used in image analysis and recognition and their speedup in some cases can significantly reduce overall execution time of recognition. More specifically, we propose fast implementation of erosion and dilation using ARM SIMD extension NEON. These operations with the rectangular structuring element are separable. They were implemented using the advantages of separability as sequential horizontal and vertical passes. Each pass was implemented using van Herk/Gil-Werman algorithm for large windows and low-constant linear complexity algorithm for small windows. Final implementation was improved with SIMD and used a combination of these methods. We also considered fast transpose implementation of 8x8 and 16x16 matrices using ARM NEON to get additional computational gain for morphological operations. Experiments showed 3 times efficiency increase for final implementation of erosion and dilation compared to van Herk/Gil-Werman algorithm without SIMD, 5.7 times speedup for 8x8 matrix transpose and 12 times speedup for 16x16 matrix transpose compared to transpose without SIMD. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09474">PDF</a>
<h3>No. 2	Faasm: Lightweight Isolation for Efficient Stateful Serverless Computing</h3><h4>Simon Shillaker, Peter Pietzuch</h4> Abstract: Serverless computing is an excellent fit for big data processing because it can scale quickly and cheaply to thousands of parallel functions. Existing serverless platforms, however, isolate functions in ephemeral, stateless containers. This means that functions cannot efficiently share memory, forcing users to serialise data repeatedly when composing functions. We observe that container-based isolation is ill-suited for serverless big data processing. Instead it requires a lightweight isolation approach that allows for efficient state sharing. We introduce Faaslets, a new isolation abstraction for serverless big data computing. Faaslets isolate the memory of executed functions using software-fault isolation (SFI), as provided by WebAssembly, while allowing memory regions to be shared between functions in the same address space. Faaslets can thus avoid expensive data movement when functions are co-located on the same machine. Our runtime for Faaslets, Faasm, isolates other resources, e.g. CPU and network, using standard Linux cgroups, and provides a low-level POSIX host interface for networking, file system access and dynamic loading. To reduce initialisation times, Faasm restores Faaslets from already-initialised snapshots. We compare Faasm to a standard container-based platform and show that, when training a machine learning model, it achieves a 2x speed-up with 10x less memory; for serving machine learning inference, Faasm doubles the throughput and reduces tail latency by 90%. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09344">PDF</a>
<h3>No. 3	Methods and Experiences for Developing Abstractions for Data-intensive,  Scientific Applications</h3><h4>Andre Luckow, Shantenu Jha</h4> Abstract: Developing software for scientific applications that require the integration of diverse types of computing, instruments and data present challenges that are distinct from commercial software, due to scale, heterogeneity, and the need to integrate various programming and computational models with evolving and heterogeneous infrastructure. Pervasive and effective abstractions are thus critical. The process of developing abstractions for scientific applications and infrastructures is not well understood. While theory-based approaches are suited for well-defined, closed environments, they have severe limitations for designing abstractions for complex, real-world systems. The design science research (DSR) method provides the basis for designing effective systems that can handle real-world complexities. DSR consists of two complementary phases: design and evaluation. This paper applies the DSR method to the development of abstractions for scientific applications. Specifically, we address the critical problem of distributed resource management on heterogeneous infrastructure, a challenge that currently limits many scientific applications. We use the pilot-abstraction, a widely used resource management abstraction for high-performance, high throughput, big data, and streaming applications, as a case study. We evaluate the activities of the process and extensively evaluate the artifacts using different methods, including conceptual modeling, performance characterizations, and modeling. We demonstrate the applicability of the DSR method for holistically handling the complexity of parallel and distributed computing environments addressing important application, system, and infrastructure challenges of scientific applications. Finally, we capture our experiences and formulate different lessons learned. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09009">PDF</a>
<h3>No. 4	Distributed Mean Estimation with Optimal Error Bounds</h3><h4>Dan Alistarh, Saleh Ashkboos, Peter Davies</h4> Abstract: Motivated by applications to distributed optimization and machine learning, we consider the distributed mean estimation problem, in which $n$ nodes are each assigned a multi-dimensional input vector, and must cooperate to estimate the mean of the input vectors, while minimizing communication. In this paper, we provide the first tight bounds for this problem, in terms of the trade-off between the amount of communication between nodes and the variance of the node estimates relative to the true value of the mean. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09268">PDF</a>
<h3>No. 5	A polynomial lower bound on adaptive complexity of submodular  maximization</h3><h4>Wenzheng Li, Paul Liu, Jan Vondrak</h4> Abstract: In large-data applications, it is desirable to design algorithms with a high degree of parallelization. In the context of submodular optimization, adaptive complexity has become a widely-used measure of an algorithm's "sequentiality". Algorithms in the adaptive model proceed in rounds, and can issue polynomially many queries to a function $f$ in each round. The queries in each round must be independent, produced by a computation that depends only on query results obtained in previous rounds. In this work, we examine two fundamental variants of submodular maximization in the adaptive complexity model: cardinality-constrained monotone maximization, and unconstrained non-monotone maximization. Our main result is that an $r$-round algorithm for cardinality-constrained monotone maximization cannot achieve a factor better than $1 - 1/e - \Omega(\min \{ \frac{1}{r}, \frac{\log^2 n}{r^3} \})$, for any $r < n^c$ (where $c>0$ is some constant). This is the first result showing that the number of rounds must blow up polynomially large as we approach the optimal factor of $1-1/e$. For the unconstrained non-monotone maximization problem, we show a positive result: For every instance, and every $\delta>0$, either we obtain a $(1/2-\delta)$-approximation in $1$ round, or a $(1/2+\Omega(\delta^2))$-approximation in $O(1/\delta^2)$ rounds. In particular (in contrast to the cardinality-constrained case), there cannot be an instance where (i) it is impossible to achieve a factor better than $1/2$ regardless of the number of rounds, and (ii) it takes $r$ rounds to achieve a factor of $1/2-O(1/r)$. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09130">PDF</a>
<h3>No. 6	Asynchronous parallel adaptive stochastic gradient methods</h3><h4>Yangyang Xu, Colin Sutcher-Shepard, Yibo Xu, Jie Chen</h4> Abstract: Stochastic gradient methods (SGMs) are the predominant approaches to train deep learning models. The adaptive versions (e.g., Adam and AMSGrad) have been extensively used in practice, partly because they achieve faster convergence than the non-adaptive versions while incurring little overhead. On the other hand, asynchronous (async) parallel computing has exhibited much better speed-up over its synchronous (sync) counterpart. However, async-parallel implementation has only been demonstrated to the non-adaptive SGMs. The difficulty for adaptive SGMs originates from the second moment term that makes the convergence analysis challenging with async updates. In this paper, we propose an async-parallel adaptive SGM based on AMSGrad. We show that the proposed method inherits the convergence guarantee of AMSGrad for both convex and non-convex problems, if the staleness (also called delay) caused by asynchrony is bounded. Our convergence rate results indicate a nearly linear parallelization speed-up if $\tau=o(K^{\frac{1}{4}})$, where $\tau$ is the staleness and $K$ is the number of iterations. The proposed method is tested on both convex and non-convex machine learning problems, and the numerical results demonstrate its clear advantages over the sync counterpart. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09095">PDF</a>
<h3>No. 7	Uncertainty Principle for Communication Compression in Distributed and  Federated Learning and the Search for an Optimal Compressor</h3><h4>Mher Safaryan, Egor Shulgin, Peter Richt√°rik</h4> Abstract: In order to mitigate the high communication cost in distributed and federated learning, various vector compression schemes, such as quantization, sparsification and dithering, have become very popular. In designing a compression method, one aims to communicate as few bits as possible, which minimizes the cost per communication round, while at the same time attempting to impart as little distortion (variance) to the communicated messages as possible, which minimizes the adverse effect of the compression on the overall number of communication rounds. However, intuitively, these two goals are fundamentally in conflict: the more compression we allow, the more distorted the messages become. We formalize this intuition and prove an {\em uncertainty principle} for randomized compression operators, thus quantifying this limitation mathematically, and {\em effectively providing lower bounds on what might be achievable with communication compression}. Motivated by these developments, we call for the search for the optimal compression operator. In an attempt to take a first step in this direction, we construct a new unbiased compression method inspired by the Kashin representation of vectors, which we call {\em Kashin compression (KC)}. In contrast to all previously proposed compression mechanisms, we prove that KC enjoys a {\em dimension independent} variance bound with an explicit formula even in the regime when only a few bits need to be communicate per each vector entry. We show how KC can be provably and efficiently combined with several existing optimization algorithms, in all cases leading to communication complexity improvements on previous state of the art. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08958">PDF</a><h2>2020-03-05</h2>
<h3>No. 1	Fast Implementation of Morphological Filtering Using ARM NEON Extension</h3><h4>Elena Limonova, Arseny Terekhin, Dmitry Nikolaev, Vladimir Arlazarov</h4> Abstract: In this paper we consider speedup potential of morphological image filtering on ARM processors. Morphological operations are widely used in image analysis and recognition and their speedup in some cases can significantly reduce overall execution time of recognition. More specifically, we propose fast implementation of erosion and dilation using ARM SIMD extension NEON. These operations with the rectangular structuring element are separable. They were implemented using the advantages of separability as sequential horizontal and vertical passes. Each pass was implemented using van Herk/Gil-Werman algorithm for large windows and low-constant linear complexity algorithm for small windows. Final implementation was improved with SIMD and used a combination of these methods. We also considered fast transpose implementation of 8x8 and 16x16 matrices using ARM NEON to get additional computational gain for morphological operations. Experiments showed 3 times efficiency increase for final implementation of erosion and dilation compared to van Herk/Gil-Werman algorithm without SIMD, 5.7 times speedup for 8x8 matrix transpose and 12 times speedup for 16x16 matrix transpose compared to transpose without SIMD. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09474">PDF</a>
<h3>No. 2	Faasm: Lightweight Isolation for Efficient Stateful Serverless Computing</h3><h4>Simon Shillaker, Peter Pietzuch</h4> Abstract: Serverless computing is an excellent fit for big data processing because it can scale quickly and cheaply to thousands of parallel functions. Existing serverless platforms, however, isolate functions in ephemeral, stateless containers. This means that functions cannot efficiently share memory, forcing users to serialise data repeatedly when composing functions. We observe that container-based isolation is ill-suited for serverless big data processing. Instead it requires a lightweight isolation approach that allows for efficient state sharing. We introduce Faaslets, a new isolation abstraction for serverless big data computing. Faaslets isolate the memory of executed functions using software-fault isolation (SFI), as provided by WebAssembly, while allowing memory regions to be shared between functions in the same address space. Faaslets can thus avoid expensive data movement when functions are co-located on the same machine. Our runtime for Faaslets, Faasm, isolates other resources, e.g. CPU and network, using standard Linux cgroups, and provides a low-level POSIX host interface for networking, file system access and dynamic loading. To reduce initialisation times, Faasm restores Faaslets from already-initialised snapshots. We compare Faasm to a standard container-based platform and show that, when training a machine learning model, it achieves a 2x speed-up with 10x less memory; for serving machine learning inference, Faasm doubles the throughput and reduces tail latency by 90%. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09344">PDF</a>
<h3>No. 3	Methods and Experiences for Developing Abstractions for Data-intensive,  Scientific Applications</h3><h4>Andre Luckow, Shantenu Jha</h4> Abstract: Developing software for scientific applications that require the integration of diverse types of computing, instruments and data present challenges that are distinct from commercial software, due to scale, heterogeneity, and the need to integrate various programming and computational models with evolving and heterogeneous infrastructure. Pervasive and effective abstractions are thus critical. The process of developing abstractions for scientific applications and infrastructures is not well understood. While theory-based approaches are suited for well-defined, closed environments, they have severe limitations for designing abstractions for complex, real-world systems. The design science research (DSR) method provides the basis for designing effective systems that can handle real-world complexities. DSR consists of two complementary phases: design and evaluation. This paper applies the DSR method to the development of abstractions for scientific applications. Specifically, we address the critical problem of distributed resource management on heterogeneous infrastructure, a challenge that currently limits many scientific applications. We use the pilot-abstraction, a widely used resource management abstraction for high-performance, high throughput, big data, and streaming applications, as a case study. We evaluate the activities of the process and extensively evaluate the artifacts using different methods, including conceptual modeling, performance characterizations, and modeling. We demonstrate the applicability of the DSR method for holistically handling the complexity of parallel and distributed computing environments addressing important application, system, and infrastructure challenges of scientific applications. Finally, we capture our experiences and formulate different lessons learned. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09009">PDF</a>
<h3>No. 4	Distributed Mean Estimation with Optimal Error Bounds</h3><h4>Dan Alistarh, Saleh Ashkboos, Peter Davies</h4> Abstract: Motivated by applications to distributed optimization and machine learning, we consider the distributed mean estimation problem, in which $n$ nodes are each assigned a multi-dimensional input vector, and must cooperate to estimate the mean of the input vectors, while minimizing communication. In this paper, we provide the first tight bounds for this problem, in terms of the trade-off between the amount of communication between nodes and the variance of the node estimates relative to the true value of the mean. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09268">PDF</a>
<h3>No. 5	A polynomial lower bound on adaptive complexity of submodular  maximization</h3><h4>Wenzheng Li, Paul Liu, Jan Vondrak</h4> Abstract: In large-data applications, it is desirable to design algorithms with a high degree of parallelization. In the context of submodular optimization, adaptive complexity has become a widely-used measure of an algorithm's "sequentiality". Algorithms in the adaptive model proceed in rounds, and can issue polynomially many queries to a function $f$ in each round. The queries in each round must be independent, produced by a computation that depends only on query results obtained in previous rounds. In this work, we examine two fundamental variants of submodular maximization in the adaptive complexity model: cardinality-constrained monotone maximization, and unconstrained non-monotone maximization. Our main result is that an $r$-round algorithm for cardinality-constrained monotone maximization cannot achieve a factor better than $1 - 1/e - \Omega(\min \{ \frac{1}{r}, \frac{\log^2 n}{r^3} \})$, for any $r < n^c$ (where $c>0$ is some constant). This is the first result showing that the number of rounds must blow up polynomially large as we approach the optimal factor of $1-1/e$. For the unconstrained non-monotone maximization problem, we show a positive result: For every instance, and every $\delta>0$, either we obtain a $(1/2-\delta)$-approximation in $1$ round, or a $(1/2+\Omega(\delta^2))$-approximation in $O(1/\delta^2)$ rounds. In particular (in contrast to the cardinality-constrained case), there cannot be an instance where (i) it is impossible to achieve a factor better than $1/2$ regardless of the number of rounds, and (ii) it takes $r$ rounds to achieve a factor of $1/2-O(1/r)$. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09130">PDF</a>
<h3>No. 6	Asynchronous parallel adaptive stochastic gradient methods</h3><h4>Yangyang Xu, Colin Sutcher-Shepard, Yibo Xu, Jie Chen</h4> Abstract: Stochastic gradient methods (SGMs) are the predominant approaches to train deep learning models. The adaptive versions (e.g., Adam and AMSGrad) have been extensively used in practice, partly because they achieve faster convergence than the non-adaptive versions while incurring little overhead. On the other hand, asynchronous (async) parallel computing has exhibited much better speed-up over its synchronous (sync) counterpart. However, async-parallel implementation has only been demonstrated to the non-adaptive SGMs. The difficulty for adaptive SGMs originates from the second moment term that makes the convergence analysis challenging with async updates. In this paper, we propose an async-parallel adaptive SGM based on AMSGrad. We show that the proposed method inherits the convergence guarantee of AMSGrad for both convex and non-convex problems, if the staleness (also called delay) caused by asynchrony is bounded. Our convergence rate results indicate a nearly linear parallelization speed-up if $\tau=o(K^{\frac{1}{4}})$, where $\tau$ is the staleness and $K$ is the number of iterations. The proposed method is tested on both convex and non-convex machine learning problems, and the numerical results demonstrate its clear advantages over the sync counterpart. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09095">PDF</a>
<h3>No. 7	Uncertainty Principle for Communication Compression in Distributed and  Federated Learning and the Search for an Optimal Compressor</h3><h4>Mher Safaryan, Egor Shulgin, Peter Richt√°rik</h4> Abstract: In order to mitigate the high communication cost in distributed and federated learning, various vector compression schemes, such as quantization, sparsification and dithering, have become very popular. In designing a compression method, one aims to communicate as few bits as possible, which minimizes the cost per communication round, while at the same time attempting to impart as little distortion (variance) to the communicated messages as possible, which minimizes the adverse effect of the compression on the overall number of communication rounds. However, intuitively, these two goals are fundamentally in conflict: the more compression we allow, the more distorted the messages become. We formalize this intuition and prove an {\em uncertainty principle} for randomized compression operators, thus quantifying this limitation mathematically, and {\em effectively providing lower bounds on what might be achievable with communication compression}. Motivated by these developments, we call for the search for the optimal compression operator. In an attempt to take a first step in this direction, we construct a new unbiased compression method inspired by the Kashin representation of vectors, which we call {\em Kashin compression (KC)}. In contrast to all previously proposed compression mechanisms, we prove that KC enjoys a {\em dimension independent} variance bound with an explicit formula even in the regime when only a few bits need to be communicate per each vector entry. We show how KC can be provably and efficiently combined with several existing optimization algorithms, in all cases leading to communication complexity improvements on previous state of the art. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08958">PDF</a><h2>2020-03-04</h2>
<h3>No. 1	Fast Implementation of Morphological Filtering Using ARM NEON Extension</h3><h4>Elena Limonova, Arseny Terekhin, Dmitry Nikolaev, Vladimir Arlazarov</h4> Abstract: In this paper we consider speedup potential of morphological image filtering on ARM processors. Morphological operations are widely used in image analysis and recognition and their speedup in some cases can significantly reduce overall execution time of recognition. More specifically, we propose fast implementation of erosion and dilation using ARM SIMD extension NEON. These operations with the rectangular structuring element are separable. They were implemented using the advantages of separability as sequential horizontal and vertical passes. Each pass was implemented using van Herk/Gil-Werman algorithm for large windows and low-constant linear complexity algorithm for small windows. Final implementation was improved with SIMD and used a combination of these methods. We also considered fast transpose implementation of 8x8 and 16x16 matrices using ARM NEON to get additional computational gain for morphological operations. Experiments showed 3 times efficiency increase for final implementation of erosion and dilation compared to van Herk/Gil-Werman algorithm without SIMD, 5.7 times speedup for 8x8 matrix transpose and 12 times speedup for 16x16 matrix transpose compared to transpose without SIMD. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09474">PDF</a>
<h3>No. 2	Faasm: Lightweight Isolation for Efficient Stateful Serverless Computing</h3><h4>Simon Shillaker, Peter Pietzuch</h4> Abstract: Serverless computing is an excellent fit for big data processing because it can scale quickly and cheaply to thousands of parallel functions. Existing serverless platforms, however, isolate functions in ephemeral, stateless containers. This means that functions cannot efficiently share memory, forcing users to serialise data repeatedly when composing functions. We observe that container-based isolation is ill-suited for serverless big data processing. Instead it requires a lightweight isolation approach that allows for efficient state sharing. We introduce Faaslets, a new isolation abstraction for serverless big data computing. Faaslets isolate the memory of executed functions using software-fault isolation (SFI), as provided by WebAssembly, while allowing memory regions to be shared between functions in the same address space. Faaslets can thus avoid expensive data movement when functions are co-located on the same machine. Our runtime for Faaslets, Faasm, isolates other resources, e.g. CPU and network, using standard Linux cgroups, and provides a low-level POSIX host interface for networking, file system access and dynamic loading. To reduce initialisation times, Faasm restores Faaslets from already-initialised snapshots. We compare Faasm to a standard container-based platform and show that, when training a machine learning model, it achieves a 2x speed-up with 10x less memory; for serving machine learning inference, Faasm doubles the throughput and reduces tail latency by 90%. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09344">PDF</a>
<h3>No. 3	Methods and Experiences for Developing Abstractions for Data-intensive,  Scientific Applications</h3><h4>Andre Luckow, Shantenu Jha</h4> Abstract: Developing software for scientific applications that require the integration of diverse types of computing, instruments and data present challenges that are distinct from commercial software, due to scale, heterogeneity, and the need to integrate various programming and computational models with evolving and heterogeneous infrastructure. Pervasive and effective abstractions are thus critical. The process of developing abstractions for scientific applications and infrastructures is not well understood. While theory-based approaches are suited for well-defined, closed environments, they have severe limitations for designing abstractions for complex, real-world systems. The design science research (DSR) method provides the basis for designing effective systems that can handle real-world complexities. DSR consists of two complementary phases: design and evaluation. This paper applies the DSR method to the development of abstractions for scientific applications. Specifically, we address the critical problem of distributed resource management on heterogeneous infrastructure, a challenge that currently limits many scientific applications. We use the pilot-abstraction, a widely used resource management abstraction for high-performance, high throughput, big data, and streaming applications, as a case study. We evaluate the activities of the process and extensively evaluate the artifacts using different methods, including conceptual modeling, performance characterizations, and modeling. We demonstrate the applicability of the DSR method for holistically handling the complexity of parallel and distributed computing environments addressing important application, system, and infrastructure challenges of scientific applications. Finally, we capture our experiences and formulate different lessons learned. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09009">PDF</a>
<h3>No. 4	Distributed Mean Estimation with Optimal Error Bounds</h3><h4>Dan Alistarh, Saleh Ashkboos, Peter Davies</h4> Abstract: Motivated by applications to distributed optimization and machine learning, we consider the distributed mean estimation problem, in which $n$ nodes are each assigned a multi-dimensional input vector, and must cooperate to estimate the mean of the input vectors, while minimizing communication. In this paper, we provide the first tight bounds for this problem, in terms of the trade-off between the amount of communication between nodes and the variance of the node estimates relative to the true value of the mean. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09268">PDF</a>
<h3>No. 5	A polynomial lower bound on adaptive complexity of submodular  maximization</h3><h4>Wenzheng Li, Paul Liu, Jan Vondrak</h4> Abstract: In large-data applications, it is desirable to design algorithms with a high degree of parallelization. In the context of submodular optimization, adaptive complexity has become a widely-used measure of an algorithm's "sequentiality". Algorithms in the adaptive model proceed in rounds, and can issue polynomially many queries to a function $f$ in each round. The queries in each round must be independent, produced by a computation that depends only on query results obtained in previous rounds. In this work, we examine two fundamental variants of submodular maximization in the adaptive complexity model: cardinality-constrained monotone maximization, and unconstrained non-monotone maximization. Our main result is that an $r$-round algorithm for cardinality-constrained monotone maximization cannot achieve a factor better than $1 - 1/e - \Omega(\min \{ \frac{1}{r}, \frac{\log^2 n}{r^3} \})$, for any $r < n^c$ (where $c>0$ is some constant). This is the first result showing that the number of rounds must blow up polynomially large as we approach the optimal factor of $1-1/e$. For the unconstrained non-monotone maximization problem, we show a positive result: For every instance, and every $\delta>0$, either we obtain a $(1/2-\delta)$-approximation in $1$ round, or a $(1/2+\Omega(\delta^2))$-approximation in $O(1/\delta^2)$ rounds. In particular (in contrast to the cardinality-constrained case), there cannot be an instance where (i) it is impossible to achieve a factor better than $1/2$ regardless of the number of rounds, and (ii) it takes $r$ rounds to achieve a factor of $1/2-O(1/r)$. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09130">PDF</a>
<h3>No. 6	Asynchronous parallel adaptive stochastic gradient methods</h3><h4>Yangyang Xu, Colin Sutcher-Shepard, Yibo Xu, Jie Chen</h4> Abstract: Stochastic gradient methods (SGMs) are the predominant approaches to train deep learning models. The adaptive versions (e.g., Adam and AMSGrad) have been extensively used in practice, partly because they achieve faster convergence than the non-adaptive versions while incurring little overhead. On the other hand, asynchronous (async) parallel computing has exhibited much better speed-up over its synchronous (sync) counterpart. However, async-parallel implementation has only been demonstrated to the non-adaptive SGMs. The difficulty for adaptive SGMs originates from the second moment term that makes the convergence analysis challenging with async updates. In this paper, we propose an async-parallel adaptive SGM based on AMSGrad. We show that the proposed method inherits the convergence guarantee of AMSGrad for both convex and non-convex problems, if the staleness (also called delay) caused by asynchrony is bounded. Our convergence rate results indicate a nearly linear parallelization speed-up if $\tau=o(K^{\frac{1}{4}})$, where $\tau$ is the staleness and $K$ is the number of iterations. The proposed method is tested on both convex and non-convex machine learning problems, and the numerical results demonstrate its clear advantages over the sync counterpart. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09095">PDF</a>
<h3>No. 7	Uncertainty Principle for Communication Compression in Distributed and  Federated Learning and the Search for an Optimal Compressor</h3><h4>Mher Safaryan, Egor Shulgin, Peter Richt√°rik</h4> Abstract: In order to mitigate the high communication cost in distributed and federated learning, various vector compression schemes, such as quantization, sparsification and dithering, have become very popular. In designing a compression method, one aims to communicate as few bits as possible, which minimizes the cost per communication round, while at the same time attempting to impart as little distortion (variance) to the communicated messages as possible, which minimizes the adverse effect of the compression on the overall number of communication rounds. However, intuitively, these two goals are fundamentally in conflict: the more compression we allow, the more distorted the messages become. We formalize this intuition and prove an {\em uncertainty principle} for randomized compression operators, thus quantifying this limitation mathematically, and {\em effectively providing lower bounds on what might be achievable with communication compression}. Motivated by these developments, we call for the search for the optimal compression operator. In an attempt to take a first step in this direction, we construct a new unbiased compression method inspired by the Kashin representation of vectors, which we call {\em Kashin compression (KC)}. In contrast to all previously proposed compression mechanisms, we prove that KC enjoys a {\em dimension independent} variance bound with an explicit formula even in the regime when only a few bits need to be communicate per each vector entry. We show how KC can be provably and efficiently combined with several existing optimization algorithms, in all cases leading to communication complexity improvements on previous state of the art. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08958">PDF</a><h2>2020-03-03</h2>
<h3>No. 1	Fast Implementation of Morphological Filtering Using ARM NEON Extension</h3><h4>Elena Limonova, Arseny Terekhin, Dmitry Nikolaev, Vladimir Arlazarov</h4> Abstract: In this paper we consider speedup potential of morphological image filtering on ARM processors. Morphological operations are widely used in image analysis and recognition and their speedup in some cases can significantly reduce overall execution time of recognition. More specifically, we propose fast implementation of erosion and dilation using ARM SIMD extension NEON. These operations with the rectangular structuring element are separable. They were implemented using the advantages of separability as sequential horizontal and vertical passes. Each pass was implemented using van Herk/Gil-Werman algorithm for large windows and low-constant linear complexity algorithm for small windows. Final implementation was improved with SIMD and used a combination of these methods. We also considered fast transpose implementation of 8x8 and 16x16 matrices using ARM NEON to get additional computational gain for morphological operations. Experiments showed 3 times efficiency increase for final implementation of erosion and dilation compared to van Herk/Gil-Werman algorithm without SIMD, 5.7 times speedup for 8x8 matrix transpose and 12 times speedup for 16x16 matrix transpose compared to transpose without SIMD. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09474">PDF</a>
<h3>No. 2	Faasm: Lightweight Isolation for Efficient Stateful Serverless Computing</h3><h4>Simon Shillaker, Peter Pietzuch</h4> Abstract: Serverless computing is an excellent fit for big data processing because it can scale quickly and cheaply to thousands of parallel functions. Existing serverless platforms, however, isolate functions in ephemeral, stateless containers. This means that functions cannot efficiently share memory, forcing users to serialise data repeatedly when composing functions. We observe that container-based isolation is ill-suited for serverless big data processing. Instead it requires a lightweight isolation approach that allows for efficient state sharing. We introduce Faaslets, a new isolation abstraction for serverless big data computing. Faaslets isolate the memory of executed functions using software-fault isolation (SFI), as provided by WebAssembly, while allowing memory regions to be shared between functions in the same address space. Faaslets can thus avoid expensive data movement when functions are co-located on the same machine. Our runtime for Faaslets, Faasm, isolates other resources, e.g. CPU and network, using standard Linux cgroups, and provides a low-level POSIX host interface for networking, file system access and dynamic loading. To reduce initialisation times, Faasm restores Faaslets from already-initialised snapshots. We compare Faasm to a standard container-based platform and show that, when training a machine learning model, it achieves a 2x speed-up with 10x less memory; for serving machine learning inference, Faasm doubles the throughput and reduces tail latency by 90%. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09344">PDF</a>
<h3>No. 3	Methods and Experiences for Developing Abstractions for Data-intensive,  Scientific Applications</h3><h4>Andre Luckow, Shantenu Jha</h4> Abstract: Developing software for scientific applications that require the integration of diverse types of computing, instruments and data present challenges that are distinct from commercial software, due to scale, heterogeneity, and the need to integrate various programming and computational models with evolving and heterogeneous infrastructure. Pervasive and effective abstractions are thus critical. The process of developing abstractions for scientific applications and infrastructures is not well understood. While theory-based approaches are suited for well-defined, closed environments, they have severe limitations for designing abstractions for complex, real-world systems. The design science research (DSR) method provides the basis for designing effective systems that can handle real-world complexities. DSR consists of two complementary phases: design and evaluation. This paper applies the DSR method to the development of abstractions for scientific applications. Specifically, we address the critical problem of distributed resource management on heterogeneous infrastructure, a challenge that currently limits many scientific applications. We use the pilot-abstraction, a widely used resource management abstraction for high-performance, high throughput, big data, and streaming applications, as a case study. We evaluate the activities of the process and extensively evaluate the artifacts using different methods, including conceptual modeling, performance characterizations, and modeling. We demonstrate the applicability of the DSR method for holistically handling the complexity of parallel and distributed computing environments addressing important application, system, and infrastructure challenges of scientific applications. Finally, we capture our experiences and formulate different lessons learned. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09009">PDF</a>
<h3>No. 4	Distributed Mean Estimation with Optimal Error Bounds</h3><h4>Dan Alistarh, Saleh Ashkboos, Peter Davies</h4> Abstract: Motivated by applications to distributed optimization and machine learning, we consider the distributed mean estimation problem, in which $n$ nodes are each assigned a multi-dimensional input vector, and must cooperate to estimate the mean of the input vectors, while minimizing communication. In this paper, we provide the first tight bounds for this problem, in terms of the trade-off between the amount of communication between nodes and the variance of the node estimates relative to the true value of the mean. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09268">PDF</a>
<h3>No. 5	A polynomial lower bound on adaptive complexity of submodular  maximization</h3><h4>Wenzheng Li, Paul Liu, Jan Vondrak</h4> Abstract: In large-data applications, it is desirable to design algorithms with a high degree of parallelization. In the context of submodular optimization, adaptive complexity has become a widely-used measure of an algorithm's "sequentiality". Algorithms in the adaptive model proceed in rounds, and can issue polynomially many queries to a function $f$ in each round. The queries in each round must be independent, produced by a computation that depends only on query results obtained in previous rounds. In this work, we examine two fundamental variants of submodular maximization in the adaptive complexity model: cardinality-constrained monotone maximization, and unconstrained non-monotone maximization. Our main result is that an $r$-round algorithm for cardinality-constrained monotone maximization cannot achieve a factor better than $1 - 1/e - \Omega(\min \{ \frac{1}{r}, \frac{\log^2 n}{r^3} \})$, for any $r < n^c$ (where $c>0$ is some constant). This is the first result showing that the number of rounds must blow up polynomially large as we approach the optimal factor of $1-1/e$. For the unconstrained non-monotone maximization problem, we show a positive result: For every instance, and every $\delta>0$, either we obtain a $(1/2-\delta)$-approximation in $1$ round, or a $(1/2+\Omega(\delta^2))$-approximation in $O(1/\delta^2)$ rounds. In particular (in contrast to the cardinality-constrained case), there cannot be an instance where (i) it is impossible to achieve a factor better than $1/2$ regardless of the number of rounds, and (ii) it takes $r$ rounds to achieve a factor of $1/2-O(1/r)$. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09130">PDF</a>
<h3>No. 6	Asynchronous parallel adaptive stochastic gradient methods</h3><h4>Yangyang Xu, Colin Sutcher-Shepard, Yibo Xu, Jie Chen</h4> Abstract: Stochastic gradient methods (SGMs) are the predominant approaches to train deep learning models. The adaptive versions (e.g., Adam and AMSGrad) have been extensively used in practice, partly because they achieve faster convergence than the non-adaptive versions while incurring little overhead. On the other hand, asynchronous (async) parallel computing has exhibited much better speed-up over its synchronous (sync) counterpart. However, async-parallel implementation has only been demonstrated to the non-adaptive SGMs. The difficulty for adaptive SGMs originates from the second moment term that makes the convergence analysis challenging with async updates. In this paper, we propose an async-parallel adaptive SGM based on AMSGrad. We show that the proposed method inherits the convergence guarantee of AMSGrad for both convex and non-convex problems, if the staleness (also called delay) caused by asynchrony is bounded. Our convergence rate results indicate a nearly linear parallelization speed-up if $\tau=o(K^{\frac{1}{4}})$, where $\tau$ is the staleness and $K$ is the number of iterations. The proposed method is tested on both convex and non-convex machine learning problems, and the numerical results demonstrate its clear advantages over the sync counterpart. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09095">PDF</a>
<h3>No. 7	Uncertainty Principle for Communication Compression in Distributed and  Federated Learning and the Search for an Optimal Compressor</h3><h4>Mher Safaryan, Egor Shulgin, Peter Richt√°rik</h4> Abstract: In order to mitigate the high communication cost in distributed and federated learning, various vector compression schemes, such as quantization, sparsification and dithering, have become very popular. In designing a compression method, one aims to communicate as few bits as possible, which minimizes the cost per communication round, while at the same time attempting to impart as little distortion (variance) to the communicated messages as possible, which minimizes the adverse effect of the compression on the overall number of communication rounds. However, intuitively, these two goals are fundamentally in conflict: the more compression we allow, the more distorted the messages become. We formalize this intuition and prove an {\em uncertainty principle} for randomized compression operators, thus quantifying this limitation mathematically, and {\em effectively providing lower bounds on what might be achievable with communication compression}. Motivated by these developments, we call for the search for the optimal compression operator. In an attempt to take a first step in this direction, we construct a new unbiased compression method inspired by the Kashin representation of vectors, which we call {\em Kashin compression (KC)}. In contrast to all previously proposed compression mechanisms, we prove that KC enjoys a {\em dimension independent} variance bound with an explicit formula even in the regime when only a few bits need to be communicate per each vector entry. We show how KC can be provably and efficiently combined with several existing optimization algorithms, in all cases leading to communication complexity improvements on previous state of the art. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08958">PDF</a><h2>2020-03-02</h2>
<h3>No. 1	Fast Implementation of Morphological Filtering Using ARM NEON Extension</h3><h4>Elena Limonova, Arseny Terekhin, Dmitry Nikolaev, Vladimir Arlazarov</h4> Abstract: In this paper we consider speedup potential of morphological image filtering on ARM processors. Morphological operations are widely used in image analysis and recognition and their speedup in some cases can significantly reduce overall execution time of recognition. More specifically, we propose fast implementation of erosion and dilation using ARM SIMD extension NEON. These operations with the rectangular structuring element are separable. They were implemented using the advantages of separability as sequential horizontal and vertical passes. Each pass was implemented using van Herk/Gil-Werman algorithm for large windows and low-constant linear complexity algorithm for small windows. Final implementation was improved with SIMD and used a combination of these methods. We also considered fast transpose implementation of 8x8 and 16x16 matrices using ARM NEON to get additional computational gain for morphological operations. Experiments showed 3 times efficiency increase for final implementation of erosion and dilation compared to van Herk/Gil-Werman algorithm without SIMD, 5.7 times speedup for 8x8 matrix transpose and 12 times speedup for 16x16 matrix transpose compared to transpose without SIMD. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09474">PDF</a>
<h3>No. 2	Faasm: Lightweight Isolation for Efficient Stateful Serverless Computing</h3><h4>Simon Shillaker, Peter Pietzuch</h4> Abstract: Serverless computing is an excellent fit for big data processing because it can scale quickly and cheaply to thousands of parallel functions. Existing serverless platforms, however, isolate functions in ephemeral, stateless containers. This means that functions cannot efficiently share memory, forcing users to serialise data repeatedly when composing functions. We observe that container-based isolation is ill-suited for serverless big data processing. Instead it requires a lightweight isolation approach that allows for efficient state sharing. We introduce Faaslets, a new isolation abstraction for serverless big data computing. Faaslets isolate the memory of executed functions using software-fault isolation (SFI), as provided by WebAssembly, while allowing memory regions to be shared between functions in the same address space. Faaslets can thus avoid expensive data movement when functions are co-located on the same machine. Our runtime for Faaslets, Faasm, isolates other resources, e.g. CPU and network, using standard Linux cgroups, and provides a low-level POSIX host interface for networking, file system access and dynamic loading. To reduce initialisation times, Faasm restores Faaslets from already-initialised snapshots. We compare Faasm to a standard container-based platform and show that, when training a machine learning model, it achieves a 2x speed-up with 10x less memory; for serving machine learning inference, Faasm doubles the throughput and reduces tail latency by 90%. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09344">PDF</a>
<h3>No. 3	Methods and Experiences for Developing Abstractions for Data-intensive,  Scientific Applications</h3><h4>Andre Luckow, Shantenu Jha</h4> Abstract: Developing software for scientific applications that require the integration of diverse types of computing, instruments and data present challenges that are distinct from commercial software, due to scale, heterogeneity, and the need to integrate various programming and computational models with evolving and heterogeneous infrastructure. Pervasive and effective abstractions are thus critical. The process of developing abstractions for scientific applications and infrastructures is not well understood. While theory-based approaches are suited for well-defined, closed environments, they have severe limitations for designing abstractions for complex, real-world systems. The design science research (DSR) method provides the basis for designing effective systems that can handle real-world complexities. DSR consists of two complementary phases: design and evaluation. This paper applies the DSR method to the development of abstractions for scientific applications. Specifically, we address the critical problem of distributed resource management on heterogeneous infrastructure, a challenge that currently limits many scientific applications. We use the pilot-abstraction, a widely used resource management abstraction for high-performance, high throughput, big data, and streaming applications, as a case study. We evaluate the activities of the process and extensively evaluate the artifacts using different methods, including conceptual modeling, performance characterizations, and modeling. We demonstrate the applicability of the DSR method for holistically handling the complexity of parallel and distributed computing environments addressing important application, system, and infrastructure challenges of scientific applications. Finally, we capture our experiences and formulate different lessons learned. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09009">PDF</a>
<h3>No. 4	Distributed Mean Estimation with Optimal Error Bounds</h3><h4>Dan Alistarh, Saleh Ashkboos, Peter Davies</h4> Abstract: Motivated by applications to distributed optimization and machine learning, we consider the distributed mean estimation problem, in which $n$ nodes are each assigned a multi-dimensional input vector, and must cooperate to estimate the mean of the input vectors, while minimizing communication. In this paper, we provide the first tight bounds for this problem, in terms of the trade-off between the amount of communication between nodes and the variance of the node estimates relative to the true value of the mean. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09268">PDF</a>
<h3>No. 5	A polynomial lower bound on adaptive complexity of submodular  maximization</h3><h4>Wenzheng Li, Paul Liu, Jan Vondrak</h4> Abstract: In large-data applications, it is desirable to design algorithms with a high degree of parallelization. In the context of submodular optimization, adaptive complexity has become a widely-used measure of an algorithm's "sequentiality". Algorithms in the adaptive model proceed in rounds, and can issue polynomially many queries to a function $f$ in each round. The queries in each round must be independent, produced by a computation that depends only on query results obtained in previous rounds. In this work, we examine two fundamental variants of submodular maximization in the adaptive complexity model: cardinality-constrained monotone maximization, and unconstrained non-monotone maximization. Our main result is that an $r$-round algorithm for cardinality-constrained monotone maximization cannot achieve a factor better than $1 - 1/e - \Omega(\min \{ \frac{1}{r}, \frac{\log^2 n}{r^3} \})$, for any $r < n^c$ (where $c>0$ is some constant). This is the first result showing that the number of rounds must blow up polynomially large as we approach the optimal factor of $1-1/e$. For the unconstrained non-monotone maximization problem, we show a positive result: For every instance, and every $\delta>0$, either we obtain a $(1/2-\delta)$-approximation in $1$ round, or a $(1/2+\Omega(\delta^2))$-approximation in $O(1/\delta^2)$ rounds. In particular (in contrast to the cardinality-constrained case), there cannot be an instance where (i) it is impossible to achieve a factor better than $1/2$ regardless of the number of rounds, and (ii) it takes $r$ rounds to achieve a factor of $1/2-O(1/r)$. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09130">PDF</a>
<h3>No. 6	Asynchronous parallel adaptive stochastic gradient methods</h3><h4>Yangyang Xu, Colin Sutcher-Shepard, Yibo Xu, Jie Chen</h4> Abstract: Stochastic gradient methods (SGMs) are the predominant approaches to train deep learning models. The adaptive versions (e.g., Adam and AMSGrad) have been extensively used in practice, partly because they achieve faster convergence than the non-adaptive versions while incurring little overhead. On the other hand, asynchronous (async) parallel computing has exhibited much better speed-up over its synchronous (sync) counterpart. However, async-parallel implementation has only been demonstrated to the non-adaptive SGMs. The difficulty for adaptive SGMs originates from the second moment term that makes the convergence analysis challenging with async updates. In this paper, we propose an async-parallel adaptive SGM based on AMSGrad. We show that the proposed method inherits the convergence guarantee of AMSGrad for both convex and non-convex problems, if the staleness (also called delay) caused by asynchrony is bounded. Our convergence rate results indicate a nearly linear parallelization speed-up if $\tau=o(K^{\frac{1}{4}})$, where $\tau$ is the staleness and $K$ is the number of iterations. The proposed method is tested on both convex and non-convex machine learning problems, and the numerical results demonstrate its clear advantages over the sync counterpart. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09095">PDF</a>
<h3>No. 7	Uncertainty Principle for Communication Compression in Distributed and  Federated Learning and the Search for an Optimal Compressor</h3><h4>Mher Safaryan, Egor Shulgin, Peter Richt√°rik</h4> Abstract: In order to mitigate the high communication cost in distributed and federated learning, various vector compression schemes, such as quantization, sparsification and dithering, have become very popular. In designing a compression method, one aims to communicate as few bits as possible, which minimizes the cost per communication round, while at the same time attempting to impart as little distortion (variance) to the communicated messages as possible, which minimizes the adverse effect of the compression on the overall number of communication rounds. However, intuitively, these two goals are fundamentally in conflict: the more compression we allow, the more distorted the messages become. We formalize this intuition and prove an {\em uncertainty principle} for randomized compression operators, thus quantifying this limitation mathematically, and {\em effectively providing lower bounds on what might be achievable with communication compression}. Motivated by these developments, we call for the search for the optimal compression operator. In an attempt to take a first step in this direction, we construct a new unbiased compression method inspired by the Kashin representation of vectors, which we call {\em Kashin compression (KC)}. In contrast to all previously proposed compression mechanisms, we prove that KC enjoys a {\em dimension independent} variance bound with an explicit formula even in the regime when only a few bits need to be communicate per each vector entry. We show how KC can be provably and efficiently combined with several existing optimization algorithms, in all cases leading to communication complexity improvements on previous state of the art. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08958">PDF</a><h2>2020-03-01</h2>
<h3>No. 1	Fast Implementation of Morphological Filtering Using ARM NEON Extension</h3><h4>Elena Limonova, Arseny Terekhin, Dmitry Nikolaev, Vladimir Arlazarov</h4> Abstract: In this paper we consider speedup potential of morphological image filtering on ARM processors. Morphological operations are widely used in image analysis and recognition and their speedup in some cases can significantly reduce overall execution time of recognition. More specifically, we propose fast implementation of erosion and dilation using ARM SIMD extension NEON. These operations with the rectangular structuring element are separable. They were implemented using the advantages of separability as sequential horizontal and vertical passes. Each pass was implemented using van Herk/Gil-Werman algorithm for large windows and low-constant linear complexity algorithm for small windows. Final implementation was improved with SIMD and used a combination of these methods. We also considered fast transpose implementation of 8x8 and 16x16 matrices using ARM NEON to get additional computational gain for morphological operations. Experiments showed 3 times efficiency increase for final implementation of erosion and dilation compared to van Herk/Gil-Werman algorithm without SIMD, 5.7 times speedup for 8x8 matrix transpose and 12 times speedup for 16x16 matrix transpose compared to transpose without SIMD. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09474">PDF</a>
<h3>No. 2	Faasm: Lightweight Isolation for Efficient Stateful Serverless Computing</h3><h4>Simon Shillaker, Peter Pietzuch</h4> Abstract: Serverless computing is an excellent fit for big data processing because it can scale quickly and cheaply to thousands of parallel functions. Existing serverless platforms, however, isolate functions in ephemeral, stateless containers. This means that functions cannot efficiently share memory, forcing users to serialise data repeatedly when composing functions. We observe that container-based isolation is ill-suited for serverless big data processing. Instead it requires a lightweight isolation approach that allows for efficient state sharing. We introduce Faaslets, a new isolation abstraction for serverless big data computing. Faaslets isolate the memory of executed functions using software-fault isolation (SFI), as provided by WebAssembly, while allowing memory regions to be shared between functions in the same address space. Faaslets can thus avoid expensive data movement when functions are co-located on the same machine. Our runtime for Faaslets, Faasm, isolates other resources, e.g. CPU and network, using standard Linux cgroups, and provides a low-level POSIX host interface for networking, file system access and dynamic loading. To reduce initialisation times, Faasm restores Faaslets from already-initialised snapshots. We compare Faasm to a standard container-based platform and show that, when training a machine learning model, it achieves a 2x speed-up with 10x less memory; for serving machine learning inference, Faasm doubles the throughput and reduces tail latency by 90%. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09344">PDF</a>
<h3>No. 3	Methods and Experiences for Developing Abstractions for Data-intensive,  Scientific Applications</h3><h4>Andre Luckow, Shantenu Jha</h4> Abstract: Developing software for scientific applications that require the integration of diverse types of computing, instruments and data present challenges that are distinct from commercial software, due to scale, heterogeneity, and the need to integrate various programming and computational models with evolving and heterogeneous infrastructure. Pervasive and effective abstractions are thus critical. The process of developing abstractions for scientific applications and infrastructures is not well understood. While theory-based approaches are suited for well-defined, closed environments, they have severe limitations for designing abstractions for complex, real-world systems. The design science research (DSR) method provides the basis for designing effective systems that can handle real-world complexities. DSR consists of two complementary phases: design and evaluation. This paper applies the DSR method to the development of abstractions for scientific applications. Specifically, we address the critical problem of distributed resource management on heterogeneous infrastructure, a challenge that currently limits many scientific applications. We use the pilot-abstraction, a widely used resource management abstraction for high-performance, high throughput, big data, and streaming applications, as a case study. We evaluate the activities of the process and extensively evaluate the artifacts using different methods, including conceptual modeling, performance characterizations, and modeling. We demonstrate the applicability of the DSR method for holistically handling the complexity of parallel and distributed computing environments addressing important application, system, and infrastructure challenges of scientific applications. Finally, we capture our experiences and formulate different lessons learned. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09009">PDF</a>
<h3>No. 4	Distributed Mean Estimation with Optimal Error Bounds</h3><h4>Dan Alistarh, Saleh Ashkboos, Peter Davies</h4> Abstract: Motivated by applications to distributed optimization and machine learning, we consider the distributed mean estimation problem, in which $n$ nodes are each assigned a multi-dimensional input vector, and must cooperate to estimate the mean of the input vectors, while minimizing communication. In this paper, we provide the first tight bounds for this problem, in terms of the trade-off between the amount of communication between nodes and the variance of the node estimates relative to the true value of the mean. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09268">PDF</a>
<h3>No. 5	A polynomial lower bound on adaptive complexity of submodular  maximization</h3><h4>Wenzheng Li, Paul Liu, Jan Vondrak</h4> Abstract: In large-data applications, it is desirable to design algorithms with a high degree of parallelization. In the context of submodular optimization, adaptive complexity has become a widely-used measure of an algorithm's "sequentiality". Algorithms in the adaptive model proceed in rounds, and can issue polynomially many queries to a function $f$ in each round. The queries in each round must be independent, produced by a computation that depends only on query results obtained in previous rounds. In this work, we examine two fundamental variants of submodular maximization in the adaptive complexity model: cardinality-constrained monotone maximization, and unconstrained non-monotone maximization. Our main result is that an $r$-round algorithm for cardinality-constrained monotone maximization cannot achieve a factor better than $1 - 1/e - \Omega(\min \{ \frac{1}{r}, \frac{\log^2 n}{r^3} \})$, for any $r < n^c$ (where $c>0$ is some constant). This is the first result showing that the number of rounds must blow up polynomially large as we approach the optimal factor of $1-1/e$. For the unconstrained non-monotone maximization problem, we show a positive result: For every instance, and every $\delta>0$, either we obtain a $(1/2-\delta)$-approximation in $1$ round, or a $(1/2+\Omega(\delta^2))$-approximation in $O(1/\delta^2)$ rounds. In particular (in contrast to the cardinality-constrained case), there cannot be an instance where (i) it is impossible to achieve a factor better than $1/2$ regardless of the number of rounds, and (ii) it takes $r$ rounds to achieve a factor of $1/2-O(1/r)$. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09130">PDF</a>
<h3>No. 6	Asynchronous parallel adaptive stochastic gradient methods</h3><h4>Yangyang Xu, Colin Sutcher-Shepard, Yibo Xu, Jie Chen</h4> Abstract: Stochastic gradient methods (SGMs) are the predominant approaches to train deep learning models. The adaptive versions (e.g., Adam and AMSGrad) have been extensively used in practice, partly because they achieve faster convergence than the non-adaptive versions while incurring little overhead. On the other hand, asynchronous (async) parallel computing has exhibited much better speed-up over its synchronous (sync) counterpart. However, async-parallel implementation has only been demonstrated to the non-adaptive SGMs. The difficulty for adaptive SGMs originates from the second moment term that makes the convergence analysis challenging with async updates. In this paper, we propose an async-parallel adaptive SGM based on AMSGrad. We show that the proposed method inherits the convergence guarantee of AMSGrad for both convex and non-convex problems, if the staleness (also called delay) caused by asynchrony is bounded. Our convergence rate results indicate a nearly linear parallelization speed-up if $\tau=o(K^{\frac{1}{4}})$, where $\tau$ is the staleness and $K$ is the number of iterations. The proposed method is tested on both convex and non-convex machine learning problems, and the numerical results demonstrate its clear advantages over the sync counterpart. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09095">PDF</a>
<h3>No. 7	Uncertainty Principle for Communication Compression in Distributed and  Federated Learning and the Search for an Optimal Compressor</h3><h4>Mher Safaryan, Egor Shulgin, Peter Richt√°rik</h4> Abstract: In order to mitigate the high communication cost in distributed and federated learning, various vector compression schemes, such as quantization, sparsification and dithering, have become very popular. In designing a compression method, one aims to communicate as few bits as possible, which minimizes the cost per communication round, while at the same time attempting to impart as little distortion (variance) to the communicated messages as possible, which minimizes the adverse effect of the compression on the overall number of communication rounds. However, intuitively, these two goals are fundamentally in conflict: the more compression we allow, the more distorted the messages become. We formalize this intuition and prove an {\em uncertainty principle} for randomized compression operators, thus quantifying this limitation mathematically, and {\em effectively providing lower bounds on what might be achievable with communication compression}. Motivated by these developments, we call for the search for the optimal compression operator. In an attempt to take a first step in this direction, we construct a new unbiased compression method inspired by the Kashin representation of vectors, which we call {\em Kashin compression (KC)}. In contrast to all previously proposed compression mechanisms, we prove that KC enjoys a {\em dimension independent} variance bound with an explicit formula even in the regime when only a few bits need to be communicate per each vector entry. We show how KC can be provably and efficiently combined with several existing optimization algorithms, in all cases leading to communication complexity improvements on previous state of the art. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08958">PDF</a><h2>2020-02-29</h2>
<h3>No. 1	Fast Implementation of Morphological Filtering Using ARM NEON Extension</h3><h4>Elena Limonova, Arseny Terekhin, Dmitry Nikolaev, Vladimir Arlazarov</h4> Abstract: In this paper we consider speedup potential of morphological image filtering on ARM processors. Morphological operations are widely used in image analysis and recognition and their speedup in some cases can significantly reduce overall execution time of recognition. More specifically, we propose fast implementation of erosion and dilation using ARM SIMD extension NEON. These operations with the rectangular structuring element are separable. They were implemented using the advantages of separability as sequential horizontal and vertical passes. Each pass was implemented using van Herk/Gil-Werman algorithm for large windows and low-constant linear complexity algorithm for small windows. Final implementation was improved with SIMD and used a combination of these methods. We also considered fast transpose implementation of 8x8 and 16x16 matrices using ARM NEON to get additional computational gain for morphological operations. Experiments showed 3 times efficiency increase for final implementation of erosion and dilation compared to van Herk/Gil-Werman algorithm without SIMD, 5.7 times speedup for 8x8 matrix transpose and 12 times speedup for 16x16 matrix transpose compared to transpose without SIMD. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09474">PDF</a>
<h3>No. 2	Faasm: Lightweight Isolation for Efficient Stateful Serverless Computing</h3><h4>Simon Shillaker, Peter Pietzuch</h4> Abstract: Serverless computing is an excellent fit for big data processing because it can scale quickly and cheaply to thousands of parallel functions. Existing serverless platforms, however, isolate functions in ephemeral, stateless containers. This means that functions cannot efficiently share memory, forcing users to serialise data repeatedly when composing functions. We observe that container-based isolation is ill-suited for serverless big data processing. Instead it requires a lightweight isolation approach that allows for efficient state sharing. We introduce Faaslets, a new isolation abstraction for serverless big data computing. Faaslets isolate the memory of executed functions using software-fault isolation (SFI), as provided by WebAssembly, while allowing memory regions to be shared between functions in the same address space. Faaslets can thus avoid expensive data movement when functions are co-located on the same machine. Our runtime for Faaslets, Faasm, isolates other resources, e.g. CPU and network, using standard Linux cgroups, and provides a low-level POSIX host interface for networking, file system access and dynamic loading. To reduce initialisation times, Faasm restores Faaslets from already-initialised snapshots. We compare Faasm to a standard container-based platform and show that, when training a machine learning model, it achieves a 2x speed-up with 10x less memory; for serving machine learning inference, Faasm doubles the throughput and reduces tail latency by 90%. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09344">PDF</a>
<h3>No. 3	Methods and Experiences for Developing Abstractions for Data-intensive,  Scientific Applications</h3><h4>Andre Luckow, Shantenu Jha</h4> Abstract: Developing software for scientific applications that require the integration of diverse types of computing, instruments and data present challenges that are distinct from commercial software, due to scale, heterogeneity, and the need to integrate various programming and computational models with evolving and heterogeneous infrastructure. Pervasive and effective abstractions are thus critical. The process of developing abstractions for scientific applications and infrastructures is not well understood. While theory-based approaches are suited for well-defined, closed environments, they have severe limitations for designing abstractions for complex, real-world systems. The design science research (DSR) method provides the basis for designing effective systems that can handle real-world complexities. DSR consists of two complementary phases: design and evaluation. This paper applies the DSR method to the development of abstractions for scientific applications. Specifically, we address the critical problem of distributed resource management on heterogeneous infrastructure, a challenge that currently limits many scientific applications. We use the pilot-abstraction, a widely used resource management abstraction for high-performance, high throughput, big data, and streaming applications, as a case study. We evaluate the activities of the process and extensively evaluate the artifacts using different methods, including conceptual modeling, performance characterizations, and modeling. We demonstrate the applicability of the DSR method for holistically handling the complexity of parallel and distributed computing environments addressing important application, system, and infrastructure challenges of scientific applications. Finally, we capture our experiences and formulate different lessons learned. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09009">PDF</a>
<h3>No. 4	Distributed Mean Estimation with Optimal Error Bounds</h3><h4>Dan Alistarh, Saleh Ashkboos, Peter Davies</h4> Abstract: Motivated by applications to distributed optimization and machine learning, we consider the distributed mean estimation problem, in which $n$ nodes are each assigned a multi-dimensional input vector, and must cooperate to estimate the mean of the input vectors, while minimizing communication. In this paper, we provide the first tight bounds for this problem, in terms of the trade-off between the amount of communication between nodes and the variance of the node estimates relative to the true value of the mean. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09268">PDF</a>
<h3>No. 5	A polynomial lower bound on adaptive complexity of submodular  maximization</h3><h4>Wenzheng Li, Paul Liu, Jan Vondrak</h4> Abstract: In large-data applications, it is desirable to design algorithms with a high degree of parallelization. In the context of submodular optimization, adaptive complexity has become a widely-used measure of an algorithm's "sequentiality". Algorithms in the adaptive model proceed in rounds, and can issue polynomially many queries to a function $f$ in each round. The queries in each round must be independent, produced by a computation that depends only on query results obtained in previous rounds. In this work, we examine two fundamental variants of submodular maximization in the adaptive complexity model: cardinality-constrained monotone maximization, and unconstrained non-monotone maximization. Our main result is that an $r$-round algorithm for cardinality-constrained monotone maximization cannot achieve a factor better than $1 - 1/e - \Omega(\min \{ \frac{1}{r}, \frac{\log^2 n}{r^3} \})$, for any $r < n^c$ (where $c>0$ is some constant). This is the first result showing that the number of rounds must blow up polynomially large as we approach the optimal factor of $1-1/e$. For the unconstrained non-monotone maximization problem, we show a positive result: For every instance, and every $\delta>0$, either we obtain a $(1/2-\delta)$-approximation in $1$ round, or a $(1/2+\Omega(\delta^2))$-approximation in $O(1/\delta^2)$ rounds. In particular (in contrast to the cardinality-constrained case), there cannot be an instance where (i) it is impossible to achieve a factor better than $1/2$ regardless of the number of rounds, and (ii) it takes $r$ rounds to achieve a factor of $1/2-O(1/r)$. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09130">PDF</a>
<h3>No. 6	Asynchronous parallel adaptive stochastic gradient methods</h3><h4>Yangyang Xu, Colin Sutcher-Shepard, Yibo Xu, Jie Chen</h4> Abstract: Stochastic gradient methods (SGMs) are the predominant approaches to train deep learning models. The adaptive versions (e.g., Adam and AMSGrad) have been extensively used in practice, partly because they achieve faster convergence than the non-adaptive versions while incurring little overhead. On the other hand, asynchronous (async) parallel computing has exhibited much better speed-up over its synchronous (sync) counterpart. However, async-parallel implementation has only been demonstrated to the non-adaptive SGMs. The difficulty for adaptive SGMs originates from the second moment term that makes the convergence analysis challenging with async updates. In this paper, we propose an async-parallel adaptive SGM based on AMSGrad. We show that the proposed method inherits the convergence guarantee of AMSGrad for both convex and non-convex problems, if the staleness (also called delay) caused by asynchrony is bounded. Our convergence rate results indicate a nearly linear parallelization speed-up if $\tau=o(K^{\frac{1}{4}})$, where $\tau$ is the staleness and $K$ is the number of iterations. The proposed method is tested on both convex and non-convex machine learning problems, and the numerical results demonstrate its clear advantages over the sync counterpart. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09095">PDF</a>
<h3>No. 7	Uncertainty Principle for Communication Compression in Distributed and  Federated Learning and the Search for an Optimal Compressor</h3><h4>Mher Safaryan, Egor Shulgin, Peter Richt√°rik</h4> Abstract: In order to mitigate the high communication cost in distributed and federated learning, various vector compression schemes, such as quantization, sparsification and dithering, have become very popular. In designing a compression method, one aims to communicate as few bits as possible, which minimizes the cost per communication round, while at the same time attempting to impart as little distortion (variance) to the communicated messages as possible, which minimizes the adverse effect of the compression on the overall number of communication rounds. However, intuitively, these two goals are fundamentally in conflict: the more compression we allow, the more distorted the messages become. We formalize this intuition and prove an {\em uncertainty principle} for randomized compression operators, thus quantifying this limitation mathematically, and {\em effectively providing lower bounds on what might be achievable with communication compression}. Motivated by these developments, we call for the search for the optimal compression operator. In an attempt to take a first step in this direction, we construct a new unbiased compression method inspired by the Kashin representation of vectors, which we call {\em Kashin compression (KC)}. In contrast to all previously proposed compression mechanisms, we prove that KC enjoys a {\em dimension independent} variance bound with an explicit formula even in the regime when only a few bits need to be communicate per each vector entry. We show how KC can be provably and efficiently combined with several existing optimization algorithms, in all cases leading to communication complexity improvements on previous state of the art. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08958">PDF</a><h2>2020-02-29</h2>
<h3>No. 1	Fast Implementation of Morphological Filtering Using ARM NEON Extension</h3><h4>Elena Limonova, Arseny Terekhin, Dmitry Nikolaev, Vladimir Arlazarov</h4> Abstract: In this paper we consider speedup potential of morphological image filtering on ARM processors. Morphological operations are widely used in image analysis and recognition and their speedup in some cases can significantly reduce overall execution time of recognition. More specifically, we propose fast implementation of erosion and dilation using ARM SIMD extension NEON. These operations with the rectangular structuring element are separable. They were implemented using the advantages of separability as sequential horizontal and vertical passes. Each pass was implemented using van Herk/Gil-Werman algorithm for large windows and low-constant linear complexity algorithm for small windows. Final implementation was improved with SIMD and used a combination of these methods. We also considered fast transpose implementation of 8x8 and 16x16 matrices using ARM NEON to get additional computational gain for morphological operations. Experiments showed 3 times efficiency increase for final implementation of erosion and dilation compared to van Herk/Gil-Werman algorithm without SIMD, 5.7 times speedup for 8x8 matrix transpose and 12 times speedup for 16x16 matrix transpose compared to transpose without SIMD. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09474">PDF</a>
<h3>No. 2	Faasm: Lightweight Isolation for Efficient Stateful Serverless Computing</h3><h4>Simon Shillaker, Peter Pietzuch</h4> Abstract: Serverless computing is an excellent fit for big data processing because it can scale quickly and cheaply to thousands of parallel functions. Existing serverless platforms, however, isolate functions in ephemeral, stateless containers. This means that functions cannot efficiently share memory, forcing users to serialise data repeatedly when composing functions. We observe that container-based isolation is ill-suited for serverless big data processing. Instead it requires a lightweight isolation approach that allows for efficient state sharing. We introduce Faaslets, a new isolation abstraction for serverless big data computing. Faaslets isolate the memory of executed functions using software-fault isolation (SFI), as provided by WebAssembly, while allowing memory regions to be shared between functions in the same address space. Faaslets can thus avoid expensive data movement when functions are co-located on the same machine. Our runtime for Faaslets, Faasm, isolates other resources, e.g. CPU and network, using standard Linux cgroups, and provides a low-level POSIX host interface for networking, file system access and dynamic loading. To reduce initialisation times, Faasm restores Faaslets from already-initialised snapshots. We compare Faasm to a standard container-based platform and show that, when training a machine learning model, it achieves a 2x speed-up with 10x less memory; for serving machine learning inference, Faasm doubles the throughput and reduces tail latency by 90%. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09344">PDF</a>
<h3>No. 3	Methods and Experiences for Developing Abstractions for Data-intensive,  Scientific Applications</h3><h4>Andre Luckow, Shantenu Jha</h4> Abstract: Developing software for scientific applications that require the integration of diverse types of computing, instruments and data present challenges that are distinct from commercial software, due to scale, heterogeneity, and the need to integrate various programming and computational models with evolving and heterogeneous infrastructure. Pervasive and effective abstractions are thus critical. The process of developing abstractions for scientific applications and infrastructures is not well understood. While theory-based approaches are suited for well-defined, closed environments, they have severe limitations for designing abstractions for complex, real-world systems. The design science research (DSR) method provides the basis for designing effective systems that can handle real-world complexities. DSR consists of two complementary phases: design and evaluation. This paper applies the DSR method to the development of abstractions for scientific applications. Specifically, we address the critical problem of distributed resource management on heterogeneous infrastructure, a challenge that currently limits many scientific applications. We use the pilot-abstraction, a widely used resource management abstraction for high-performance, high throughput, big data, and streaming applications, as a case study. We evaluate the activities of the process and extensively evaluate the artifacts using different methods, including conceptual modeling, performance characterizations, and modeling. We demonstrate the applicability of the DSR method for holistically handling the complexity of parallel and distributed computing environments addressing important application, system, and infrastructure challenges of scientific applications. Finally, we capture our experiences and formulate different lessons learned. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09009">PDF</a>
<h3>No. 4	Distributed Mean Estimation with Optimal Error Bounds</h3><h4>Dan Alistarh, Saleh Ashkboos, Peter Davies</h4> Abstract: Motivated by applications to distributed optimization and machine learning, we consider the distributed mean estimation problem, in which $n$ nodes are each assigned a multi-dimensional input vector, and must cooperate to estimate the mean of the input vectors, while minimizing communication. In this paper, we provide the first tight bounds for this problem, in terms of the trade-off between the amount of communication between nodes and the variance of the node estimates relative to the true value of the mean. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09268">PDF</a>
<h3>No. 5	A polynomial lower bound on adaptive complexity of submodular  maximization</h3><h4>Wenzheng Li, Paul Liu, Jan Vondrak</h4> Abstract: In large-data applications, it is desirable to design algorithms with a high degree of parallelization. In the context of submodular optimization, adaptive complexity has become a widely-used measure of an algorithm's "sequentiality". Algorithms in the adaptive model proceed in rounds, and can issue polynomially many queries to a function $f$ in each round. The queries in each round must be independent, produced by a computation that depends only on query results obtained in previous rounds. In this work, we examine two fundamental variants of submodular maximization in the adaptive complexity model: cardinality-constrained monotone maximization, and unconstrained non-monotone maximization. Our main result is that an $r$-round algorithm for cardinality-constrained monotone maximization cannot achieve a factor better than $1 - 1/e - \Omega(\min \{ \frac{1}{r}, \frac{\log^2 n}{r^3} \})$, for any $r < n^c$ (where $c>0$ is some constant). This is the first result showing that the number of rounds must blow up polynomially large as we approach the optimal factor of $1-1/e$. For the unconstrained non-monotone maximization problem, we show a positive result: For every instance, and every $\delta>0$, either we obtain a $(1/2-\delta)$-approximation in $1$ round, or a $(1/2+\Omega(\delta^2))$-approximation in $O(1/\delta^2)$ rounds. In particular (in contrast to the cardinality-constrained case), there cannot be an instance where (i) it is impossible to achieve a factor better than $1/2$ regardless of the number of rounds, and (ii) it takes $r$ rounds to achieve a factor of $1/2-O(1/r)$. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09130">PDF</a>
<h3>No. 6	Asynchronous parallel adaptive stochastic gradient methods</h3><h4>Yangyang Xu, Colin Sutcher-Shepard, Yibo Xu, Jie Chen</h4> Abstract: Stochastic gradient methods (SGMs) are the predominant approaches to train deep learning models. The adaptive versions (e.g., Adam and AMSGrad) have been extensively used in practice, partly because they achieve faster convergence than the non-adaptive versions while incurring little overhead. On the other hand, asynchronous (async) parallel computing has exhibited much better speed-up over its synchronous (sync) counterpart. However, async-parallel implementation has only been demonstrated to the non-adaptive SGMs. The difficulty for adaptive SGMs originates from the second moment term that makes the convergence analysis challenging with async updates. In this paper, we propose an async-parallel adaptive SGM based on AMSGrad. We show that the proposed method inherits the convergence guarantee of AMSGrad for both convex and non-convex problems, if the staleness (also called delay) caused by asynchrony is bounded. Our convergence rate results indicate a nearly linear parallelization speed-up if $\tau=o(K^{\frac{1}{4}})$, where $\tau$ is the staleness and $K$ is the number of iterations. The proposed method is tested on both convex and non-convex machine learning problems, and the numerical results demonstrate its clear advantages over the sync counterpart. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09095">PDF</a>
<h3>No. 7	Uncertainty Principle for Communication Compression in Distributed and  Federated Learning and the Search for an Optimal Compressor</h3><h4>Mher Safaryan, Egor Shulgin, Peter Richt√°rik</h4> Abstract: In order to mitigate the high communication cost in distributed and federated learning, various vector compression schemes, such as quantization, sparsification and dithering, have become very popular. In designing a compression method, one aims to communicate as few bits as possible, which minimizes the cost per communication round, while at the same time attempting to impart as little distortion (variance) to the communicated messages as possible, which minimizes the adverse effect of the compression on the overall number of communication rounds. However, intuitively, these two goals are fundamentally in conflict: the more compression we allow, the more distorted the messages become. We formalize this intuition and prove an {\em uncertainty principle} for randomized compression operators, thus quantifying this limitation mathematically, and {\em effectively providing lower bounds on what might be achievable with communication compression}. Motivated by these developments, we call for the search for the optimal compression operator. In an attempt to take a first step in this direction, we construct a new unbiased compression method inspired by the Kashin representation of vectors, which we call {\em Kashin compression (KC)}. In contrast to all previously proposed compression mechanisms, we prove that KC enjoys a {\em dimension independent} variance bound with an explicit formula even in the regime when only a few bits need to be communicate per each vector entry. We show how KC can be provably and efficiently combined with several existing optimization algorithms, in all cases leading to communication complexity improvements on previous state of the art. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08958">PDF</a><h2>2020-02-28</h2>
<h3>No. 1	Fast Implementation of Morphological Filtering Using ARM NEON Extension</h3><h4>Elena Limonova, Arseny Terekhin, Dmitry Nikolaev, Vladimir Arlazarov</h4> Abstract: In this paper we consider speedup potential of morphological image filtering on ARM processors. Morphological operations are widely used in image analysis and recognition and their speedup in some cases can significantly reduce overall execution time of recognition. More specifically, we propose fast implementation of erosion and dilation using ARM SIMD extension NEON. These operations with the rectangular structuring element are separable. They were implemented using the advantages of separability as sequential horizontal and vertical passes. Each pass was implemented using van Herk/Gil-Werman algorithm for large windows and low-constant linear complexity algorithm for small windows. Final implementation was improved with SIMD and used a combination of these methods. We also considered fast transpose implementation of 8x8 and 16x16 matrices using ARM NEON to get additional computational gain for morphological operations. Experiments showed 3 times efficiency increase for final implementation of erosion and dilation compared to van Herk/Gil-Werman algorithm without SIMD, 5.7 times speedup for 8x8 matrix transpose and 12 times speedup for 16x16 matrix transpose compared to transpose without SIMD. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09474">PDF</a>
<h3>No. 2	Faasm: Lightweight Isolation for Efficient Stateful Serverless Computing</h3><h4>Simon Shillaker, Peter Pietzuch</h4> Abstract: Serverless computing is an excellent fit for big data processing because it can scale quickly and cheaply to thousands of parallel functions. Existing serverless platforms, however, isolate functions in ephemeral, stateless containers. This means that functions cannot efficiently share memory, forcing users to serialise data repeatedly when composing functions. We observe that container-based isolation is ill-suited for serverless big data processing. Instead it requires a lightweight isolation approach that allows for efficient state sharing. We introduce Faaslets, a new isolation abstraction for serverless big data computing. Faaslets isolate the memory of executed functions using software-fault isolation (SFI), as provided by WebAssembly, while allowing memory regions to be shared between functions in the same address space. Faaslets can thus avoid expensive data movement when functions are co-located on the same machine. Our runtime for Faaslets, Faasm, isolates other resources, e.g. CPU and network, using standard Linux cgroups, and provides a low-level POSIX host interface for networking, file system access and dynamic loading. To reduce initialisation times, Faasm restores Faaslets from already-initialised snapshots. We compare Faasm to a standard container-based platform and show that, when training a machine learning model, it achieves a 2x speed-up with 10x less memory; for serving machine learning inference, Faasm doubles the throughput and reduces tail latency by 90%. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09344">PDF</a>
<h3>No. 3	Methods and Experiences for Developing Abstractions for Data-intensive,  Scientific Applications</h3><h4>Andre Luckow, Shantenu Jha</h4> Abstract: Developing software for scientific applications that require the integration of diverse types of computing, instruments and data present challenges that are distinct from commercial software, due to scale, heterogeneity, and the need to integrate various programming and computational models with evolving and heterogeneous infrastructure. Pervasive and effective abstractions are thus critical. The process of developing abstractions for scientific applications and infrastructures is not well understood. While theory-based approaches are suited for well-defined, closed environments, they have severe limitations for designing abstractions for complex, real-world systems. The design science research (DSR) method provides the basis for designing effective systems that can handle real-world complexities. DSR consists of two complementary phases: design and evaluation. This paper applies the DSR method to the development of abstractions for scientific applications. Specifically, we address the critical problem of distributed resource management on heterogeneous infrastructure, a challenge that currently limits many scientific applications. We use the pilot-abstraction, a widely used resource management abstraction for high-performance, high throughput, big data, and streaming applications, as a case study. We evaluate the activities of the process and extensively evaluate the artifacts using different methods, including conceptual modeling, performance characterizations, and modeling. We demonstrate the applicability of the DSR method for holistically handling the complexity of parallel and distributed computing environments addressing important application, system, and infrastructure challenges of scientific applications. Finally, we capture our experiences and formulate different lessons learned. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09009">PDF</a>
<h3>No. 4	Distributed Mean Estimation with Optimal Error Bounds</h3><h4>Dan Alistarh, Saleh Ashkboos, Peter Davies</h4> Abstract: Motivated by applications to distributed optimization and machine learning, we consider the distributed mean estimation problem, in which $n$ nodes are each assigned a multi-dimensional input vector, and must cooperate to estimate the mean of the input vectors, while minimizing communication. In this paper, we provide the first tight bounds for this problem, in terms of the trade-off between the amount of communication between nodes and the variance of the node estimates relative to the true value of the mean. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09268">PDF</a>
<h3>No. 5	A polynomial lower bound on adaptive complexity of submodular  maximization</h3><h4>Wenzheng Li, Paul Liu, Jan Vondrak</h4> Abstract: In large-data applications, it is desirable to design algorithms with a high degree of parallelization. In the context of submodular optimization, adaptive complexity has become a widely-used measure of an algorithm's "sequentiality". Algorithms in the adaptive model proceed in rounds, and can issue polynomially many queries to a function $f$ in each round. The queries in each round must be independent, produced by a computation that depends only on query results obtained in previous rounds. In this work, we examine two fundamental variants of submodular maximization in the adaptive complexity model: cardinality-constrained monotone maximization, and unconstrained non-monotone maximization. Our main result is that an $r$-round algorithm for cardinality-constrained monotone maximization cannot achieve a factor better than $1 - 1/e - \Omega(\min \{ \frac{1}{r}, \frac{\log^2 n}{r^3} \})$, for any $r < n^c$ (where $c>0$ is some constant). This is the first result showing that the number of rounds must blow up polynomially large as we approach the optimal factor of $1-1/e$. For the unconstrained non-monotone maximization problem, we show a positive result: For every instance, and every $\delta>0$, either we obtain a $(1/2-\delta)$-approximation in $1$ round, or a $(1/2+\Omega(\delta^2))$-approximation in $O(1/\delta^2)$ rounds. In particular (in contrast to the cardinality-constrained case), there cannot be an instance where (i) it is impossible to achieve a factor better than $1/2$ regardless of the number of rounds, and (ii) it takes $r$ rounds to achieve a factor of $1/2-O(1/r)$. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09130">PDF</a>
<h3>No. 6	Asynchronous parallel adaptive stochastic gradient methods</h3><h4>Yangyang Xu, Colin Sutcher-Shepard, Yibo Xu, Jie Chen</h4> Abstract: Stochastic gradient methods (SGMs) are the predominant approaches to train deep learning models. The adaptive versions (e.g., Adam and AMSGrad) have been extensively used in practice, partly because they achieve faster convergence than the non-adaptive versions while incurring little overhead. On the other hand, asynchronous (async) parallel computing has exhibited much better speed-up over its synchronous (sync) counterpart. However, async-parallel implementation has only been demonstrated to the non-adaptive SGMs. The difficulty for adaptive SGMs originates from the second moment term that makes the convergence analysis challenging with async updates. In this paper, we propose an async-parallel adaptive SGM based on AMSGrad. We show that the proposed method inherits the convergence guarantee of AMSGrad for both convex and non-convex problems, if the staleness (also called delay) caused by asynchrony is bounded. Our convergence rate results indicate a nearly linear parallelization speed-up if $\tau=o(K^{\frac{1}{4}})$, where $\tau$ is the staleness and $K$ is the number of iterations. The proposed method is tested on both convex and non-convex machine learning problems, and the numerical results demonstrate its clear advantages over the sync counterpart. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09095">PDF</a>
<h3>No. 7	Uncertainty Principle for Communication Compression in Distributed and  Federated Learning and the Search for an Optimal Compressor</h3><h4>Mher Safaryan, Egor Shulgin, Peter Richt√°rik</h4> Abstract: In order to mitigate the high communication cost in distributed and federated learning, various vector compression schemes, such as quantization, sparsification and dithering, have become very popular. In designing a compression method, one aims to communicate as few bits as possible, which minimizes the cost per communication round, while at the same time attempting to impart as little distortion (variance) to the communicated messages as possible, which minimizes the adverse effect of the compression on the overall number of communication rounds. However, intuitively, these two goals are fundamentally in conflict: the more compression we allow, the more distorted the messages become. We formalize this intuition and prove an {\em uncertainty principle} for randomized compression operators, thus quantifying this limitation mathematically, and {\em effectively providing lower bounds on what might be achievable with communication compression}. Motivated by these developments, we call for the search for the optimal compression operator. In an attempt to take a first step in this direction, we construct a new unbiased compression method inspired by the Kashin representation of vectors, which we call {\em Kashin compression (KC)}. In contrast to all previously proposed compression mechanisms, we prove that KC enjoys a {\em dimension independent} variance bound with an explicit formula even in the regime when only a few bits need to be communicate per each vector entry. We show how KC can be provably and efficiently combined with several existing optimization algorithms, in all cases leading to communication complexity improvements on previous state of the art. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08958">PDF</a><h2>2020-02-27</h2>
<h3>No. 1	Fast Implementation of Morphological Filtering Using ARM NEON Extension</h3><h4>Elena Limonova, Arseny Terekhin, Dmitry Nikolaev, Vladimir Arlazarov</h4> Abstract: In this paper we consider speedup potential of morphological image filtering on ARM processors. Morphological operations are widely used in image analysis and recognition and their speedup in some cases can significantly reduce overall execution time of recognition. More specifically, we propose fast implementation of erosion and dilation using ARM SIMD extension NEON. These operations with the rectangular structuring element are separable. They were implemented using the advantages of separability as sequential horizontal and vertical passes. Each pass was implemented using van Herk/Gil-Werman algorithm for large windows and low-constant linear complexity algorithm for small windows. Final implementation was improved with SIMD and used a combination of these methods. We also considered fast transpose implementation of 8x8 and 16x16 matrices using ARM NEON to get additional computational gain for morphological operations. Experiments showed 3 times efficiency increase for final implementation of erosion and dilation compared to van Herk/Gil-Werman algorithm without SIMD, 5.7 times speedup for 8x8 matrix transpose and 12 times speedup for 16x16 matrix transpose compared to transpose without SIMD. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09474">PDF</a>
<h3>No. 2	Faasm: Lightweight Isolation for Efficient Stateful Serverless Computing</h3><h4>Simon Shillaker, Peter Pietzuch</h4> Abstract: Serverless computing is an excellent fit for big data processing because it can scale quickly and cheaply to thousands of parallel functions. Existing serverless platforms, however, isolate functions in ephemeral, stateless containers. This means that functions cannot efficiently share memory, forcing users to serialise data repeatedly when composing functions. We observe that container-based isolation is ill-suited for serverless big data processing. Instead it requires a lightweight isolation approach that allows for efficient state sharing. We introduce Faaslets, a new isolation abstraction for serverless big data computing. Faaslets isolate the memory of executed functions using software-fault isolation (SFI), as provided by WebAssembly, while allowing memory regions to be shared between functions in the same address space. Faaslets can thus avoid expensive data movement when functions are co-located on the same machine. Our runtime for Faaslets, Faasm, isolates other resources, e.g. CPU and network, using standard Linux cgroups, and provides a low-level POSIX host interface for networking, file system access and dynamic loading. To reduce initialisation times, Faasm restores Faaslets from already-initialised snapshots. We compare Faasm to a standard container-based platform and show that, when training a machine learning model, it achieves a 2x speed-up with 10x less memory; for serving machine learning inference, Faasm doubles the throughput and reduces tail latency by 90%. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09344">PDF</a>
<h3>No. 3	Methods and Experiences for Developing Abstractions for Data-intensive,  Scientific Applications</h3><h4>Andre Luckow, Shantenu Jha</h4> Abstract: Developing software for scientific applications that require the integration of diverse types of computing, instruments and data present challenges that are distinct from commercial software, due to scale, heterogeneity, and the need to integrate various programming and computational models with evolving and heterogeneous infrastructure. Pervasive and effective abstractions are thus critical. The process of developing abstractions for scientific applications and infrastructures is not well understood. While theory-based approaches are suited for well-defined, closed environments, they have severe limitations for designing abstractions for complex, real-world systems. The design science research (DSR) method provides the basis for designing effective systems that can handle real-world complexities. DSR consists of two complementary phases: design and evaluation. This paper applies the DSR method to the development of abstractions for scientific applications. Specifically, we address the critical problem of distributed resource management on heterogeneous infrastructure, a challenge that currently limits many scientific applications. We use the pilot-abstraction, a widely used resource management abstraction for high-performance, high throughput, big data, and streaming applications, as a case study. We evaluate the activities of the process and extensively evaluate the artifacts using different methods, including conceptual modeling, performance characterizations, and modeling. We demonstrate the applicability of the DSR method for holistically handling the complexity of parallel and distributed computing environments addressing important application, system, and infrastructure challenges of scientific applications. Finally, we capture our experiences and formulate different lessons learned. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09009">PDF</a>
<h3>No. 4	Distributed Mean Estimation with Optimal Error Bounds</h3><h4>Dan Alistarh, Saleh Ashkboos, Peter Davies</h4> Abstract: Motivated by applications to distributed optimization and machine learning, we consider the distributed mean estimation problem, in which $n$ nodes are each assigned a multi-dimensional input vector, and must cooperate to estimate the mean of the input vectors, while minimizing communication. In this paper, we provide the first tight bounds for this problem, in terms of the trade-off between the amount of communication between nodes and the variance of the node estimates relative to the true value of the mean. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09268">PDF</a>
<h3>No. 5	A polynomial lower bound on adaptive complexity of submodular  maximization</h3><h4>Wenzheng Li, Paul Liu, Jan Vondrak</h4> Abstract: In large-data applications, it is desirable to design algorithms with a high degree of parallelization. In the context of submodular optimization, adaptive complexity has become a widely-used measure of an algorithm's "sequentiality". Algorithms in the adaptive model proceed in rounds, and can issue polynomially many queries to a function $f$ in each round. The queries in each round must be independent, produced by a computation that depends only on query results obtained in previous rounds. In this work, we examine two fundamental variants of submodular maximization in the adaptive complexity model: cardinality-constrained monotone maximization, and unconstrained non-monotone maximization. Our main result is that an $r$-round algorithm for cardinality-constrained monotone maximization cannot achieve a factor better than $1 - 1/e - \Omega(\min \{ \frac{1}{r}, \frac{\log^2 n}{r^3} \})$, for any $r < n^c$ (where $c>0$ is some constant). This is the first result showing that the number of rounds must blow up polynomially large as we approach the optimal factor of $1-1/e$. For the unconstrained non-monotone maximization problem, we show a positive result: For every instance, and every $\delta>0$, either we obtain a $(1/2-\delta)$-approximation in $1$ round, or a $(1/2+\Omega(\delta^2))$-approximation in $O(1/\delta^2)$ rounds. In particular (in contrast to the cardinality-constrained case), there cannot be an instance where (i) it is impossible to achieve a factor better than $1/2$ regardless of the number of rounds, and (ii) it takes $r$ rounds to achieve a factor of $1/2-O(1/r)$. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09130">PDF</a>
<h3>No. 6	Asynchronous parallel adaptive stochastic gradient methods</h3><h4>Yangyang Xu, Colin Sutcher-Shepard, Yibo Xu, Jie Chen</h4> Abstract: Stochastic gradient methods (SGMs) are the predominant approaches to train deep learning models. The adaptive versions (e.g., Adam and AMSGrad) have been extensively used in practice, partly because they achieve faster convergence than the non-adaptive versions while incurring little overhead. On the other hand, asynchronous (async) parallel computing has exhibited much better speed-up over its synchronous (sync) counterpart. However, async-parallel implementation has only been demonstrated to the non-adaptive SGMs. The difficulty for adaptive SGMs originates from the second moment term that makes the convergence analysis challenging with async updates. In this paper, we propose an async-parallel adaptive SGM based on AMSGrad. We show that the proposed method inherits the convergence guarantee of AMSGrad for both convex and non-convex problems, if the staleness (also called delay) caused by asynchrony is bounded. Our convergence rate results indicate a nearly linear parallelization speed-up if $\tau=o(K^{\frac{1}{4}})$, where $\tau$ is the staleness and $K$ is the number of iterations. The proposed method is tested on both convex and non-convex machine learning problems, and the numerical results demonstrate its clear advantages over the sync counterpart. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09095">PDF</a>
<h3>No. 7	Uncertainty Principle for Communication Compression in Distributed and  Federated Learning and the Search for an Optimal Compressor</h3><h4>Mher Safaryan, Egor Shulgin, Peter Richt√°rik</h4> Abstract: In order to mitigate the high communication cost in distributed and federated learning, various vector compression schemes, such as quantization, sparsification and dithering, have become very popular. In designing a compression method, one aims to communicate as few bits as possible, which minimizes the cost per communication round, while at the same time attempting to impart as little distortion (variance) to the communicated messages as possible, which minimizes the adverse effect of the compression on the overall number of communication rounds. However, intuitively, these two goals are fundamentally in conflict: the more compression we allow, the more distorted the messages become. We formalize this intuition and prove an {\em uncertainty principle} for randomized compression operators, thus quantifying this limitation mathematically, and {\em effectively providing lower bounds on what might be achievable with communication compression}. Motivated by these developments, we call for the search for the optimal compression operator. In an attempt to take a first step in this direction, we construct a new unbiased compression method inspired by the Kashin representation of vectors, which we call {\em Kashin compression (KC)}. In contrast to all previously proposed compression mechanisms, we prove that KC enjoys a {\em dimension independent} variance bound with an explicit formula even in the regime when only a few bits need to be communicate per each vector entry. We show how KC can be provably and efficiently combined with several existing optimization algorithms, in all cases leading to communication complexity improvements on previous state of the art. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08958">PDF</a><h2>2020-02-26</h2>
<h3>No. 1	Fast Implementation of Morphological Filtering Using ARM NEON Extension</h3><h4>Elena Limonova, Arseny Terekhin, Dmitry Nikolaev, Vladimir Arlazarov</h4> Abstract: In this paper we consider speedup potential of morphological image filtering on ARM processors. Morphological operations are widely used in image analysis and recognition and their speedup in some cases can significantly reduce overall execution time of recognition. More specifically, we propose fast implementation of erosion and dilation using ARM SIMD extension NEON. These operations with the rectangular structuring element are separable. They were implemented using the advantages of separability as sequential horizontal and vertical passes. Each pass was implemented using van Herk/Gil-Werman algorithm for large windows and low-constant linear complexity algorithm for small windows. Final implementation was improved with SIMD and used a combination of these methods. We also considered fast transpose implementation of 8x8 and 16x16 matrices using ARM NEON to get additional computational gain for morphological operations. Experiments showed 3 times efficiency increase for final implementation of erosion and dilation compared to van Herk/Gil-Werman algorithm without SIMD, 5.7 times speedup for 8x8 matrix transpose and 12 times speedup for 16x16 matrix transpose compared to transpose without SIMD. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09474">PDF</a>
<h3>No. 2	Faasm: Lightweight Isolation for Efficient Stateful Serverless Computing</h3><h4>Simon Shillaker, Peter Pietzuch</h4> Abstract: Serverless computing is an excellent fit for big data processing because it can scale quickly and cheaply to thousands of parallel functions. Existing serverless platforms, however, isolate functions in ephemeral, stateless containers. This means that functions cannot efficiently share memory, forcing users to serialise data repeatedly when composing functions. We observe that container-based isolation is ill-suited for serverless big data processing. Instead it requires a lightweight isolation approach that allows for efficient state sharing. We introduce Faaslets, a new isolation abstraction for serverless big data computing. Faaslets isolate the memory of executed functions using software-fault isolation (SFI), as provided by WebAssembly, while allowing memory regions to be shared between functions in the same address space. Faaslets can thus avoid expensive data movement when functions are co-located on the same machine. Our runtime for Faaslets, Faasm, isolates other resources, e.g. CPU and network, using standard Linux cgroups, and provides a low-level POSIX host interface for networking, file system access and dynamic loading. To reduce initialisation times, Faasm restores Faaslets from already-initialised snapshots. We compare Faasm to a standard container-based platform and show that, when training a machine learning model, it achieves a 2x speed-up with 10x less memory; for serving machine learning inference, Faasm doubles the throughput and reduces tail latency by 90%. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09344">PDF</a>
<h3>No. 3	Methods and Experiences for Developing Abstractions for Data-intensive,  Scientific Applications</h3><h4>Andre Luckow, Shantenu Jha</h4> Abstract: Developing software for scientific applications that require the integration of diverse types of computing, instruments and data present challenges that are distinct from commercial software, due to scale, heterogeneity, and the need to integrate various programming and computational models with evolving and heterogeneous infrastructure. Pervasive and effective abstractions are thus critical. The process of developing abstractions for scientific applications and infrastructures is not well understood. While theory-based approaches are suited for well-defined, closed environments, they have severe limitations for designing abstractions for complex, real-world systems. The design science research (DSR) method provides the basis for designing effective systems that can handle real-world complexities. DSR consists of two complementary phases: design and evaluation. This paper applies the DSR method to the development of abstractions for scientific applications. Specifically, we address the critical problem of distributed resource management on heterogeneous infrastructure, a challenge that currently limits many scientific applications. We use the pilot-abstraction, a widely used resource management abstraction for high-performance, high throughput, big data, and streaming applications, as a case study. We evaluate the activities of the process and extensively evaluate the artifacts using different methods, including conceptual modeling, performance characterizations, and modeling. We demonstrate the applicability of the DSR method for holistically handling the complexity of parallel and distributed computing environments addressing important application, system, and infrastructure challenges of scientific applications. Finally, we capture our experiences and formulate different lessons learned. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09009">PDF</a>
<h3>No. 4	Distributed Mean Estimation with Optimal Error Bounds</h3><h4>Dan Alistarh, Saleh Ashkboos, Peter Davies</h4> Abstract: Motivated by applications to distributed optimization and machine learning, we consider the distributed mean estimation problem, in which $n$ nodes are each assigned a multi-dimensional input vector, and must cooperate to estimate the mean of the input vectors, while minimizing communication. In this paper, we provide the first tight bounds for this problem, in terms of the trade-off between the amount of communication between nodes and the variance of the node estimates relative to the true value of the mean. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09268">PDF</a>
<h3>No. 5	A polynomial lower bound on adaptive complexity of submodular  maximization</h3><h4>Wenzheng Li, Paul Liu, Jan Vondrak</h4> Abstract: In large-data applications, it is desirable to design algorithms with a high degree of parallelization. In the context of submodular optimization, adaptive complexity has become a widely-used measure of an algorithm's "sequentiality". Algorithms in the adaptive model proceed in rounds, and can issue polynomially many queries to a function $f$ in each round. The queries in each round must be independent, produced by a computation that depends only on query results obtained in previous rounds. In this work, we examine two fundamental variants of submodular maximization in the adaptive complexity model: cardinality-constrained monotone maximization, and unconstrained non-monotone maximization. Our main result is that an $r$-round algorithm for cardinality-constrained monotone maximization cannot achieve a factor better than $1 - 1/e - \Omega(\min \{ \frac{1}{r}, \frac{\log^2 n}{r^3} \})$, for any $r < n^c$ (where $c>0$ is some constant). This is the first result showing that the number of rounds must blow up polynomially large as we approach the optimal factor of $1-1/e$. For the unconstrained non-monotone maximization problem, we show a positive result: For every instance, and every $\delta>0$, either we obtain a $(1/2-\delta)$-approximation in $1$ round, or a $(1/2+\Omega(\delta^2))$-approximation in $O(1/\delta^2)$ rounds. In particular (in contrast to the cardinality-constrained case), there cannot be an instance where (i) it is impossible to achieve a factor better than $1/2$ regardless of the number of rounds, and (ii) it takes $r$ rounds to achieve a factor of $1/2-O(1/r)$. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09130">PDF</a>
<h3>No. 6	Asynchronous parallel adaptive stochastic gradient methods</h3><h4>Yangyang Xu, Colin Sutcher-Shepard, Yibo Xu, Jie Chen</h4> Abstract: Stochastic gradient methods (SGMs) are the predominant approaches to train deep learning models. The adaptive versions (e.g., Adam and AMSGrad) have been extensively used in practice, partly because they achieve faster convergence than the non-adaptive versions while incurring little overhead. On the other hand, asynchronous (async) parallel computing has exhibited much better speed-up over its synchronous (sync) counterpart. However, async-parallel implementation has only been demonstrated to the non-adaptive SGMs. The difficulty for adaptive SGMs originates from the second moment term that makes the convergence analysis challenging with async updates. In this paper, we propose an async-parallel adaptive SGM based on AMSGrad. We show that the proposed method inherits the convergence guarantee of AMSGrad for both convex and non-convex problems, if the staleness (also called delay) caused by asynchrony is bounded. Our convergence rate results indicate a nearly linear parallelization speed-up if $\tau=o(K^{\frac{1}{4}})$, where $\tau$ is the staleness and $K$ is the number of iterations. The proposed method is tested on both convex and non-convex machine learning problems, and the numerical results demonstrate its clear advantages over the sync counterpart. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09095">PDF</a>
<h3>No. 7	Uncertainty Principle for Communication Compression in Distributed and  Federated Learning and the Search for an Optimal Compressor</h3><h4>Mher Safaryan, Egor Shulgin, Peter Richt√°rik</h4> Abstract: In order to mitigate the high communication cost in distributed and federated learning, various vector compression schemes, such as quantization, sparsification and dithering, have become very popular. In designing a compression method, one aims to communicate as few bits as possible, which minimizes the cost per communication round, while at the same time attempting to impart as little distortion (variance) to the communicated messages as possible, which minimizes the adverse effect of the compression on the overall number of communication rounds. However, intuitively, these two goals are fundamentally in conflict: the more compression we allow, the more distorted the messages become. We formalize this intuition and prove an {\em uncertainty principle} for randomized compression operators, thus quantifying this limitation mathematically, and {\em effectively providing lower bounds on what might be achievable with communication compression}. Motivated by these developments, we call for the search for the optimal compression operator. In an attempt to take a first step in this direction, we construct a new unbiased compression method inspired by the Kashin representation of vectors, which we call {\em Kashin compression (KC)}. In contrast to all previously proposed compression mechanisms, we prove that KC enjoys a {\em dimension independent} variance bound with an explicit formula even in the regime when only a few bits need to be communicate per each vector entry. We show how KC can be provably and efficiently combined with several existing optimization algorithms, in all cases leading to communication complexity improvements on previous state of the art. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08958">PDF</a><h2>2020-02-25</h2>
<h3>No. 1	Fast Implementation of Morphological Filtering Using ARM NEON Extension</h3><h4>Elena Limonova, Arseny Terekhin, Dmitry Nikolaev, Vladimir Arlazarov</h4> Abstract: In this paper we consider speedup potential of morphological image filtering on ARM processors. Morphological operations are widely used in image analysis and recognition and their speedup in some cases can significantly reduce overall execution time of recognition. More specifically, we propose fast implementation of erosion and dilation using ARM SIMD extension NEON. These operations with the rectangular structuring element are separable. They were implemented using the advantages of separability as sequential horizontal and vertical passes. Each pass was implemented using van Herk/Gil-Werman algorithm for large windows and low-constant linear complexity algorithm for small windows. Final implementation was improved with SIMD and used a combination of these methods. We also considered fast transpose implementation of 8x8 and 16x16 matrices using ARM NEON to get additional computational gain for morphological operations. Experiments showed 3 times efficiency increase for final implementation of erosion and dilation compared to van Herk/Gil-Werman algorithm without SIMD, 5.7 times speedup for 8x8 matrix transpose and 12 times speedup for 16x16 matrix transpose compared to transpose without SIMD. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09474">PDF</a>
<h3>No. 2	Faasm: Lightweight Isolation for Efficient Stateful Serverless Computing</h3><h4>Simon Shillaker, Peter Pietzuch</h4> Abstract: Serverless computing is an excellent fit for big data processing because it can scale quickly and cheaply to thousands of parallel functions. Existing serverless platforms, however, isolate functions in ephemeral, stateless containers. This means that functions cannot efficiently share memory, forcing users to serialise data repeatedly when composing functions. We observe that container-based isolation is ill-suited for serverless big data processing. Instead it requires a lightweight isolation approach that allows for efficient state sharing. We introduce Faaslets, a new isolation abstraction for serverless big data computing. Faaslets isolate the memory of executed functions using software-fault isolation (SFI), as provided by WebAssembly, while allowing memory regions to be shared between functions in the same address space. Faaslets can thus avoid expensive data movement when functions are co-located on the same machine. Our runtime for Faaslets, Faasm, isolates other resources, e.g. CPU and network, using standard Linux cgroups, and provides a low-level POSIX host interface for networking, file system access and dynamic loading. To reduce initialisation times, Faasm restores Faaslets from already-initialised snapshots. We compare Faasm to a standard container-based platform and show that, when training a machine learning model, it achieves a 2x speed-up with 10x less memory; for serving machine learning inference, Faasm doubles the throughput and reduces tail latency by 90%. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09344">PDF</a>
<h3>No. 3	Methods and Experiences for Developing Abstractions for Data-intensive,  Scientific Applications</h3><h4>Andre Luckow, Shantenu Jha</h4> Abstract: Developing software for scientific applications that require the integration of diverse types of computing, instruments and data present challenges that are distinct from commercial software, due to scale, heterogeneity, and the need to integrate various programming and computational models with evolving and heterogeneous infrastructure. Pervasive and effective abstractions are thus critical. The process of developing abstractions for scientific applications and infrastructures is not well understood. While theory-based approaches are suited for well-defined, closed environments, they have severe limitations for designing abstractions for complex, real-world systems. The design science research (DSR) method provides the basis for designing effective systems that can handle real-world complexities. DSR consists of two complementary phases: design and evaluation. This paper applies the DSR method to the development of abstractions for scientific applications. Specifically, we address the critical problem of distributed resource management on heterogeneous infrastructure, a challenge that currently limits many scientific applications. We use the pilot-abstraction, a widely used resource management abstraction for high-performance, high throughput, big data, and streaming applications, as a case study. We evaluate the activities of the process and extensively evaluate the artifacts using different methods, including conceptual modeling, performance characterizations, and modeling. We demonstrate the applicability of the DSR method for holistically handling the complexity of parallel and distributed computing environments addressing important application, system, and infrastructure challenges of scientific applications. Finally, we capture our experiences and formulate different lessons learned. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09009">PDF</a>
<h3>No. 4	Distributed Mean Estimation with Optimal Error Bounds</h3><h4>Dan Alistarh, Saleh Ashkboos, Peter Davies</h4> Abstract: Motivated by applications to distributed optimization and machine learning, we consider the distributed mean estimation problem, in which $n$ nodes are each assigned a multi-dimensional input vector, and must cooperate to estimate the mean of the input vectors, while minimizing communication. In this paper, we provide the first tight bounds for this problem, in terms of the trade-off between the amount of communication between nodes and the variance of the node estimates relative to the true value of the mean. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09268">PDF</a>
<h3>No. 5	A polynomial lower bound on adaptive complexity of submodular  maximization</h3><h4>Wenzheng Li, Paul Liu, Jan Vondrak</h4> Abstract: In large-data applications, it is desirable to design algorithms with a high degree of parallelization. In the context of submodular optimization, adaptive complexity has become a widely-used measure of an algorithm's "sequentiality". Algorithms in the adaptive model proceed in rounds, and can issue polynomially many queries to a function $f$ in each round. The queries in each round must be independent, produced by a computation that depends only on query results obtained in previous rounds. In this work, we examine two fundamental variants of submodular maximization in the adaptive complexity model: cardinality-constrained monotone maximization, and unconstrained non-monotone maximization. Our main result is that an $r$-round algorithm for cardinality-constrained monotone maximization cannot achieve a factor better than $1 - 1/e - \Omega(\min \{ \frac{1}{r}, \frac{\log^2 n}{r^3} \})$, for any $r < n^c$ (where $c>0$ is some constant). This is the first result showing that the number of rounds must blow up polynomially large as we approach the optimal factor of $1-1/e$. For the unconstrained non-monotone maximization problem, we show a positive result: For every instance, and every $\delta>0$, either we obtain a $(1/2-\delta)$-approximation in $1$ round, or a $(1/2+\Omega(\delta^2))$-approximation in $O(1/\delta^2)$ rounds. In particular (in contrast to the cardinality-constrained case), there cannot be an instance where (i) it is impossible to achieve a factor better than $1/2$ regardless of the number of rounds, and (ii) it takes $r$ rounds to achieve a factor of $1/2-O(1/r)$. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09130">PDF</a>
<h3>No. 6	Asynchronous parallel adaptive stochastic gradient methods</h3><h4>Yangyang Xu, Colin Sutcher-Shepard, Yibo Xu, Jie Chen</h4> Abstract: Stochastic gradient methods (SGMs) are the predominant approaches to train deep learning models. The adaptive versions (e.g., Adam and AMSGrad) have been extensively used in practice, partly because they achieve faster convergence than the non-adaptive versions while incurring little overhead. On the other hand, asynchronous (async) parallel computing has exhibited much better speed-up over its synchronous (sync) counterpart. However, async-parallel implementation has only been demonstrated to the non-adaptive SGMs. The difficulty for adaptive SGMs originates from the second moment term that makes the convergence analysis challenging with async updates. In this paper, we propose an async-parallel adaptive SGM based on AMSGrad. We show that the proposed method inherits the convergence guarantee of AMSGrad for both convex and non-convex problems, if the staleness (also called delay) caused by asynchrony is bounded. Our convergence rate results indicate a nearly linear parallelization speed-up if $\tau=o(K^{\frac{1}{4}})$, where $\tau$ is the staleness and $K$ is the number of iterations. The proposed method is tested on both convex and non-convex machine learning problems, and the numerical results demonstrate its clear advantages over the sync counterpart. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09095">PDF</a>
<h3>No. 7	Uncertainty Principle for Communication Compression in Distributed and  Federated Learning and the Search for an Optimal Compressor</h3><h4>Mher Safaryan, Egor Shulgin, Peter Richt√°rik</h4> Abstract: In order to mitigate the high communication cost in distributed and federated learning, various vector compression schemes, such as quantization, sparsification and dithering, have become very popular. In designing a compression method, one aims to communicate as few bits as possible, which minimizes the cost per communication round, while at the same time attempting to impart as little distortion (variance) to the communicated messages as possible, which minimizes the adverse effect of the compression on the overall number of communication rounds. However, intuitively, these two goals are fundamentally in conflict: the more compression we allow, the more distorted the messages become. We formalize this intuition and prove an {\em uncertainty principle} for randomized compression operators, thus quantifying this limitation mathematically, and {\em effectively providing lower bounds on what might be achievable with communication compression}. Motivated by these developments, we call for the search for the optimal compression operator. In an attempt to take a first step in this direction, we construct a new unbiased compression method inspired by the Kashin representation of vectors, which we call {\em Kashin compression (KC)}. In contrast to all previously proposed compression mechanisms, we prove that KC enjoys a {\em dimension independent} variance bound with an explicit formula even in the regime when only a few bits need to be communicate per each vector entry. We show how KC can be provably and efficiently combined with several existing optimization algorithms, in all cases leading to communication complexity improvements on previous state of the art. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08958">PDF</a><h2>2020-02-24</h2>
<h3>No. 1	Fast Implementation of Morphological Filtering Using ARM NEON Extension</h3><h4>Elena Limonova, Arseny Terekhin, Dmitry Nikolaev, Vladimir Arlazarov</h4> Abstract: In this paper we consider speedup potential of morphological image filtering on ARM processors. Morphological operations are widely used in image analysis and recognition and their speedup in some cases can significantly reduce overall execution time of recognition. More specifically, we propose fast implementation of erosion and dilation using ARM SIMD extension NEON. These operations with the rectangular structuring element are separable. They were implemented using the advantages of separability as sequential horizontal and vertical passes. Each pass was implemented using van Herk/Gil-Werman algorithm for large windows and low-constant linear complexity algorithm for small windows. Final implementation was improved with SIMD and used a combination of these methods. We also considered fast transpose implementation of 8x8 and 16x16 matrices using ARM NEON to get additional computational gain for morphological operations. Experiments showed 3 times efficiency increase for final implementation of erosion and dilation compared to van Herk/Gil-Werman algorithm without SIMD, 5.7 times speedup for 8x8 matrix transpose and 12 times speedup for 16x16 matrix transpose compared to transpose without SIMD. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09474">PDF</a>
<h3>No. 2	Faasm: Lightweight Isolation for Efficient Stateful Serverless Computing</h3><h4>Simon Shillaker, Peter Pietzuch</h4> Abstract: Serverless computing is an excellent fit for big data processing because it can scale quickly and cheaply to thousands of parallel functions. Existing serverless platforms, however, isolate functions in ephemeral, stateless containers. This means that functions cannot efficiently share memory, forcing users to serialise data repeatedly when composing functions. We observe that container-based isolation is ill-suited for serverless big data processing. Instead it requires a lightweight isolation approach that allows for efficient state sharing. We introduce Faaslets, a new isolation abstraction for serverless big data computing. Faaslets isolate the memory of executed functions using software-fault isolation (SFI), as provided by WebAssembly, while allowing memory regions to be shared between functions in the same address space. Faaslets can thus avoid expensive data movement when functions are co-located on the same machine. Our runtime for Faaslets, Faasm, isolates other resources, e.g. CPU and network, using standard Linux cgroups, and provides a low-level POSIX host interface for networking, file system access and dynamic loading. To reduce initialisation times, Faasm restores Faaslets from already-initialised snapshots. We compare Faasm to a standard container-based platform and show that, when training a machine learning model, it achieves a 2x speed-up with 10x less memory; for serving machine learning inference, Faasm doubles the throughput and reduces tail latency by 90%. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09344">PDF</a>
<h3>No. 3	Methods and Experiences for Developing Abstractions for Data-intensive,  Scientific Applications</h3><h4>Andre Luckow, Shantenu Jha</h4> Abstract: Developing software for scientific applications that require the integration of diverse types of computing, instruments and data present challenges that are distinct from commercial software, due to scale, heterogeneity, and the need to integrate various programming and computational models with evolving and heterogeneous infrastructure. Pervasive and effective abstractions are thus critical. The process of developing abstractions for scientific applications and infrastructures is not well understood. While theory-based approaches are suited for well-defined, closed environments, they have severe limitations for designing abstractions for complex, real-world systems. The design science research (DSR) method provides the basis for designing effective systems that can handle real-world complexities. DSR consists of two complementary phases: design and evaluation. This paper applies the DSR method to the development of abstractions for scientific applications. Specifically, we address the critical problem of distributed resource management on heterogeneous infrastructure, a challenge that currently limits many scientific applications. We use the pilot-abstraction, a widely used resource management abstraction for high-performance, high throughput, big data, and streaming applications, as a case study. We evaluate the activities of the process and extensively evaluate the artifacts using different methods, including conceptual modeling, performance characterizations, and modeling. We demonstrate the applicability of the DSR method for holistically handling the complexity of parallel and distributed computing environments addressing important application, system, and infrastructure challenges of scientific applications. Finally, we capture our experiences and formulate different lessons learned. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09009">PDF</a>
<h3>No. 4	Distributed Mean Estimation with Optimal Error Bounds</h3><h4>Dan Alistarh, Saleh Ashkboos, Peter Davies</h4> Abstract: Motivated by applications to distributed optimization and machine learning, we consider the distributed mean estimation problem, in which $n$ nodes are each assigned a multi-dimensional input vector, and must cooperate to estimate the mean of the input vectors, while minimizing communication. In this paper, we provide the first tight bounds for this problem, in terms of the trade-off between the amount of communication between nodes and the variance of the node estimates relative to the true value of the mean. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09268">PDF</a>
<h3>No. 5	A polynomial lower bound on adaptive complexity of submodular  maximization</h3><h4>Wenzheng Li, Paul Liu, Jan Vondrak</h4> Abstract: In large-data applications, it is desirable to design algorithms with a high degree of parallelization. In the context of submodular optimization, adaptive complexity has become a widely-used measure of an algorithm's "sequentiality". Algorithms in the adaptive model proceed in rounds, and can issue polynomially many queries to a function $f$ in each round. The queries in each round must be independent, produced by a computation that depends only on query results obtained in previous rounds. In this work, we examine two fundamental variants of submodular maximization in the adaptive complexity model: cardinality-constrained monotone maximization, and unconstrained non-monotone maximization. Our main result is that an $r$-round algorithm for cardinality-constrained monotone maximization cannot achieve a factor better than $1 - 1/e - \Omega(\min \{ \frac{1}{r}, \frac{\log^2 n}{r^3} \})$, for any $r < n^c$ (where $c>0$ is some constant). This is the first result showing that the number of rounds must blow up polynomially large as we approach the optimal factor of $1-1/e$. For the unconstrained non-monotone maximization problem, we show a positive result: For every instance, and every $\delta>0$, either we obtain a $(1/2-\delta)$-approximation in $1$ round, or a $(1/2+\Omega(\delta^2))$-approximation in $O(1/\delta^2)$ rounds. In particular (in contrast to the cardinality-constrained case), there cannot be an instance where (i) it is impossible to achieve a factor better than $1/2$ regardless of the number of rounds, and (ii) it takes $r$ rounds to achieve a factor of $1/2-O(1/r)$. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09130">PDF</a>
<h3>No. 6	Asynchronous parallel adaptive stochastic gradient methods</h3><h4>Yangyang Xu, Colin Sutcher-Shepard, Yibo Xu, Jie Chen</h4> Abstract: Stochastic gradient methods (SGMs) are the predominant approaches to train deep learning models. The adaptive versions (e.g., Adam and AMSGrad) have been extensively used in practice, partly because they achieve faster convergence than the non-adaptive versions while incurring little overhead. On the other hand, asynchronous (async) parallel computing has exhibited much better speed-up over its synchronous (sync) counterpart. However, async-parallel implementation has only been demonstrated to the non-adaptive SGMs. The difficulty for adaptive SGMs originates from the second moment term that makes the convergence analysis challenging with async updates. In this paper, we propose an async-parallel adaptive SGM based on AMSGrad. We show that the proposed method inherits the convergence guarantee of AMSGrad for both convex and non-convex problems, if the staleness (also called delay) caused by asynchrony is bounded. Our convergence rate results indicate a nearly linear parallelization speed-up if $\tau=o(K^{\frac{1}{4}})$, where $\tau$ is the staleness and $K$ is the number of iterations. The proposed method is tested on both convex and non-convex machine learning problems, and the numerical results demonstrate its clear advantages over the sync counterpart. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.09095">PDF</a>
<h3>No. 7	Uncertainty Principle for Communication Compression in Distributed and  Federated Learning and the Search for an Optimal Compressor</h3><h4>Mher Safaryan, Egor Shulgin, Peter Richt√°rik</h4> Abstract: In order to mitigate the high communication cost in distributed and federated learning, various vector compression schemes, such as quantization, sparsification and dithering, have become very popular. In designing a compression method, one aims to communicate as few bits as possible, which minimizes the cost per communication round, while at the same time attempting to impart as little distortion (variance) to the communicated messages as possible, which minimizes the adverse effect of the compression on the overall number of communication rounds. However, intuitively, these two goals are fundamentally in conflict: the more compression we allow, the more distorted the messages become. We formalize this intuition and prove an {\em uncertainty principle} for randomized compression operators, thus quantifying this limitation mathematically, and {\em effectively providing lower bounds on what might be achievable with communication compression}. Motivated by these developments, we call for the search for the optimal compression operator. In an attempt to take a first step in this direction, we construct a new unbiased compression method inspired by the Kashin representation of vectors, which we call {\em Kashin compression (KC)}. In contrast to all previously proposed compression mechanisms, we prove that KC enjoys a {\em dimension independent} variance bound with an explicit formula even in the regime when only a few bits need to be communicate per each vector entry. We show how KC can be provably and efficiently combined with several existing optimization algorithms, in all cases leading to communication complexity improvements on previous state of the art. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08958">PDF</a><h2>2020-02-24</h2>
<h3>No. 1	Reliable Distributed Clustering with Redundant Data Assignment</h3><h4>Venkata Gandikota, Arya Mazumdar, Ankit Singh Rawat</h4> Abstract: In this paper, we present distributed generalized clustering algorithms that can handle large scale data across multiple machines in spite of straggling or unreliable machines. We propose a novel data assignment scheme that enables us to obtain global information about the entire data even when some machines fail to respond with the results of the assigned local computations. The assignment scheme leads to distributed algorithms with good approximation guarantees for a variety of clustering and dimensionality reduction problems. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08892">PDF</a>
<h3>No. 2	Knowledge and simplicial complexes</h3><h4>Hans van Ditmarsch, Eric Goubault, Jeremy Ledent, Sergio Rajsbaum</h4> Abstract: Simplicial complexes are a versatile and convenient paradigm on which to build all the tools and techniques of the logic of knowledge, on the assumption that initial epistemic models can be described in a distributed fashion. Thus, we can define: knowledge, belief, bisimulation, the group notions of mutual, distributed and common knowledge, and also dynamics in the shape of simplicial action models. We give a survey on how to interpret all such notions on simplicial complexes, building upon the foundations laid in prior work by Goubault and others. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08863">PDF</a>
<h3>No. 3	Two More Algorithms for Randomized Signature-Free Asynchronous Binary  Byzantine Consensus with $t < n/3$ and $O(n^2)$ Messages and $O(1)$ Round  Expected Termination</h3><h4>Tyler Crain</h4> Abstract: This work describes two randomized, asynchronous, round based, Binary Byzantine faulty tolerant consensus algorithms based on the algorithms of [25] and [26]. Like the algorithms of [25] and [26] they do not use signatures, use $O(n^2)$ messages per round (where each message is composed of a round number and a constant number of bits), tolerate up to one third failures, and have expected termination in constant number of rounds. The first, like [26], uses a weak common coin (i.e. one that can return different values at different processes with a constant probability) to ensure termination. The algorithm consists of $5$ to $7$ message broadcasts per round. An optimization is described that reduces this to $4$ to $5$ broadcasts per round for rounds following the first round. Comparatively, [26] consists of $8$ to $12$ message broadcasts per round. The second algorithm, like [25], uses a perfect common coin (i.e. one that returns the same value at all non-faulty processes) for both termination and correctness. Unlike [25], it does not require a fair scheduler to ensure termination. Furthermore, the algorithm consists of $2$ to $3$ message broadcasts for the first round and $1$ to $2$ broadcasts for the following rounds, while [29] consists of $2$ to $3$ broadcasts per round. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08765">PDF</a>
<h3>No. 4	SpArch: Efficient Architecture for Sparse Matrix Multiplication</h3><h4>Zhekai Zhang, Hanrui Wang, Song Han, William J. Dally</h4> Abstract: Generalized Sparse Matrix-Matrix Multiplication (SpGEMM) is a ubiquitous task in various engineering and scientific applications. However, inner product based SpGENN introduces redundant input fetches for mismatched nonzero operands, while outer product based approach suffers from poor output locality due to numerous partial product matrices. Inefficiency in the reuse of either inputs or outputs data leads to extensive and expensive DRAM access. To address this problem, this paper proposes an efficient sparse matrix multiplication accelerator architecture, SpArch, which jointly optimizes the data locality for both input and output matrices. We first design a highly parallelized streaming-based merger to pipeline the multiply and merge stage of partial matrices so that partial matrices are merged on chip immediately after produced. We then propose a condensed matrix representation that reduces the number of partial matrices by three orders of magnitude and thus reduces DRAM access by 5.4x. We further develop a Huffman tree scheduler to improve the scalability of the merger for larger sparse matrices, which reduces the DRAM access by another 1.8x. We also resolve the increased input matrix read induced by the new representation using a row prefetcher with near-optimal buffer replacement policy, further reducing the DRAM access by 1.5x. Evaluated on 20 benchmarks, SpArch reduces the total DRAM access by 2.8x over previous state-of-the-art. On average, SpArch achieves 4x, 19x, 18x, 17x, 1285x speedup and 6x, 164x, 435x, 307x, 62x energy savings over OuterSPACE, MKL, cuSPARSE, CUSP, and ARM Armadillo, respectively. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08947">PDF</a>
<h3>No. 5	Asymptotically Optimal Load Balancing in Large-scale Heterogeneous  Systems with Multiple Dispatchers</h3><h4>Xingyu Zhou, Ness Shroff, Adam Wierman</h4> Abstract: We consider the load balancing problem in large-scale heterogeneous systems with multiple dispatchers. We introduce a general framework called Local-Estimation-Driven (LED). Under this framework, each dispatcher keeps local (possibly outdated) estimates of queue lengths for all the servers, and the dispatching decision is made purely based on these local estimates. The local estimates are updated via infrequent communications between dispatchers and servers. We derive sufficient conditions for LED policies to achieve throughput optimality and delay optimality in heavy-traffic, respectively. These conditions directly imply delay optimality for many previous local-memory based policies in heavy traffic. Moreover, the results enable us to design new delay optimal policies for heterogeneous systems with multiple dispatchers. Finally, the heavy-traffic delay optimality of the LED framework directly resolves a recent open problem on how to design optimal load balancing schemes using delayed information. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08908">PDF</a>
<h3>No. 6	Spatio-Temporal Coverage Enhancement in Drive-By Sensing Through  Utility-Aware Mobile Agent Selection</h3><h4>Navid Hashemi Tonekaboni, Lakshmish Ramaswamy, Deepak Mishra, Sorush Omidvar</h4> Abstract: In recent years, the drive-by sensing paradigm has become increasingly popular for cost-effective monitoring of urban areas. Drive-by sensing is a form of crowdsensing wherein sensor-equipped vehicles (aka, mobile agents) are the primary data gathering agents. Enhancing the efficacy of drive-by sensing poses many challenges, an important one of which is to select non-dedicated mobile agents on which a limited number of sensors are to be mounted. This problem, which we refer to as the mobile-agent selection problem, has a significant impact on the spatio-temporal coverage of the drive-by sensing platforms and the resultant datasets. The challenge here is to achieve maximum spatiotemporal coverage while taking the relative importance levels of geographical areas into account. In this paper, we address this problem in the context of the SCOUTS project, the goal of which is to map and analyze the urban heat island phenomenon accurately. Our work makes several major technical contributions. First, we delineate a model for representing the mobile agents selection problem. This model takes into account the trajectories of the vehicles (public transportation buses in our case) and the relative importance of the urban regions, and formulates it as an optimization problem. Second, we provide two algorithms that are based upon the utility (coverage) values of mobile agents, namely, a hotspot-based algorithm that limits the search space to important sub-regions and a utility-aware genetic algorithm that enables the latter algorithm to make unbiased selections. Third, we design a highly efficient coverage redundancy minimization algorithm that, at each step, chooses the mobile agent, which provides maximal improvement to the spatio-temporal coverage. This paper reports a series of experiments on a real-world dataset from Athens, GA, USA, to demonstrate the effectiveness of the proposed approaches. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08886">PDF</a>
<h3>No. 7	PrivacyFL: A simulator for privacy-preserving and secure federated  learning</h3><h4>Vaikkunth Mugunthan, Anton Peraire-Bueno, Lalana Kagal</h4> Abstract: Federated learning is a technique that enables distributed clients to collaboratively learn a shared machine learning model while keeping their training data localized. This reduces data privacy risks, however, privacy concerns still exist since it is possible to leak information about the training dataset from the trained model's weights or parameters. Setting up a federated learning environment, especially with security and privacy guarantees, is a time-consuming process with numerous configurations and parameters that can be manipulated. In order to help clients ensure that collaboration is feasible and to check that it improves their model accuracy, a real-world simulator for privacy-preserving and secure federated learning is required. In this paper, we introduce PrivacyFL, which is an extensible, easily configurable and scalable simulator for federated learning environments. Its key features include latency simulation, robustness to client departure, support for both centralized and decentralized learning, and configurable privacy and security mechanisms based on differential privacy and secure multiparty computation. In this paper, we motivate our research, describe the architecture of the simulator and associated protocols, and discuss its evaluation in numerous scenarios that highlight its wide range of functionality and its advantages. Our paper addresses a significant real-world problem: checking the feasibility of participating in a federated learning environment under a variety of circumstances. It also has a strong practical impact because organizations such as hospitals, banks, and research institutes, which have large amounts of sensitive data and would like to collaborate, would greatly benefit from having a system that enables them to do so in a privacy-preserving and secure manner. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08423">PDF</a><h2>2020-02-23</h2>
<h3>No. 1	Reliable Distributed Clustering with Redundant Data Assignment</h3><h4>Venkata Gandikota, Arya Mazumdar, Ankit Singh Rawat</h4> Abstract: In this paper, we present distributed generalized clustering algorithms that can handle large scale data across multiple machines in spite of straggling or unreliable machines. We propose a novel data assignment scheme that enables us to obtain global information about the entire data even when some machines fail to respond with the results of the assigned local computations. The assignment scheme leads to distributed algorithms with good approximation guarantees for a variety of clustering and dimensionality reduction problems. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08892">PDF</a>
<h3>No. 2	Knowledge and simplicial complexes</h3><h4>Hans van Ditmarsch, Eric Goubault, Jeremy Ledent, Sergio Rajsbaum</h4> Abstract: Simplicial complexes are a versatile and convenient paradigm on which to build all the tools and techniques of the logic of knowledge, on the assumption that initial epistemic models can be described in a distributed fashion. Thus, we can define: knowledge, belief, bisimulation, the group notions of mutual, distributed and common knowledge, and also dynamics in the shape of simplicial action models. We give a survey on how to interpret all such notions on simplicial complexes, building upon the foundations laid in prior work by Goubault and others. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08863">PDF</a>
<h3>No. 3	Two More Algorithms for Randomized Signature-Free Asynchronous Binary  Byzantine Consensus with $t < n/3$ and $O(n^2)$ Messages and $O(1)$ Round  Expected Termination</h3><h4>Tyler Crain</h4> Abstract: This work describes two randomized, asynchronous, round based, Binary Byzantine faulty tolerant consensus algorithms based on the algorithms of [25] and [26]. Like the algorithms of [25] and [26] they do not use signatures, use $O(n^2)$ messages per round (where each message is composed of a round number and a constant number of bits), tolerate up to one third failures, and have expected termination in constant number of rounds. The first, like [26], uses a weak common coin (i.e. one that can return different values at different processes with a constant probability) to ensure termination. The algorithm consists of $5$ to $7$ message broadcasts per round. An optimization is described that reduces this to $4$ to $5$ broadcasts per round for rounds following the first round. Comparatively, [26] consists of $8$ to $12$ message broadcasts per round. The second algorithm, like [25], uses a perfect common coin (i.e. one that returns the same value at all non-faulty processes) for both termination and correctness. Unlike [25], it does not require a fair scheduler to ensure termination. Furthermore, the algorithm consists of $2$ to $3$ message broadcasts for the first round and $1$ to $2$ broadcasts for the following rounds, while [29] consists of $2$ to $3$ broadcasts per round. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08765">PDF</a>
<h3>No. 4	SpArch: Efficient Architecture for Sparse Matrix Multiplication</h3><h4>Zhekai Zhang, Hanrui Wang, Song Han, William J. Dally</h4> Abstract: Generalized Sparse Matrix-Matrix Multiplication (SpGEMM) is a ubiquitous task in various engineering and scientific applications. However, inner product based SpGENN introduces redundant input fetches for mismatched nonzero operands, while outer product based approach suffers from poor output locality due to numerous partial product matrices. Inefficiency in the reuse of either inputs or outputs data leads to extensive and expensive DRAM access. To address this problem, this paper proposes an efficient sparse matrix multiplication accelerator architecture, SpArch, which jointly optimizes the data locality for both input and output matrices. We first design a highly parallelized streaming-based merger to pipeline the multiply and merge stage of partial matrices so that partial matrices are merged on chip immediately after produced. We then propose a condensed matrix representation that reduces the number of partial matrices by three orders of magnitude and thus reduces DRAM access by 5.4x. We further develop a Huffman tree scheduler to improve the scalability of the merger for larger sparse matrices, which reduces the DRAM access by another 1.8x. We also resolve the increased input matrix read induced by the new representation using a row prefetcher with near-optimal buffer replacement policy, further reducing the DRAM access by 1.5x. Evaluated on 20 benchmarks, SpArch reduces the total DRAM access by 2.8x over previous state-of-the-art. On average, SpArch achieves 4x, 19x, 18x, 17x, 1285x speedup and 6x, 164x, 435x, 307x, 62x energy savings over OuterSPACE, MKL, cuSPARSE, CUSP, and ARM Armadillo, respectively. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08947">PDF</a>
<h3>No. 5	Asymptotically Optimal Load Balancing in Large-scale Heterogeneous  Systems with Multiple Dispatchers</h3><h4>Xingyu Zhou, Ness Shroff, Adam Wierman</h4> Abstract: We consider the load balancing problem in large-scale heterogeneous systems with multiple dispatchers. We introduce a general framework called Local-Estimation-Driven (LED). Under this framework, each dispatcher keeps local (possibly outdated) estimates of queue lengths for all the servers, and the dispatching decision is made purely based on these local estimates. The local estimates are updated via infrequent communications between dispatchers and servers. We derive sufficient conditions for LED policies to achieve throughput optimality and delay optimality in heavy-traffic, respectively. These conditions directly imply delay optimality for many previous local-memory based policies in heavy traffic. Moreover, the results enable us to design new delay optimal policies for heterogeneous systems with multiple dispatchers. Finally, the heavy-traffic delay optimality of the LED framework directly resolves a recent open problem on how to design optimal load balancing schemes using delayed information. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08908">PDF</a>
<h3>No. 6	Spatio-Temporal Coverage Enhancement in Drive-By Sensing Through  Utility-Aware Mobile Agent Selection</h3><h4>Navid Hashemi Tonekaboni, Lakshmish Ramaswamy, Deepak Mishra, Sorush Omidvar</h4> Abstract: In recent years, the drive-by sensing paradigm has become increasingly popular for cost-effective monitoring of urban areas. Drive-by sensing is a form of crowdsensing wherein sensor-equipped vehicles (aka, mobile agents) are the primary data gathering agents. Enhancing the efficacy of drive-by sensing poses many challenges, an important one of which is to select non-dedicated mobile agents on which a limited number of sensors are to be mounted. This problem, which we refer to as the mobile-agent selection problem, has a significant impact on the spatio-temporal coverage of the drive-by sensing platforms and the resultant datasets. The challenge here is to achieve maximum spatiotemporal coverage while taking the relative importance levels of geographical areas into account. In this paper, we address this problem in the context of the SCOUTS project, the goal of which is to map and analyze the urban heat island phenomenon accurately. Our work makes several major technical contributions. First, we delineate a model for representing the mobile agents selection problem. This model takes into account the trajectories of the vehicles (public transportation buses in our case) and the relative importance of the urban regions, and formulates it as an optimization problem. Second, we provide two algorithms that are based upon the utility (coverage) values of mobile agents, namely, a hotspot-based algorithm that limits the search space to important sub-regions and a utility-aware genetic algorithm that enables the latter algorithm to make unbiased selections. Third, we design a highly efficient coverage redundancy minimization algorithm that, at each step, chooses the mobile agent, which provides maximal improvement to the spatio-temporal coverage. This paper reports a series of experiments on a real-world dataset from Athens, GA, USA, to demonstrate the effectiveness of the proposed approaches. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08886">PDF</a>
<h3>No. 7	PrivacyFL: A simulator for privacy-preserving and secure federated  learning</h3><h4>Vaikkunth Mugunthan, Anton Peraire-Bueno, Lalana Kagal</h4> Abstract: Federated learning is a technique that enables distributed clients to collaboratively learn a shared machine learning model while keeping their training data localized. This reduces data privacy risks, however, privacy concerns still exist since it is possible to leak information about the training dataset from the trained model's weights or parameters. Setting up a federated learning environment, especially with security and privacy guarantees, is a time-consuming process with numerous configurations and parameters that can be manipulated. In order to help clients ensure that collaboration is feasible and to check that it improves their model accuracy, a real-world simulator for privacy-preserving and secure federated learning is required. In this paper, we introduce PrivacyFL, which is an extensible, easily configurable and scalable simulator for federated learning environments. Its key features include latency simulation, robustness to client departure, support for both centralized and decentralized learning, and configurable privacy and security mechanisms based on differential privacy and secure multiparty computation. In this paper, we motivate our research, describe the architecture of the simulator and associated protocols, and discuss its evaluation in numerous scenarios that highlight its wide range of functionality and its advantages. Our paper addresses a significant real-world problem: checking the feasibility of participating in a federated learning environment under a variety of circumstances. It also has a strong practical impact because organizations such as hospitals, banks, and research institutes, which have large amounts of sensitive data and would like to collaborate, would greatly benefit from having a system that enables them to do so in a privacy-preserving and secure manner. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08423">PDF</a><h2>2020-02-22</h2>
<h3>No. 1	Reliable Distributed Clustering with Redundant Data Assignment</h3><h4>Venkata Gandikota, Arya Mazumdar, Ankit Singh Rawat</h4> Abstract: In this paper, we present distributed generalized clustering algorithms that can handle large scale data across multiple machines in spite of straggling or unreliable machines. We propose a novel data assignment scheme that enables us to obtain global information about the entire data even when some machines fail to respond with the results of the assigned local computations. The assignment scheme leads to distributed algorithms with good approximation guarantees for a variety of clustering and dimensionality reduction problems. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08892">PDF</a>
<h3>No. 2	Knowledge and simplicial complexes</h3><h4>Hans van Ditmarsch, Eric Goubault, Jeremy Ledent, Sergio Rajsbaum</h4> Abstract: Simplicial complexes are a versatile and convenient paradigm on which to build all the tools and techniques of the logic of knowledge, on the assumption that initial epistemic models can be described in a distributed fashion. Thus, we can define: knowledge, belief, bisimulation, the group notions of mutual, distributed and common knowledge, and also dynamics in the shape of simplicial action models. We give a survey on how to interpret all such notions on simplicial complexes, building upon the foundations laid in prior work by Goubault and others. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08863">PDF</a>
<h3>No. 3	Two More Algorithms for Randomized Signature-Free Asynchronous Binary  Byzantine Consensus with $t < n/3$ and $O(n^2)$ Messages and $O(1)$ Round  Expected Termination</h3><h4>Tyler Crain</h4> Abstract: This work describes two randomized, asynchronous, round based, Binary Byzantine faulty tolerant consensus algorithms based on the algorithms of [25] and [26]. Like the algorithms of [25] and [26] they do not use signatures, use $O(n^2)$ messages per round (where each message is composed of a round number and a constant number of bits), tolerate up to one third failures, and have expected termination in constant number of rounds. The first, like [26], uses a weak common coin (i.e. one that can return different values at different processes with a constant probability) to ensure termination. The algorithm consists of $5$ to $7$ message broadcasts per round. An optimization is described that reduces this to $4$ to $5$ broadcasts per round for rounds following the first round. Comparatively, [26] consists of $8$ to $12$ message broadcasts per round. The second algorithm, like [25], uses a perfect common coin (i.e. one that returns the same value at all non-faulty processes) for both termination and correctness. Unlike [25], it does not require a fair scheduler to ensure termination. Furthermore, the algorithm consists of $2$ to $3$ message broadcasts for the first round and $1$ to $2$ broadcasts for the following rounds, while [29] consists of $2$ to $3$ broadcasts per round. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08765">PDF</a>
<h3>No. 4	SpArch: Efficient Architecture for Sparse Matrix Multiplication</h3><h4>Zhekai Zhang, Hanrui Wang, Song Han, William J. Dally</h4> Abstract: Generalized Sparse Matrix-Matrix Multiplication (SpGEMM) is a ubiquitous task in various engineering and scientific applications. However, inner product based SpGENN introduces redundant input fetches for mismatched nonzero operands, while outer product based approach suffers from poor output locality due to numerous partial product matrices. Inefficiency in the reuse of either inputs or outputs data leads to extensive and expensive DRAM access. To address this problem, this paper proposes an efficient sparse matrix multiplication accelerator architecture, SpArch, which jointly optimizes the data locality for both input and output matrices. We first design a highly parallelized streaming-based merger to pipeline the multiply and merge stage of partial matrices so that partial matrices are merged on chip immediately after produced. We then propose a condensed matrix representation that reduces the number of partial matrices by three orders of magnitude and thus reduces DRAM access by 5.4x. We further develop a Huffman tree scheduler to improve the scalability of the merger for larger sparse matrices, which reduces the DRAM access by another 1.8x. We also resolve the increased input matrix read induced by the new representation using a row prefetcher with near-optimal buffer replacement policy, further reducing the DRAM access by 1.5x. Evaluated on 20 benchmarks, SpArch reduces the total DRAM access by 2.8x over previous state-of-the-art. On average, SpArch achieves 4x, 19x, 18x, 17x, 1285x speedup and 6x, 164x, 435x, 307x, 62x energy savings over OuterSPACE, MKL, cuSPARSE, CUSP, and ARM Armadillo, respectively. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08947">PDF</a>
<h3>No. 5	Asymptotically Optimal Load Balancing in Large-scale Heterogeneous  Systems with Multiple Dispatchers</h3><h4>Xingyu Zhou, Ness Shroff, Adam Wierman</h4> Abstract: We consider the load balancing problem in large-scale heterogeneous systems with multiple dispatchers. We introduce a general framework called Local-Estimation-Driven (LED). Under this framework, each dispatcher keeps local (possibly outdated) estimates of queue lengths for all the servers, and the dispatching decision is made purely based on these local estimates. The local estimates are updated via infrequent communications between dispatchers and servers. We derive sufficient conditions for LED policies to achieve throughput optimality and delay optimality in heavy-traffic, respectively. These conditions directly imply delay optimality for many previous local-memory based policies in heavy traffic. Moreover, the results enable us to design new delay optimal policies for heterogeneous systems with multiple dispatchers. Finally, the heavy-traffic delay optimality of the LED framework directly resolves a recent open problem on how to design optimal load balancing schemes using delayed information. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08908">PDF</a>
<h3>No. 6	Spatio-Temporal Coverage Enhancement in Drive-By Sensing Through  Utility-Aware Mobile Agent Selection</h3><h4>Navid Hashemi Tonekaboni, Lakshmish Ramaswamy, Deepak Mishra, Sorush Omidvar</h4> Abstract: In recent years, the drive-by sensing paradigm has become increasingly popular for cost-effective monitoring of urban areas. Drive-by sensing is a form of crowdsensing wherein sensor-equipped vehicles (aka, mobile agents) are the primary data gathering agents. Enhancing the efficacy of drive-by sensing poses many challenges, an important one of which is to select non-dedicated mobile agents on which a limited number of sensors are to be mounted. This problem, which we refer to as the mobile-agent selection problem, has a significant impact on the spatio-temporal coverage of the drive-by sensing platforms and the resultant datasets. The challenge here is to achieve maximum spatiotemporal coverage while taking the relative importance levels of geographical areas into account. In this paper, we address this problem in the context of the SCOUTS project, the goal of which is to map and analyze the urban heat island phenomenon accurately. Our work makes several major technical contributions. First, we delineate a model for representing the mobile agents selection problem. This model takes into account the trajectories of the vehicles (public transportation buses in our case) and the relative importance of the urban regions, and formulates it as an optimization problem. Second, we provide two algorithms that are based upon the utility (coverage) values of mobile agents, namely, a hotspot-based algorithm that limits the search space to important sub-regions and a utility-aware genetic algorithm that enables the latter algorithm to make unbiased selections. Third, we design a highly efficient coverage redundancy minimization algorithm that, at each step, chooses the mobile agent, which provides maximal improvement to the spatio-temporal coverage. This paper reports a series of experiments on a real-world dataset from Athens, GA, USA, to demonstrate the effectiveness of the proposed approaches. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08886">PDF</a>
<h3>No. 7	PrivacyFL: A simulator for privacy-preserving and secure federated  learning</h3><h4>Vaikkunth Mugunthan, Anton Peraire-Bueno, Lalana Kagal</h4> Abstract: Federated learning is a technique that enables distributed clients to collaboratively learn a shared machine learning model while keeping their training data localized. This reduces data privacy risks, however, privacy concerns still exist since it is possible to leak information about the training dataset from the trained model's weights or parameters. Setting up a federated learning environment, especially with security and privacy guarantees, is a time-consuming process with numerous configurations and parameters that can be manipulated. In order to help clients ensure that collaboration is feasible and to check that it improves their model accuracy, a real-world simulator for privacy-preserving and secure federated learning is required. In this paper, we introduce PrivacyFL, which is an extensible, easily configurable and scalable simulator for federated learning environments. Its key features include latency simulation, robustness to client departure, support for both centralized and decentralized learning, and configurable privacy and security mechanisms based on differential privacy and secure multiparty computation. In this paper, we motivate our research, describe the architecture of the simulator and associated protocols, and discuss its evaluation in numerous scenarios that highlight its wide range of functionality and its advantages. Our paper addresses a significant real-world problem: checking the feasibility of participating in a federated learning environment under a variety of circumstances. It also has a strong practical impact because organizations such as hospitals, banks, and research institutes, which have large amounts of sensitive data and would like to collaborate, would greatly benefit from having a system that enables them to do so in a privacy-preserving and secure manner. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08423">PDF</a><h2>2020-02-21</h2>
<h3>No. 1	Balancing Efficiency and Flexibility for DNN Acceleration via Temporal  GPU-Systolic Array Integration</h3><h4>Cong Guo, Yangjie Zhou, Jingwen Leng, Yuhao Zhu, Zidong Du, Quan Chen, Chao Li, Minyi Guo, Bin Yao</h4> Abstract: The research interest in specialized hardware accelerators for deep neural networks (DNN) spiked recently owing to their superior performance and efficiency. However, today's DNN accelerators primarily focus on accelerating specific "kernels" such as convolution and matrix multiplication, which are vital but only part of an end-to-end DNN-enabled application. Meaningful speedups over the entire application often require supporting computations that are, while massively parallel, ill-suited to DNN accelerators. Integrating a general-purpose processor such as a CPU or a GPU incurs significant data movement overhead and leads to resource under-utilization on the DNN accelerators. We propose Simultaneous Multi-mode Architecture (SMA), a novel architecture design and execution model that offers general-purpose programmability on DNN accelerators in order to accelerate end-to-end applications. The key to SMA is the temporal integration of the systolic execution model with the GPU-like SIMD execution model. The SMA exploits the common components shared between the systolic-array accelerator and the GPU, and provides lightweight reconfiguration capability to switch between the two modes in-situ. The SMA achieves up to 63% performance improvement while consuming 23% less energy than the baseline Volta architecture with TensorCore. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08326">PDF</a>
<h3>No. 2	MLModelScope: A Distributed Platform for Model Evaluation and  Benchmarking at Scale</h3><h4>Abdul Dakkak, Cheng Li, Jinjun Xiong, Wen-mei Hwu</h4> Abstract: Machine Learning (ML) and Deep Learning (DL) innovations are being introduced at such a rapid pace that researchers are hard-pressed to analyze and study them. The complicated procedures for evaluating innovations, along with the lack of standard and efficient ways of specifying and provisioning ML/DL evaluation, is a major "pain point" for the community. This paper proposes MLModelScope, an open-source, framework/hardware agnostic, extensible and customizable design that enables repeatable, fair, and scalable model evaluation and benchmarking. We implement the distributed design with support for all major frameworks and hardware, and equip it with web, command-line, and library interfaces. To demonstrate MLModelScope's capabilities we perform parallel evaluation and show how subtle changes to model evaluation pipeline affects the accuracy and HW/SW stack choices affect performance. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08295">PDF</a>
<h3>No. 3	Truly Tight-in-$Œî$ Bounds for Bipartite Maximal Matching and  Variants</h3><h4>Sebastian Brandt, Dennis Olivetti</h4> Abstract: In a recent breakthrough result, Balliu et al. [FOCS'19] proved a deterministic $\Omega(\min(\Delta,\log n /\log \log n))$-round and a randomized $\Omega(\min(\Delta,\log \log n/\log \log \log n))$-round lower bound for the complexity of the bipartite maximal matching problem on $n$-node graphs in the LOCAL model of distributed computing. Both lower bounds are asymptotically tight as a function of the maximum degree $\Delta$. We provide truly tight bounds in $\Delta$ for the complexity of bipartite maximal matching and many natural variants, up to and including the additive constant. As a by-product, our results yield a considerably simplified version of the proof by Balliu et al. We show that our results can be obtained via bounded automatic round elimination, a version of the recent automatic round elimination technique by Brandt [PODC'19] that is particularly suited for automatization from a practical perspective. In this context, our work can be seen as another step towards the automatization of lower bounds in the LOCAL model. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08216">PDF</a>
<h3>No. 4	Honing and proofing Astrophysical codes on the road to Exascale.  Experiences from code modernization on many-core systems</h3><h4>Salvatore Cielo, Luigi Iapichino, Fabio Baruffa, Matteo Bugli, Christoph Federrath</h4> Abstract: The complexity of modern and upcoming computing architectures poses severe challenges for code developers and application specialists, and forces them to expose the highest possible degree of parallelism, in order to make the best use of the available hardware. The Intel$^{(R)}$ Xeon Phi$^{(TM)}$ of second generation (code-named Knights Landing, henceforth KNL) is the latest many-core system, which implements several interesting hardware features like for example a large number of cores per node (up to 72), the 512 bits-wide vector registers and the high-bandwidth memory. The unique features of KNL make this platform a powerful testbed for modern HPC applications. The performance of codes on KNL is therefore a useful proxy of their readiness for future architectures. In this work we describe the lessons learnt during the optimisation of the widely used codes for computational astrophysics P-Gadget-3, Flash and Echo. Moreover, we present results for the visualisation and analysis tools VisIt and yt. These examples show that modern architectures benefit from code optimisation at different levels, even more than traditional multi-core systems. However, the level of modernisation of typical community codes still needs improvements, for them to fully utilise resources of novel architectures. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08161">PDF</a>
<h3>No. 5	The Sum of Its Parts: Analysis of Federated Byzantine Agreement Systems</h3><h4>Martin Florian, Sebastian Henningsen, Bj√∂rn Scheuermann</h4> Abstract: Federated Byzantine Agreement Systems (FBASs) are a fascinating new paradigm in the context of consensus protocols. Originally proposed for powering the Stellar payment network, FBASs can be thought of as a middle way between typical permissionless systems (like Bitcoin) and permissioned approaches for solving consensus (like classical BFT protocols). Unlike Bitcoin and the like, validators must be explicitly chosen by peers. Unlike permissioned protocols, there is no need for the whole system to agree on the same set of validators. Instead, every node is free to decide for itself with whom it requires agreement. In this paper, we propose an intuitive yet precise methodology for determining whether the quorum systems resulting from such individual configurations can enable liveness and safety, respectively how many (byzantine) node failures they are away from losing these qualities. We apply our analysis approach and software to evaluate the effects of different node configuration policies, i.e., logics through which node configurations result from strategic considerations or an existing inter-node relationship graph. Lastly, we also investigate the reported "open-membership" property of FBASs. We observe that an often small group of nodes is exclusively relevant for determining safety and liveness "buffers", and prove that these top tiers are effectively "closed-membership" if maintaining safety is a core requirement. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08101">PDF</a>
<h3>No. 6	Holistic Slowdown Driven Scheduling and Resource Management for  Malleable Jobs</h3><h4>Marco D'Amico, Ana Jokanovic, Julita Corbalan</h4> Abstract: In job scheduling, the concept of malleability has been explored since many years ago. Research shows that malleability improves system performance, but its utilization in HPC never became widespread. The causes are the difficulty in developing malleable applications, and the lack of support and integration of the different layers of the HPC software stack. However, in the last years, malleability in job scheduling is becoming more critical because of the increasing complexity of hardware and workloads. In this context, using nodes in an exclusive mode is not always the most efficient solution as in traditional HPC jobs, where applications were highly tuned for static allocations, but offering zero flexibility to dynamic executions. This paper proposes a new holistic, dynamic job scheduling policy, Slowdown Driven (SD-Policy), which exploits the malleability of applications as the key technology to reduce the average slowdown and response time of jobs. SD-Policy is based on backfill and node sharing. It applies malleability to running jobs to make room for jobs that will run with a reduced set of resources, only when the estimated slowdown improves over the static approach. We implemented SD-Policy in SLURM and evaluated it in a real production environment, and with a simulator using workloads of up to 198K jobs. Results show better resource utilization with the reduction of makespan, response time, slowdown, and energy consumption, up to respectively 7%, 50%, 70%, and 6%, for the evaluated workloads. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08088">PDF</a>
<h3>No. 7	Supporting OpenMP 5.0 Tasks in hpxMP -- A study of an OpenMP  implementation within Task Based Runtime Systems</h3><h4>Tianyi Zhang, Shahrzad Shirzad, Bibek Wagle, Adrian S. Lemoine, Patrick Diehl, Hartmut Kaiser</h4> Abstract: OpenMP has been the de facto standard for single node parallelism for more than a decade. Recently, asynchronous many-task runtime (AMT) systems have increased in popularity as a new programming paradigm for high performance computing applications. One of the major challenges of this new paradigm is the incompatibility of the OpenMP thread model and other AMTs. Highly optimized OpenMP-based libraries do not perform well when coupled with AMTs because the threading of both libraries will compete for resources. This paper is a follow-up paper on the fundamental implementation of hpxMP, an implementation of the OpenMP standard which utilizes the C++ standard library for Parallelism and Concurrency (HPX) to schedule and manage tasks. In this paper, we present the implementation of task features, e.g. taskgroup, task depend, and task_reduction, of the OpenMP 5.0 standard and optimization of the #pragma omp parallel for pragma. We use the daxpy benchmark, the Barcelona OpenMP Tasks Suite, Parallel research kernels, and OpenBLAS benchmarks to compare the different OpenMp implementations: hpxMP, llvm-OpenMP, and GOMP. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.07970">PDF</a>
<h3>No. 8	Parallel Algorithms for Small Subgraph Counting</h3><h4>Amartya Shankha Biswas, Talya Eden, Quanquan C. Liu, Slobodan Mitroviƒá, Ronitt Rubinfeld</h4> Abstract: Subgraph counting is a fundamental problem in analyzing massive graphs, often studied in the context of social and complex networks. There is a rich literature on designing efficient, accurate, and scalable algorithms for this problem. In this work, we tackle this challenge and design several new algorithms for subgraph counting in the Massively Parallel Computation (MPC) model: Given a graph $G$ over $n$ vertices, $m$ edges and $T$ triangles, our first main result is an algorithm that, with high probability, outputs a $(1+\varepsilon)$-approximation to $T$, with optimal round and space complexity provided any $S \geq \max{(\sqrt m, n^2/m)}$ space per machine, assuming $T=\Omega(\sqrt{m/n})$. Our second main result is an $\tilde{O}_{\delta}(\log \log n)$-rounds algorithm for exactly counting the number of triangles, parametrized by the arboricity $\alpha$ of the input graph. The space per machine is $O(n^{\delta})$ for any constant $\delta$, and the total space is $O(m\alpha)$, which matches the time complexity of (combinatorial) triangle counting in the sequential model. We also prove that this result can be extended to exactly counting $k$-cliques for any constant $k$, with the same round complexity and total space $O(m\alpha^{k-2})$. Alternatively, allowing $O(\alpha^2)$ space per machine, the total space requirement reduces to $O(n\alpha^2)$. Finally, we prove that a recent result of Bera, Pashanasangi and Seshadhri (ITCS 2020) for exactly counting all subgraphs of size at most $5$, can be implemented in the MPC model in $\tilde{O}_{\delta}(\sqrt{\log n})$ rounds, $O(n^{\delta})$ space per machine and $O(m\alpha^3)$ total space. Therefore, this result also exhibits the phenomenon that a time bound in the sequential model translates to a space bound in the MPC model. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08299">PDF</a>
<h3>No. 9	BatchLayout: A Batch-Parallel Force-Directed Graph Layout Algorithm in  Shared Memory</h3><h4>Md. Khaledur Rahman, Majedul Haque Sujon, Ariful Azad</h4> Abstract: Force-directed algorithms are widely used to generate aesthetically pleasing layouts of graphs or networks arisen in many scientific disciplines. To visualize large-scale graphs, several parallel algorithms have been discussed in the literature. However, existing parallel algorithms do not utilize memory hierarchy efficiently and often offer limited parallelism. This paper addresses these limitations with BatchLayout, an algorithm that groups vertices into minibatches and processes them in parallel. BatchLayout also employs cache blocking techniques to utilize memory hierarchy efficiently. More parallelism and improved memory accesses coupled with force approximating techniques, better initialization, and optimized learning rate make BatchLayout significantly faster than other state-of-the-art algorithms such as ForceAtlas2 and OpenOrd. The visualization quality of layouts from BatchLayout is comparable or better than similar visualization tools. All of our source code, links to datasets, results and log files are available at this https URL <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08233">PDF</a>
<h3>No. 10	Toward Low-Cost and Stable Blockchain Networks</h3><h4>Minghong Fang, Jia Liu</h4> Abstract: Envisioned to be the future of distributed systems, blockchain networks have received increasing attentions from both industry and academic research in recent years. However, the blockchain mining process consumes vast amounts of energy, and studies have shown that the amount of energy consumed in Bitcoin mining is almost the same as electricity used in Ireland. To address the high mining energy cost problem of blockchain networks, in this paper, we propose a blockchain mining resources allocation algorithm to reduce the mining cost in PoW-based (proof-of-work-based) blockchain networks. We first provide a systematic study on general blockchain queueing model. In our queueing model, transactions arrive randomly to the queue and served in a batch manner with unknown probability distribution and agnostic to any priority mechanism. Then, we leverage Lyapunov optimization techniques to propose a dynamic mining resources allocation algorithm (DMRA), which is parameterized by a tuning parameter $K>0$. We show that our algorithm achieves performance-delay tradeoff as $[O(1/K), O(K)]$. The simulation results also demonstrate the effectiveness of DMRA in reducing the mining cost. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08027">PDF</a>
<h3>No. 11	Repair rate lower bounds for distributed storage</h3><h4>Michael Luby</h4> Abstract: One of the primary objectives of a distributed storage system is to reliably store a large amount $dsize$ of source data for a long duration using a large number $N$ of unreliable storage nodes, each with capacity $nsize$. The storage overhead $\beta$ is the fraction of system capacity available beyond $dsize$, i.e., $\beta = 1- \frac{dsize}{N \cdot nsize}$. Storage nodes fail randomly over time and are replaced with initially empty nodes, and thus data is erased from the system at an average rate $erate = \lambda \cdot N \cdot nsize$, where $1/\lambda$ is the average lifetime of a node before failure. To maintain recoverability of the source data, a repairer continually reads data over a network from nodes at some average rate $rrate$, and generates and writes data to nodes based on the read data. The main result is that, for any repairer, if the source data is recoverable at each point in time then it must be the case that $rrate \ge \frac{erate}{2 \cdot \beta}$ asymptotically as $N$ goes to infinity and beta goes to zero. This inequality provides a fundamental lower bound on the average rate that any repairer needs to read data from the system in order to maintain recoverability of the source data. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.07904">PDF</a><h2>2020-02-20</h2>
<h3>No. 1	Balancing Efficiency and Flexibility for DNN Acceleration via Temporal  GPU-Systolic Array Integration</h3><h4>Cong Guo, Yangjie Zhou, Jingwen Leng, Yuhao Zhu, Zidong Du, Quan Chen, Chao Li, Minyi Guo, Bin Yao</h4> Abstract: The research interest in specialized hardware accelerators for deep neural networks (DNN) spiked recently owing to their superior performance and efficiency. However, today's DNN accelerators primarily focus on accelerating specific "kernels" such as convolution and matrix multiplication, which are vital but only part of an end-to-end DNN-enabled application. Meaningful speedups over the entire application often require supporting computations that are, while massively parallel, ill-suited to DNN accelerators. Integrating a general-purpose processor such as a CPU or a GPU incurs significant data movement overhead and leads to resource under-utilization on the DNN accelerators. We propose Simultaneous Multi-mode Architecture (SMA), a novel architecture design and execution model that offers general-purpose programmability on DNN accelerators in order to accelerate end-to-end applications. The key to SMA is the temporal integration of the systolic execution model with the GPU-like SIMD execution model. The SMA exploits the common components shared between the systolic-array accelerator and the GPU, and provides lightweight reconfiguration capability to switch between the two modes in-situ. The SMA achieves up to 63% performance improvement while consuming 23% less energy than the baseline Volta architecture with TensorCore. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08326">PDF</a>
<h3>No. 2	MLModelScope: A Distributed Platform for Model Evaluation and  Benchmarking at Scale</h3><h4>Abdul Dakkak, Cheng Li, Jinjun Xiong, Wen-mei Hwu</h4> Abstract: Machine Learning (ML) and Deep Learning (DL) innovations are being introduced at such a rapid pace that researchers are hard-pressed to analyze and study them. The complicated procedures for evaluating innovations, along with the lack of standard and efficient ways of specifying and provisioning ML/DL evaluation, is a major "pain point" for the community. This paper proposes MLModelScope, an open-source, framework/hardware agnostic, extensible and customizable design that enables repeatable, fair, and scalable model evaluation and benchmarking. We implement the distributed design with support for all major frameworks and hardware, and equip it with web, command-line, and library interfaces. To demonstrate MLModelScope's capabilities we perform parallel evaluation and show how subtle changes to model evaluation pipeline affects the accuracy and HW/SW stack choices affect performance. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08295">PDF</a>
<h3>No. 3	Truly Tight-in-$Œî$ Bounds for Bipartite Maximal Matching and  Variants</h3><h4>Sebastian Brandt, Dennis Olivetti</h4> Abstract: In a recent breakthrough result, Balliu et al. [FOCS'19] proved a deterministic $\Omega(\min(\Delta,\log n /\log \log n))$-round and a randomized $\Omega(\min(\Delta,\log \log n/\log \log \log n))$-round lower bound for the complexity of the bipartite maximal matching problem on $n$-node graphs in the LOCAL model of distributed computing. Both lower bounds are asymptotically tight as a function of the maximum degree $\Delta$. We provide truly tight bounds in $\Delta$ for the complexity of bipartite maximal matching and many natural variants, up to and including the additive constant. As a by-product, our results yield a considerably simplified version of the proof by Balliu et al. We show that our results can be obtained via bounded automatic round elimination, a version of the recent automatic round elimination technique by Brandt [PODC'19] that is particularly suited for automatization from a practical perspective. In this context, our work can be seen as another step towards the automatization of lower bounds in the LOCAL model. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08216">PDF</a>
<h3>No. 4	Honing and proofing Astrophysical codes on the road to Exascale.  Experiences from code modernization on many-core systems</h3><h4>Salvatore Cielo, Luigi Iapichino, Fabio Baruffa, Matteo Bugli, Christoph Federrath</h4> Abstract: The complexity of modern and upcoming computing architectures poses severe challenges for code developers and application specialists, and forces them to expose the highest possible degree of parallelism, in order to make the best use of the available hardware. The Intel$^{(R)}$ Xeon Phi$^{(TM)}$ of second generation (code-named Knights Landing, henceforth KNL) is the latest many-core system, which implements several interesting hardware features like for example a large number of cores per node (up to 72), the 512 bits-wide vector registers and the high-bandwidth memory. The unique features of KNL make this platform a powerful testbed for modern HPC applications. The performance of codes on KNL is therefore a useful proxy of their readiness for future architectures. In this work we describe the lessons learnt during the optimisation of the widely used codes for computational astrophysics P-Gadget-3, Flash and Echo. Moreover, we present results for the visualisation and analysis tools VisIt and yt. These examples show that modern architectures benefit from code optimisation at different levels, even more than traditional multi-core systems. However, the level of modernisation of typical community codes still needs improvements, for them to fully utilise resources of novel architectures. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08161">PDF</a>
<h3>No. 5	The Sum of Its Parts: Analysis of Federated Byzantine Agreement Systems</h3><h4>Martin Florian, Sebastian Henningsen, Bj√∂rn Scheuermann</h4> Abstract: Federated Byzantine Agreement Systems (FBASs) are a fascinating new paradigm in the context of consensus protocols. Originally proposed for powering the Stellar payment network, FBASs can be thought of as a middle way between typical permissionless systems (like Bitcoin) and permissioned approaches for solving consensus (like classical BFT protocols). Unlike Bitcoin and the like, validators must be explicitly chosen by peers. Unlike permissioned protocols, there is no need for the whole system to agree on the same set of validators. Instead, every node is free to decide for itself with whom it requires agreement. In this paper, we propose an intuitive yet precise methodology for determining whether the quorum systems resulting from such individual configurations can enable liveness and safety, respectively how many (byzantine) node failures they are away from losing these qualities. We apply our analysis approach and software to evaluate the effects of different node configuration policies, i.e., logics through which node configurations result from strategic considerations or an existing inter-node relationship graph. Lastly, we also investigate the reported "open-membership" property of FBASs. We observe that an often small group of nodes is exclusively relevant for determining safety and liveness "buffers", and prove that these top tiers are effectively "closed-membership" if maintaining safety is a core requirement. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08101">PDF</a>
<h3>No. 6	Holistic Slowdown Driven Scheduling and Resource Management for  Malleable Jobs</h3><h4>Marco D'Amico, Ana Jokanovic, Julita Corbalan</h4> Abstract: In job scheduling, the concept of malleability has been explored since many years ago. Research shows that malleability improves system performance, but its utilization in HPC never became widespread. The causes are the difficulty in developing malleable applications, and the lack of support and integration of the different layers of the HPC software stack. However, in the last years, malleability in job scheduling is becoming more critical because of the increasing complexity of hardware and workloads. In this context, using nodes in an exclusive mode is not always the most efficient solution as in traditional HPC jobs, where applications were highly tuned for static allocations, but offering zero flexibility to dynamic executions. This paper proposes a new holistic, dynamic job scheduling policy, Slowdown Driven (SD-Policy), which exploits the malleability of applications as the key technology to reduce the average slowdown and response time of jobs. SD-Policy is based on backfill and node sharing. It applies malleability to running jobs to make room for jobs that will run with a reduced set of resources, only when the estimated slowdown improves over the static approach. We implemented SD-Policy in SLURM and evaluated it in a real production environment, and with a simulator using workloads of up to 198K jobs. Results show better resource utilization with the reduction of makespan, response time, slowdown, and energy consumption, up to respectively 7%, 50%, 70%, and 6%, for the evaluated workloads. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08088">PDF</a>
<h3>No. 7	Supporting OpenMP 5.0 Tasks in hpxMP -- A study of an OpenMP  implementation within Task Based Runtime Systems</h3><h4>Tianyi Zhang, Shahrzad Shirzad, Bibek Wagle, Adrian S. Lemoine, Patrick Diehl, Hartmut Kaiser</h4> Abstract: OpenMP has been the de facto standard for single node parallelism for more than a decade. Recently, asynchronous many-task runtime (AMT) systems have increased in popularity as a new programming paradigm for high performance computing applications. One of the major challenges of this new paradigm is the incompatibility of the OpenMP thread model and other AMTs. Highly optimized OpenMP-based libraries do not perform well when coupled with AMTs because the threading of both libraries will compete for resources. This paper is a follow-up paper on the fundamental implementation of hpxMP, an implementation of the OpenMP standard which utilizes the C++ standard library for Parallelism and Concurrency (HPX) to schedule and manage tasks. In this paper, we present the implementation of task features, e.g. taskgroup, task depend, and task_reduction, of the OpenMP 5.0 standard and optimization of the #pragma omp parallel for pragma. We use the daxpy benchmark, the Barcelona OpenMP Tasks Suite, Parallel research kernels, and OpenBLAS benchmarks to compare the different OpenMp implementations: hpxMP, llvm-OpenMP, and GOMP. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.07970">PDF</a>
<h3>No. 8	Parallel Algorithms for Small Subgraph Counting</h3><h4>Amartya Shankha Biswas, Talya Eden, Quanquan C. Liu, Slobodan Mitroviƒá, Ronitt Rubinfeld</h4> Abstract: Subgraph counting is a fundamental problem in analyzing massive graphs, often studied in the context of social and complex networks. There is a rich literature on designing efficient, accurate, and scalable algorithms for this problem. In this work, we tackle this challenge and design several new algorithms for subgraph counting in the Massively Parallel Computation (MPC) model: Given a graph $G$ over $n$ vertices, $m$ edges and $T$ triangles, our first main result is an algorithm that, with high probability, outputs a $(1+\varepsilon)$-approximation to $T$, with optimal round and space complexity provided any $S \geq \max{(\sqrt m, n^2/m)}$ space per machine, assuming $T=\Omega(\sqrt{m/n})$. Our second main result is an $\tilde{O}_{\delta}(\log \log n)$-rounds algorithm for exactly counting the number of triangles, parametrized by the arboricity $\alpha$ of the input graph. The space per machine is $O(n^{\delta})$ for any constant $\delta$, and the total space is $O(m\alpha)$, which matches the time complexity of (combinatorial) triangle counting in the sequential model. We also prove that this result can be extended to exactly counting $k$-cliques for any constant $k$, with the same round complexity and total space $O(m\alpha^{k-2})$. Alternatively, allowing $O(\alpha^2)$ space per machine, the total space requirement reduces to $O(n\alpha^2)$. Finally, we prove that a recent result of Bera, Pashanasangi and Seshadhri (ITCS 2020) for exactly counting all subgraphs of size at most $5$, can be implemented in the MPC model in $\tilde{O}_{\delta}(\sqrt{\log n})$ rounds, $O(n^{\delta})$ space per machine and $O(m\alpha^3)$ total space. Therefore, this result also exhibits the phenomenon that a time bound in the sequential model translates to a space bound in the MPC model. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08299">PDF</a>
<h3>No. 9	BatchLayout: A Batch-Parallel Force-Directed Graph Layout Algorithm in  Shared Memory</h3><h4>Md. Khaledur Rahman, Majedul Haque Sujon, Ariful Azad</h4> Abstract: Force-directed algorithms are widely used to generate aesthetically pleasing layouts of graphs or networks arisen in many scientific disciplines. To visualize large-scale graphs, several parallel algorithms have been discussed in the literature. However, existing parallel algorithms do not utilize memory hierarchy efficiently and often offer limited parallelism. This paper addresses these limitations with BatchLayout, an algorithm that groups vertices into minibatches and processes them in parallel. BatchLayout also employs cache blocking techniques to utilize memory hierarchy efficiently. More parallelism and improved memory accesses coupled with force approximating techniques, better initialization, and optimized learning rate make BatchLayout significantly faster than other state-of-the-art algorithms such as ForceAtlas2 and OpenOrd. The visualization quality of layouts from BatchLayout is comparable or better than similar visualization tools. All of our source code, links to datasets, results and log files are available at this https URL <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08233">PDF</a>
<h3>No. 10	Toward Low-Cost and Stable Blockchain Networks</h3><h4>Minghong Fang, Jia Liu</h4> Abstract: Envisioned to be the future of distributed systems, blockchain networks have received increasing attentions from both industry and academic research in recent years. However, the blockchain mining process consumes vast amounts of energy, and studies have shown that the amount of energy consumed in Bitcoin mining is almost the same as electricity used in Ireland. To address the high mining energy cost problem of blockchain networks, in this paper, we propose a blockchain mining resources allocation algorithm to reduce the mining cost in PoW-based (proof-of-work-based) blockchain networks. We first provide a systematic study on general blockchain queueing model. In our queueing model, transactions arrive randomly to the queue and served in a batch manner with unknown probability distribution and agnostic to any priority mechanism. Then, we leverage Lyapunov optimization techniques to propose a dynamic mining resources allocation algorithm (DMRA), which is parameterized by a tuning parameter $K>0$. We show that our algorithm achieves performance-delay tradeoff as $[O(1/K), O(K)]$. The simulation results also demonstrate the effectiveness of DMRA in reducing the mining cost. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.08027">PDF</a>
<h3>No. 11	Repair rate lower bounds for distributed storage</h3><h4>Michael Luby</h4> Abstract: One of the primary objectives of a distributed storage system is to reliably store a large amount $dsize$ of source data for a long duration using a large number $N$ of unreliable storage nodes, each with capacity $nsize$. The storage overhead $\beta$ is the fraction of system capacity available beyond $dsize$, i.e., $\beta = 1- \frac{dsize}{N \cdot nsize}$. Storage nodes fail randomly over time and are replaced with initially empty nodes, and thus data is erased from the system at an average rate $erate = \lambda \cdot N \cdot nsize$, where $1/\lambda$ is the average lifetime of a node before failure. To maintain recoverability of the source data, a repairer continually reads data over a network from nodes at some average rate $rrate$, and generates and writes data to nodes based on the read data. The main result is that, for any repairer, if the source data is recoverable at each point in time then it must be the case that $rrate \ge \frac{erate}{2 \cdot \beta}$ asymptotically as $N$ goes to infinity and beta goes to zero. This inequality provides a fundamental lower bound on the average rate that any repairer needs to read data from the system in order to maintain recoverability of the source data. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.07904">PDF</a><h2>2020-02-19</h2>
<h3>No. 1	Concurrent Reference Counting and Resource Management in Wait-free  Constant Time</h3><h4>Guy E. Blelloch, Yuanhao Wei</h4> Abstract: A common problem when implementing concurrent programs is efficiently protecting against unsafe races between processes reading and then using a resource (e.g., memory blocks, file descriptors, or network connections) and other processes that are concurrently overwriting and then destructing the same resource. Such read-destruct races can be protected with locks, or with lock-free solutions such as hazard-pointers or read-copy-update (RCU). In this paper we describe a method for protecting read-destruct races with expected constant time overhead, $O(P^2)$ space and $O(P^2)$ delayed destructs, and with just single word atomic memory operations (reads, writes, and CAS). It is based on an interface with four primitives, an acquire-release pair to protect accesses, and a retire-eject pair to delay the destruct until it is safe. We refer to this as the acquire-retire interface. Using the acquire-retire interface, we develop simple implementations for three common use cases: (1) memory reclamation with applications to stacks and queues, (2) reference counted objects, and (3) objects manage by ownership with moves, copies, and destructs. The first two results significantly improve on previous results, and the third application is original. Importantly, all operations have expected constant time overhead. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.07053">PDF</a>
<h3>No. 2	In Search for a Linear Byzantine Agreement</h3><h4>Alexander Spiegelman</h4> Abstract: The long-standing byzantine agreement problem gets more attention in recent years due to the increasing demand for scalable geo-replicated Byzantine state machine replication (SMR) systems (e.g., Blockchains). To date, the key bottleneck of such systems is the communication cost of the byzantine agreement they employ as a building block, which motivates many researchers to search for low-communication byzantine agreement protocols. The conventional approach is to design deterministic protocols in the eventually synchronous communication model that are optimized to reduce the communication cost after the global stabilization time (GST). In this paper, we challenge the conventional approach and argue it is not the best fit for scalable SMR systems since it might induce an unbounded communication cost during asynchronous periods before GST, which we prove to be inherent. Instead, we forgo eventual synchrony and propose a different approach that hopes for the best (synchrony) but prepares for the worst (asynchrony). Accordingly, we design an optimistic protocol that first tries to reach an agreement via an efficient deterministic algorithm that relies on synchrony for termination, and then, only if an agreement was not reached due to asynchrony, the protocol uses a randomized asynchronous algorithm for fallback that guarantees termination with probability $1$. Although randomized asynchronous algorithms are considered to be costly, we design our solution to pay this cost only when an equivalent cost has already been paid while unsuccessfully trying the synchronous protocol. Moreover, we formally prove that our protocol achieves optimal communication complexity under all network conditions and failure scenarios. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.06993">PDF</a>
<h3>No. 3	Simulating Performance of ML Systems with Offline Profiling</h3><h4>Hongming Huang, Peng Cheng, Hong Xu, Yongqiang Xiong</h4> Abstract: We advocate that simulation based on offline profiling is a promising approach to better understand and improve the complex ML systems. Our approach uses operation-level profiling and dataflow based simulation to ensure it offers a unified and automated solution for all frameworks and ML models, and is also accurate by considering the various parallelization strategies in a real system. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.06790">PDF</a>
<h3>No. 4	Byzantine Lattice Agreement in Asynchronous Systems</h3><h4>Xiong Zheng, Vijay Garg</h4> Abstract: We study the Byzantine lattice agreement (BLA) problem in asynchronous distributed message passing systems. In the BLA problem, each process proposes a value from a join semi-lattice and needs to output a value also in the lattice such that all output values of correct processes lie on a chain despite the presence of Byzantine processes. We present an algorithm for this problem with round complexity of $O(\log f)$ which tolerates $f < \frac{n}{5}$ Byzantine failures in the asynchronous setting without digital signatures, where $n$ is the number of processes. We also show how this algorithm can be modified to work in the authenticated setting (i.e., with digital signatures) to tolerate $f < \frac{n}{3}$ Byzantine failures. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.06779">PDF</a>
<h3>No. 5	How fast can you update your MST? (Dynamic algorithms for cluster  computing)</h3><h4>Seth Gilbert, Lawrence Li</h4> Abstract: Imagine a large graph that is being processed by a cluster of computers, e.g., described by the $k$-machine model or the Massively Parallel Computation Model. The graph, however, is not static; instead it is receiving a constant stream of updates. How fast can the cluster process the stream of updates? The fundamental question we want to ask in this paper is whether we can update the graph fast enough to keep up with the stream. We focus specifically on the problem of maintaining a minimum spanning tree (MST), and we give an algorithm for the $k$-machine model that can process $O(k)$ graph updates per $O(1)$ rounds with high probability. (And these results carry over to the Massively Parallel Computation (MPC) model.) We also show a lower bound, i.e., it is impossible to process $k^{1+\epsilon}$ updates in $O(1)$ rounds. Thus we provide a nearly tight answer to the question of how fast a cluster can respond to a stream of graph modifications while maintaining an MST. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.06762">PDF</a>
<h3>No. 6	Running a Pre-Exascale, Geographically Distributed, Multi-Cloud  Scientific Simulation</h3><h4>Igor Sfiligoi, Frank Wuerthwein, Benedikt Riedel, David Schultz</h4> Abstract: As we approach the Exascale era, it is important to verify that the existing frameworks and tools will still work at that scale. Moreover, public Cloud computing has been emerging as a viable solution for both prototyping and urgent computing. Using the elasticity of the Cloud, we have thus put in place a pre-exascale HTCondor setup for running a scientific simulation in the Cloud, with the chosen application being IceCube's photon propagation simulation. I.e. this was not a purely demonstration run, but it was also used to produce valuable and much needed scientific results for the IceCube collaboration. In order to reach the desired scale, we aggregated GPU resources across 8 GPU models from many geographic regions across Amazon Web Services, Microsoft Azure, and the Google Cloud Platform. Using this setup, we reached a peak of over 51k GPUs corresponding to almost 380 PFLOP32s, for a total integrated compute of about 100k GPU hours. In this paper we provide the description of the setup, the problems that were discovered and overcome, as well as a short description of the actual science output of the exercise. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.06667">PDF</a>
<h3>No. 7	Not a COINcidence: Sub-Quadratic Asynchronous Byzantine Agreement WHP</h3><h4>Shir Cohen, Idit Keidar, Alexander Spiegelman</h4> Abstract: King and Saia were the first to break the quadratic word complexity bound for Byzantine Agreement in synchronous systems against an adaptive adversary, and Algorand broke this bound with near-optimal resilience in the eventual-synchrony model. Yet the question of asynchronous sub-quadratic Byzantine Agreement remained open. To the best of our knowledge, we are the first to answer this question in the affirmative. A key component of our solution is a novel shared coin algorithm based on a VRF, without any further trusted setup. A second essential ingredient is VRF-based committee sampling, which we formalize and utilize in the asynchronous model for the first time. Our algorithms work against a delayed-adaptive adversary, which cannot perform after-the-fact removals but has full control of Byzantine processes and full information about communication in earlier rounds. Using committee sampling and our shared coin, we solve Byzantine Agreement with high probability, with a word complexity of $\widetilde{O}(n)$ and $O(1)$ expected time, breaking the $O(n^2)$ bit barrier for asynchronous Byzantine Agreement. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.06545">PDF</a>
<h3>No. 8	Distributed Sketching Methods for Privacy Preserving Regression</h3><h4>Burak Bartan, Mert Pilanci</h4> Abstract: In this work, we study distributed sketching methods for large scale regression problems. We leverage multiple randomized sketches for reducing the problem dimensions as well as preserving privacy and improving straggler resilience in asynchronous distributed systems. We derive novel approximation guarantees for classical sketching methods and analyze the accuracy of parameter averaging for distributed sketches. We consider random matrices including Gaussian, randomized Hadamard, uniform sampling and leverage score sampling in the distributed setting. Moreover, we propose a hybrid approach combining sampling and fast random projections for better computational efficiency. We illustrate the performance of distributed sketches in a serverless computing platform with large scale experiments. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.06538">PDF</a>
<h3>No. 9	Demystifying the Performance of HPC Scientific Applications on NVM-based  Memory Systems</h3><h4>Ivy Peng, Kai Wu, Jie Ren, Dong Li, Maya Gokhale</h4> Abstract: The emergence of high-density byte-addressable non-volatile memory (NVM) is promising to accelerate data- and compute-intensive applications. Current NVM technologies have lower performance than DRAM and, thus, are often paired with DRAM in a heterogeneous main memory. Recently, byte-addressable NVM hardware becomes available. This work provides a timely evaluation of representative HPC applications from the "Seven Dwarfs" on NVM-based main memory. Our results quantify the effectiveness of DRAM-cached-NVM for accelerating HPC applications and enabling large problems beyond the DRAM capacity. On uncached-NVM, HPC applications exhibit three tiers of performance sensitivity, i.e., insensitive, scaled, and bottlenecked. We identify write throttling and concurrency control as the priorities in optimizing applications. We highlight that concurrency change may have a diverging effect on read and write accesses in applications. Based on these findings, we explore two optimization approaches. First, we provide a prediction model that uses datasets from a small set of configurations to estimate performance at various concurrency and data sizes to avoid exhaustive search in the configuration space. Second, we demonstrate that write-aware data placement on uncached-NVM could achieve $2$x performance improvement with a 60% reduction in DRAM usage. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.06499">PDF</a>
<h3>No. 10	Big Data Staging with MPI-IO for Interactive X-ray Science</h3><h4>Justin M. Wozniak, Hemant Sharma, Timothy G. Armstrong, Michael Wilde, Jonathan D. Almer, Ian Foster</h4> Abstract: New techniques in X-ray scattering science experiments produce large data sets that can require millions of high-performance processing hours per week of computation for analysis. In such applications, data is typically moved from X-ray detectors to a large parallel file system shared by all nodes of a petascale supercomputer and then is read repeatedly as different science application tasks proceed. However, this straightforward implementation causes significant contention in the file system. We propose an alternative approach in which data is instead staged into and cached in compute node memory for extended periods, during which time various processing tasks may efficiently access it. We describe here such a big data staging framework, based on MPI-IO and the Swift parallel scripting language. We discuss a range of large-scale data management issues involved in X-ray scattering science and measure the performance benefits of the new staging framework for high-energy diffraction microscopy, an important emerging application in data-intensive X-ray scattering. We show that our framework accelerates scientific processing turnaround from three months to under 10 minutes, and that our I/O technique reduces input overheads by a factor of 5 on 8K Blue Gene/Q nodes. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.06258">PDF</a>
<h3>No. 11	Computing rank-revealing factorizations of matrices stored out-of-core</h3><h4>Nathan Heavner, Per-Gunnar Martinsson, Gregorio Quintana-Ort√≠</h4> Abstract: This paper describes efficient algorithms for computing rank-revealing factorizations of matrices that are too large to fit in RAM, and must instead be stored on slow external memory devices such as solid-state or spinning disk hard drives (out-of-core or out-of-memory). Traditional algorithms for computing rank revealing factorizations, such as the column pivoted QR factorization, or techniques for computing a full singular value decomposition of a matrix, are very communication intensive. They are naturally expressed as a sequence of matrix-vector operations, which become prohibitively expensive when data is not available in main memory. Randomization allows these methods to be reformulated so that large contiguous blocks of the matrix can be processed in bulk. The paper describes two distinct methods. The first is a blocked version of column pivoted Householder QR, organized as a ``left-looking'' method to minimize the number of write operations (which are more expensive than read operations on a spinning disk drive). The second method results in a so called UTV factorization which expresses a matrix $A$ as $A = U T V^*$ where $U$ and $V$ are unitary, and $T$ is triangular. This method is organized as an algorithm-by-blocks, in which floating point operations overlap read and write operations. The second method incorporates power iterations, and is exceptionally good at revealing the numerical rank; it can often be used as a substitute for a full singular value decomposition. Numerical experiments demonstrate that the new algorithms are almost as fast when processing data stored on a hard drive as traditional algorithms are for data stored in main memory. To be precise, the computational time for fully factorizing an $n\times n$ matrix scales as $cn^{3}$, with a scaling constant $c$ that is only marginally larger when the matrix is stored out of core. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.06960">PDF</a>
<h3>No. 12	Distributed Averaging Methods for Randomized Second Order Optimization</h3><h4>Burak Bartan, Mert Pilanci</h4> Abstract: We consider distributed optimization problems where forming the Hessian is computationally challenging and communication is a significant bottleneck. We develop unbiased parameter averaging methods for randomized second order optimization that employ sampling and sketching of the Hessian. Existing works do not take the bias of the estimators into consideration, which limits their application to massively parallel computation. We provide closed-form formulas for regularization parameters and step sizes that provably minimize the bias for sketched Newton directions. We also extend the framework of second order averaging methods to introduce an unbiased distributed optimization framework for heterogeneous computing systems with varying worker resources. Additionally, we demonstrate the implications of our theoretical findings via large scale experiments performed on a serverless computing platform. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.06540">PDF</a>
<h3>No. 13	Bitcoin's Blockchain Data Analytics: A Graph Theoretic Perspective</h3><h4>Aman Sharma, Ashutosh Bhatia</h4> Abstract: Bitcoin is the most popular cryptocurrency used worldwide. It provides pseudonymity to its users by establishing identity using public keys as transaction end-points. These transactions are recorded on an immutable public ledger called Blockchain which is an append-only data structure. The popularity of Bitcoin has increased unreasonably. The general trend shows a positive response from the common masses indicating an increase in trust and privacy concerns which makes an interesting use case from the analysis point of view. Moreover, since the blockchain is publicly available and up-to-date, any analysis would provide a live insight into the usage patterns which ultimately would be useful for making a number of inferences by law-enforcement agencies, economists, tech-enthusiasts, etc. In this paper, we study various applications and techniques of performing data analytics over Bitcoin blockchain from a graph theoretic perspective. We also propose a framework for performing such data analytics and explored a couple of use cases using the proposed framework. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.06403">PDF</a>
<h3>No. 14	Neural Architecture Search over Decentralized Data</h3><h4>Mengwei Xu, Yuxin Zhao, Kaigui Bian, Gang Huang, Qiaozhu Mei, Xuanzhe Liu</h4> Abstract: To preserve user privacy while enabling mobile intelligence, techniques have been proposed to train deep neural networks on decentralized data. However, training over decentralized data makes the design of neural architecture quite difficult as it already was. Such difficulty is further amplified when designing and deploying different neural architectures for heterogeneous mobile platforms. In this work, we propose an automatic neural architecture search into the decentralized training, as a new DNN training paradigm called Federated Neural Architecture Search, namely federated NAS. To deal with the primary challenge of limited on-client computational and communication resources, we present FedNAS, a highly optimized framework for efficient federated NAS. FedNAS fully exploits the key opportunity of insufficient model candidate re-training during the architecture search process, and incorporates three key optimizations: parallel candidates training on partial clients, early dropping candidates with inferior performance, and dynamic round numbers. Tested on large-scale datasets and typical CNN architectures, FedNAS achieves comparable model accuracy as state-of-the-art NAS algorithm that trains models with centralized data, and also reduces the client cost by up to two orders of magnitude compared to a straightforward design of federated NAS. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.06352">PDF</a><h2>2020-02-18</h2>
<h3>No. 1	Concurrent Reference Counting and Resource Management in Wait-free  Constant Time</h3><h4>Guy E. Blelloch, Yuanhao Wei</h4> Abstract: A common problem when implementing concurrent programs is efficiently protecting against unsafe races between processes reading and then using a resource (e.g., memory blocks, file descriptors, or network connections) and other processes that are concurrently overwriting and then destructing the same resource. Such read-destruct races can be protected with locks, or with lock-free solutions such as hazard-pointers or read-copy-update (RCU). In this paper we describe a method for protecting read-destruct races with expected constant time overhead, $O(P^2)$ space and $O(P^2)$ delayed destructs, and with just single word atomic memory operations (reads, writes, and CAS). It is based on an interface with four primitives, an acquire-release pair to protect accesses, and a retire-eject pair to delay the destruct until it is safe. We refer to this as the acquire-retire interface. Using the acquire-retire interface, we develop simple implementations for three common use cases: (1) memory reclamation with applications to stacks and queues, (2) reference counted objects, and (3) objects manage by ownership with moves, copies, and destructs. The first two results significantly improve on previous results, and the third application is original. Importantly, all operations have expected constant time overhead. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.07053">PDF</a>
<h3>No. 2	In Search for a Linear Byzantine Agreement</h3><h4>Alexander Spiegelman</h4> Abstract: The long-standing byzantine agreement problem gets more attention in recent years due to the increasing demand for scalable geo-replicated Byzantine state machine replication (SMR) systems (e.g., Blockchains). To date, the key bottleneck of such systems is the communication cost of the byzantine agreement they employ as a building block, which motivates many researchers to search for low-communication byzantine agreement protocols. The conventional approach is to design deterministic protocols in the eventually synchronous communication model that are optimized to reduce the communication cost after the global stabilization time (GST). In this paper, we challenge the conventional approach and argue it is not the best fit for scalable SMR systems since it might induce an unbounded communication cost during asynchronous periods before GST, which we prove to be inherent. Instead, we forgo eventual synchrony and propose a different approach that hopes for the best (synchrony) but prepares for the worst (asynchrony). Accordingly, we design an optimistic protocol that first tries to reach an agreement via an efficient deterministic algorithm that relies on synchrony for termination, and then, only if an agreement was not reached due to asynchrony, the protocol uses a randomized asynchronous algorithm for fallback that guarantees termination with probability $1$. Although randomized asynchronous algorithms are considered to be costly, we design our solution to pay this cost only when an equivalent cost has already been paid while unsuccessfully trying the synchronous protocol. Moreover, we formally prove that our protocol achieves optimal communication complexity under all network conditions and failure scenarios. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.06993">PDF</a>
<h3>No. 3	Simulating Performance of ML Systems with Offline Profiling</h3><h4>Hongming Huang, Peng Cheng, Hong Xu, Yongqiang Xiong</h4> Abstract: We advocate that simulation based on offline profiling is a promising approach to better understand and improve the complex ML systems. Our approach uses operation-level profiling and dataflow based simulation to ensure it offers a unified and automated solution for all frameworks and ML models, and is also accurate by considering the various parallelization strategies in a real system. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.06790">PDF</a>
<h3>No. 4	Byzantine Lattice Agreement in Asynchronous Systems</h3><h4>Xiong Zheng, Vijay Garg</h4> Abstract: We study the Byzantine lattice agreement (BLA) problem in asynchronous distributed message passing systems. In the BLA problem, each process proposes a value from a join semi-lattice and needs to output a value also in the lattice such that all output values of correct processes lie on a chain despite the presence of Byzantine processes. We present an algorithm for this problem with round complexity of $O(\log f)$ which tolerates $f < \frac{n}{5}$ Byzantine failures in the asynchronous setting without digital signatures, where $n$ is the number of processes. We also show how this algorithm can be modified to work in the authenticated setting (i.e., with digital signatures) to tolerate $f < \frac{n}{3}$ Byzantine failures. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.06779">PDF</a>
<h3>No. 5	How fast can you update your MST? (Dynamic algorithms for cluster  computing)</h3><h4>Seth Gilbert, Lawrence Li</h4> Abstract: Imagine a large graph that is being processed by a cluster of computers, e.g., described by the $k$-machine model or the Massively Parallel Computation Model. The graph, however, is not static; instead it is receiving a constant stream of updates. How fast can the cluster process the stream of updates? The fundamental question we want to ask in this paper is whether we can update the graph fast enough to keep up with the stream. We focus specifically on the problem of maintaining a minimum spanning tree (MST), and we give an algorithm for the $k$-machine model that can process $O(k)$ graph updates per $O(1)$ rounds with high probability. (And these results carry over to the Massively Parallel Computation (MPC) model.) We also show a lower bound, i.e., it is impossible to process $k^{1+\epsilon}$ updates in $O(1)$ rounds. Thus we provide a nearly tight answer to the question of how fast a cluster can respond to a stream of graph modifications while maintaining an MST. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.06762">PDF</a>
<h3>No. 6	Running a Pre-Exascale, Geographically Distributed, Multi-Cloud  Scientific Simulation</h3><h4>Igor Sfiligoi, Frank Wuerthwein, Benedikt Riedel, David Schultz</h4> Abstract: As we approach the Exascale era, it is important to verify that the existing frameworks and tools will still work at that scale. Moreover, public Cloud computing has been emerging as a viable solution for both prototyping and urgent computing. Using the elasticity of the Cloud, we have thus put in place a pre-exascale HTCondor setup for running a scientific simulation in the Cloud, with the chosen application being IceCube's photon propagation simulation. I.e. this was not a purely demonstration run, but it was also used to produce valuable and much needed scientific results for the IceCube collaboration. In order to reach the desired scale, we aggregated GPU resources across 8 GPU models from many geographic regions across Amazon Web Services, Microsoft Azure, and the Google Cloud Platform. Using this setup, we reached a peak of over 51k GPUs corresponding to almost 380 PFLOP32s, for a total integrated compute of about 100k GPU hours. In this paper we provide the description of the setup, the problems that were discovered and overcome, as well as a short description of the actual science output of the exercise. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.06667">PDF</a>
<h3>No. 7	Not a COINcidence: Sub-Quadratic Asynchronous Byzantine Agreement WHP</h3><h4>Shir Cohen, Idit Keidar, Alexander Spiegelman</h4> Abstract: King and Saia were the first to break the quadratic word complexity bound for Byzantine Agreement in synchronous systems against an adaptive adversary, and Algorand broke this bound with near-optimal resilience in the eventual-synchrony model. Yet the question of asynchronous sub-quadratic Byzantine Agreement remained open. To the best of our knowledge, we are the first to answer this question in the affirmative. A key component of our solution is a novel shared coin algorithm based on a VRF, without any further trusted setup. A second essential ingredient is VRF-based committee sampling, which we formalize and utilize in the asynchronous model for the first time. Our algorithms work against a delayed-adaptive adversary, which cannot perform after-the-fact removals but has full control of Byzantine processes and full information about communication in earlier rounds. Using committee sampling and our shared coin, we solve Byzantine Agreement with high probability, with a word complexity of $\widetilde{O}(n)$ and $O(1)$ expected time, breaking the $O(n^2)$ bit barrier for asynchronous Byzantine Agreement. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.06545">PDF</a>
<h3>No. 8	Distributed Sketching Methods for Privacy Preserving Regression</h3><h4>Burak Bartan, Mert Pilanci</h4> Abstract: In this work, we study distributed sketching methods for large scale regression problems. We leverage multiple randomized sketches for reducing the problem dimensions as well as preserving privacy and improving straggler resilience in asynchronous distributed systems. We derive novel approximation guarantees for classical sketching methods and analyze the accuracy of parameter averaging for distributed sketches. We consider random matrices including Gaussian, randomized Hadamard, uniform sampling and leverage score sampling in the distributed setting. Moreover, we propose a hybrid approach combining sampling and fast random projections for better computational efficiency. We illustrate the performance of distributed sketches in a serverless computing platform with large scale experiments. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.06538">PDF</a>
<h3>No. 9	Demystifying the Performance of HPC Scientific Applications on NVM-based  Memory Systems</h3><h4>Ivy Peng, Kai Wu, Jie Ren, Dong Li, Maya Gokhale</h4> Abstract: The emergence of high-density byte-addressable non-volatile memory (NVM) is promising to accelerate data- and compute-intensive applications. Current NVM technologies have lower performance than DRAM and, thus, are often paired with DRAM in a heterogeneous main memory. Recently, byte-addressable NVM hardware becomes available. This work provides a timely evaluation of representative HPC applications from the "Seven Dwarfs" on NVM-based main memory. Our results quantify the effectiveness of DRAM-cached-NVM for accelerating HPC applications and enabling large problems beyond the DRAM capacity. On uncached-NVM, HPC applications exhibit three tiers of performance sensitivity, i.e., insensitive, scaled, and bottlenecked. We identify write throttling and concurrency control as the priorities in optimizing applications. We highlight that concurrency change may have a diverging effect on read and write accesses in applications. Based on these findings, we explore two optimization approaches. First, we provide a prediction model that uses datasets from a small set of configurations to estimate performance at various concurrency and data sizes to avoid exhaustive search in the configuration space. Second, we demonstrate that write-aware data placement on uncached-NVM could achieve $2$x performance improvement with a 60% reduction in DRAM usage. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.06499">PDF</a>
<h3>No. 10	Big Data Staging with MPI-IO for Interactive X-ray Science</h3><h4>Justin M. Wozniak, Hemant Sharma, Timothy G. Armstrong, Michael Wilde, Jonathan D. Almer, Ian Foster</h4> Abstract: New techniques in X-ray scattering science experiments produce large data sets that can require millions of high-performance processing hours per week of computation for analysis. In such applications, data is typically moved from X-ray detectors to a large parallel file system shared by all nodes of a petascale supercomputer and then is read repeatedly as different science application tasks proceed. However, this straightforward implementation causes significant contention in the file system. We propose an alternative approach in which data is instead staged into and cached in compute node memory for extended periods, during which time various processing tasks may efficiently access it. We describe here such a big data staging framework, based on MPI-IO and the Swift parallel scripting language. We discuss a range of large-scale data management issues involved in X-ray scattering science and measure the performance benefits of the new staging framework for high-energy diffraction microscopy, an important emerging application in data-intensive X-ray scattering. We show that our framework accelerates scientific processing turnaround from three months to under 10 minutes, and that our I/O technique reduces input overheads by a factor of 5 on 8K Blue Gene/Q nodes. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.06258">PDF</a>
<h3>No. 11	Bitcoin's Blockchain Data Analytics: A Graph Theoretic Perspective</h3><h4>Aman Sharma, Ashutosh Bhatia</h4> Abstract: Bitcoin is the most popular cryptocurrency used worldwide. It provides pseudonymity to its users by establishing identity using public keys as transaction end-points. These transactions are recorded on an immutable public ledger called Blockchain which is an append-only data structure. The popularity of Bitcoin has increased unreasonably. The general trend shows a positive response from the common masses indicating an increase in trust and privacy concerns which makes an interesting use case from the analysis point of view. Moreover, since the blockchain is publicly available and up-to-date, any analysis would provide a live insight into the usage patterns which ultimately would be useful for making a number of inferences by law-enforcement agencies, economists, tech-enthusiasts, etc. In this paper, we study various applications and techniques of performing data analytics over Bitcoin blockchain from a graph theoretic perspective. We also propose a framework for performing such data analytics and explored a couple of use cases using the proposed framework. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.06403">PDF</a>
<h3>No. 12	Neural Architecture Search over Decentralized Data</h3><h4>Mengwei Xu, Yuxin Zhao, Kaigui Bian, Gang Huang, Qiaozhu Mei, Xuanzhe Liu</h4> Abstract: To preserve user privacy while enabling mobile intelligence, techniques have been proposed to train deep neural networks on decentralized data. However, training over decentralized data makes the design of neural architecture quite difficult as it already was. Such difficulty is further amplified when designing and deploying different neural architectures for heterogeneous mobile platforms. In this work, we propose an automatic neural architecture search into the decentralized training, as a new DNN training paradigm called Federated Neural Architecture Search, namely federated NAS. To deal with the primary challenge of limited on-client computational and communication resources, we present FedNAS, a highly optimized framework for efficient federated NAS. FedNAS fully exploits the key opportunity of insufficient model candidate re-training during the architecture search process, and incorporates three key optimizations: parallel candidates training on partial clients, early dropping candidates with inferior performance, and dynamic round numbers. Tested on large-scale datasets and typical CNN architectures, FedNAS achieves comparable model accuracy as state-of-the-art NAS algorithm that trains models with centralized data, and also reduces the client cost by up to two orders of magnitude compared to a straightforward design of federated NAS. <br><a href = "http://xxx.itp.ac.cn/pdf/2002.06352">PDF</a>
</body></html>